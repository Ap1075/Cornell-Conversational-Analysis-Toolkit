import numpy as np
from convokit.transformer import Transformer
from typing import Optional, Callable, List
from convokit.model import Corpus, Conversation, CorpusObject
from tensorly.decomposition import parafac
import matplotlib.pyplot as plt
import os
import seaborn as sns
from .graphics import get_graphic_dict
from sklearn.preprocessing import StandardScaler
from collections import defaultdict, Counter
from jinja2 import Environment, FileSystemLoader
import shutil
from sklearn import metrics
from sklearn.cluster import KMeans
from statistics import mode
import tensortools as tt


class TensorDecomposer(Transformer):
    """

    :param feature_set: ordered list of features to use in construct the tensor, e.g. hyperconv-1, hyperconv-2...,
        where hyperconv-1 is a dictionary
    :param normalize: whether to normalize the tensor before decomposition

    :ivar tensor: constructed tensor (constructed during transform step), None otherwise
    :ivar features: features used in constructing the tensor
    """

    def __init__(self, feature_set: List[str], obj_type: str = "conversation", rank: int = 9,
                 normalize_func=lambda tensor: tensor, tensor_func="tensorly",
                 anomaly_threshold = 1.5, impute_na: float = -1, group_func = lambda obj: obj.id):
        self.obj_type = obj_type
        self.feature_set = feature_set
        self.rank = rank
        self.normalize_func = normalize_func
        self.anomaly_threshold = anomaly_threshold
        self.impute_na = impute_na
        self.group_func = group_func
        self.tensor_func = tensor_func

        self.tensor = None
        self.features = None
        self.obj_ids = None
        self.factors = None
        self.groups = None # equivalent of subreddits

    def _construct_tensor(self, corpus, selector: Optional[Callable[[CorpusObject], bool]] = lambda obj: True):
        """
        Constructs the 3-way data tensor
        :param corpus: input corpus
        :param selector: selector for corpus objects
        :return: constructed tensor
        """
        objs = list(corpus.iter_objs(self.obj_type, selector=selector))
        num_objs = len(objs)
        self.obj_ids = [obj.id for obj in objs]

        self.groups = [self.group_func(obj) for obj in objs]

        num_feature_sets = len(self.feature_set)

        self.features = list(objs[0].meta[self.feature_set[0]])
        num_features = len(self.features)

        tensor = np.zeros((num_feature_sets, num_objs, num_features))

        for obj_idx, obj in enumerate(objs):
            for feat_idx, feat_name in enumerate(self.feature_set):
                tensor[feat_idx][obj_idx] = list(obj.meta[feat_name].values())

        if self.impute_na is not None:
            tensor[np.isnan(tensor)] = self.impute_na

        return tensor

    @staticmethod
    def _min_max_scale(mat):
        max_ = np.max(mat)
        min_ = np.min(mat)
        return (mat - min_) / (max_ - min_)

    @staticmethod
    def _normalize(tensor):
        tensor = tensor.copy()
        for i in range(tensor.shape[2]):
            tensor[:, :, i] = TensorDecomposer._min_max_scale(tensor[:, :, i])
        return tensor

    def fit(self, corpus: Corpus, y=None, selector: Optional[Callable[[CorpusObject], bool]] = lambda obj: True):
        """
        Retrieves features from the Corpus Conversations using retrieve_feats() and annotates Conversations with this feature set

        :param corpus: Corpus object to retrieve feature information from
        :param selector: a (lambda) function that takes a Conversation and returns True / False; function selects
            conversations to be annotated with hypergraph features. By default, all conversations will be annotated.
        :return: corpus with conversations having a new meta field with the specified feature name  containing the stats generated by retrieve_feats().
        """
        # construct + normalize tensor
        print("Constructing tensor...", end="")
        tensor = self._construct_tensor(corpus, selector)
        print("Done.")
        tensor = self.normalize_func(tensor)
        self.tensor = tensor

        # decompose tensor
        print("Decomposing tensor...", end="")
        if self.tensor_func == 'tensorly':
            self.factors = parafac(tensor, rank=self.rank)[1]
        elif self.tensor_func == 'tensortools-ncp-hals':
            self.factors = tt.ncp_hals(self.tensor, 3, random_state=2020).factors.factors
        elif self.tensor_func == 'tensortools-ncp-bcd':
            self.factors = tt.ncp_bcd(self.tensor, 3, random_state=2020).factors.factors
        else:
            raise ValueError("Invalid tensor function.")
        print("Done.")

        return self

    def _generate_plots(self, factors, axis_names, output_dir, d=3):
        os.makedirs(os.path.join(output_dir, 'graphs'), exist_ok=True)
        a, b, c = factors
        rank = a.shape[1]
        for component_idx in range(rank):
            fig, ax = plt.subplots(1, d, figsize=(12, 0.3+int(self.rank * 1.2)))
            ax[0].set_ylabel("Component " + str(component_idx+1))
            factors_name = axis_names if d==3 else ["Time", "Features"]

            for col_idx in range(d):
                sns.despine(top=True, ax=ax[col_idx])
                ax[col_idx].plot(factors[col_idx].T[component_idx])
                ax[col_idx].set_xlabel(factors_name[col_idx])
            plt.savefig(os.path.join(output_dir, 'graphs', 'component_plot_{}.png'.format(component_idx+1)))


    def _get_anomalous_points(self, factor_full, idx):
        scaler = StandardScaler()
        factor = factor_full[:, idx]
        reshaped = factor.reshape((factor.shape[0], 1))
        scaled = scaler.fit_transform(reshaped)
        pos_pts = np.argwhere(scaled.reshape(factor.shape[0]) > self.anomaly_threshold).flatten()
        neg_pts = np.argwhere(scaled.reshape(factor.shape[0]) < -self.anomaly_threshold).flatten()
        return pos_pts, neg_pts

    def _generate_high_level_summary(self):
        # generate_plots()

        factor1, factor2, factor3 = self.factors # time, thread, feature

        idx_to_distinctive_threads = defaultdict(dict)
        idx_to_distinctive_features = defaultdict(dict)

        # normalizing
        group_totals = Counter(self.groups)
        for idx in range(self.rank):
            pos_thread_pts, neg_thread_pts = self._get_anomalous_points(factor2, idx)
            idx_to_distinctive_threads[idx]['pos_threads'] = Counter([self.groups[i] for i in pos_thread_pts])
            idx_to_distinctive_threads[idx]['neg_threads'] = Counter([self.groups[i] for i in neg_thread_pts])

            # normalize group counts
            for group in idx_to_distinctive_threads[idx]['pos_threads']:
                idx_to_distinctive_threads[idx]['pos_threads'][group] /= group_totals[group]
                idx_to_distinctive_threads[idx]['neg_threads'][group] /= group_totals[group]

            pos_features, neg_features = self._get_anomalous_points(factor3, idx)
            idx_to_distinctive_features[idx]['pos_features'] = [self.features[i] for i in pos_features]
            idx_to_distinctive_features[idx]['neg_features'] = [self.features[i] for i in neg_features]

        component_to_details = dict()
        for idx in range(self.rank):
            component_to_details[idx] = dict()
            pos_groups = sorted(list(idx_to_distinctive_threads[idx]['pos_threads'].items()),
                                    key=lambda x: x[1], reverse=True)
            component_to_details[idx]['pos_groups'] = [k for k, v in pos_groups[:5]]
            neg_groups = sorted(list(idx_to_distinctive_threads[idx]['neg_threads'].items()),
                                    key=lambda x: x[1], reverse=True)
            component_to_details[idx]['neg_groups'] = [k for k, v in neg_groups[:5]]

            component_to_details[idx]['pos_features'] = get_graphic_dict(idx_to_distinctive_features[idx]['pos_features'][:10])
            component_to_details[idx]['neg_features'] = get_graphic_dict(idx_to_distinctive_features[idx]['neg_features'][:10])

        return component_to_details


    def _generate_html(self, title, output_dir):
        root = os.path.dirname(os.path.abspath(__file__))
        component_to_details = self._generate_high_level_summary()
        env = Environment(loader=FileSystemLoader(os.path.join(root, 'template')))
        template = env.get_template('report.html')
        filename = os.path.join(output_dir, "report.html")
        with open(filename, 'w') as fh:
            fh.write(
                template.render(title=title, component_to_details=component_to_details,
                                graph_filepath=os.path.join(output_dir, "graphs"))
            )

    def transform(self, corpus: Corpus, selector: Optional[Callable[[CorpusObject], bool]] = lambda obj: True) -> Corpus:
        obj_factor = self.factors[1]
        for idx, obj in enumerate(corpus.iter_objs(self.obj_type, selector)):
            obj.meta['tensor_factor'] = obj_factor[idx]
        return corpus

    def summarize(self, corpus: Corpus, axis_names: List[str], output_dir: str = '.',
                  report_title="Report"):

        os.makedirs(output_dir, exist_ok=True)
        self._generate_plots(self.factors, axis_names, output_dir)
        root = os.path.dirname(os.path.abspath(__file__))
        try:
            shutil.copytree(os.path.join(root, 'static'), os.path.join(output_dir, 'static'))
            shutil.copytree(os.path.join(root, 'images'), os.path.join(output_dir, 'images'))
        except FileExistsError:
            print("Directory already exists. Exiting summarize()")
            return
        self._generate_html(report_title, output_dir)
        print("Report generated at {}".format(os.path.join(output_dir, "report.html")))

    @staticmethod
    def _purity_score(y_true, y_pred):
        contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)
        return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)

    def purity(self, n_clusters):
        assert self.factors is not None
        time_factor, thread_factor, feature_factor = self.factors
        kmeans = KMeans(n_clusters=n_clusters, random_state=2020).fit(thread_factor)
        y_pred = kmeans.predict(thread_factor)
        y_true = np.array([0]*333 + [1]*333 + [2]*333)

        y_pred_cluster0 = y_true[y_pred == 0]
        y_pred_cluster1 = y_true[y_pred == 1]
        y_pred_cluster2 = y_true[y_pred == 2]

        correct = np.sum(y_pred_cluster0 == mode(y_pred_cluster0)) + \
                  np.sum(y_pred_cluster1 == mode(y_pred_cluster1)) + \
                  np.sum(y_pred_cluster2 == mode(y_pred_cluster2))
        return correct / len(y_pred)