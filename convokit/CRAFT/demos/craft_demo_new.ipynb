{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5JqrpJWoNaB"
   },
   "source": [
    "# CRAFT demo (inference only) using ConvoKit\n",
    "\n",
    "This example notebook shows how an already-trained CRAFT model can be applied to conversational data to predict future derailment. This example uses the fully trained Wikiconv-based model as reported in the \"Trouble on the Horizon\" paper, and applies it to ConvoKit's version of the labeled Wikiconv corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHkNojY7tzAh"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries, including convokit\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import unicodedata\n",
    "import itertools\n",
    "from urllib.request import urlretrieve\n",
    "from convokit import download, Corpus\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6sA5pzI7LRtz"
   },
   "outputs": [],
   "source": [
    "# define globals and constants\n",
    "\n",
    "MAX_LENGTH = 80  # Maximum sentence length (number of tokens) to consider\n",
    "\n",
    "# configure model\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "context_encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "labeled_learning_rate = 1e-5\n",
    "decoder_learning_ratio = 5.0\n",
    "print_every = 10\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "UNK_token = 3  # Unknown word token\n",
    "\n",
    "# model download paths\n",
    "WORD2INDEX_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/word2index.json\"\n",
    "INDEX2WORD_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/index2word.json\"\n",
    "MODEL_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/craft_full.tar\"\n",
    "\n",
    "# confidence score threshold for declaring a positive prediction.\n",
    "# this value was previously learned on the validation set.\n",
    "FORECAST_THRESH = 0.570617"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_w8yFiXuCpg"
   },
   "source": [
    "## Part 1: set up data conversion utilities\n",
    "\n",
    "We begin by setting up some helper functions and classes for converting conversational text data into a torch-friendly Tensor format. Note that these low-level routines are largely taken from the [PyTorch seq2seq chatbot tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QM8PeUAduAhq"
   },
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    \"\"\"A class for representing the vocabulary used by a CRAFT model\"\"\"\n",
    "\n",
    "    def __init__(self, name, word2index=None, index2word=None):\n",
    "        self.name = name\n",
    "        self.trimmed = False if not word2index else True # if a precomputed vocab is specified assume the user wants to use it as-is\n",
    "        self.word2index = word2index if word2index else {\"UNK\": UNK_token}\n",
    "        self.word2count = {}\n",
    "        self.index2word = index2word if index2word else {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4 if not index2word else len(index2word)  # Count SOS, EOS, PAD, UNK\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {\"UNK\": UNK_token}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "\n",
    "# Create a Voc object from precomputed data structures\n",
    "def loadPrecomputedVoc(corpus_name, word2index_url, index2word_url):\n",
    "    # load the word-to-index lookup map\n",
    "    r = requests.get(word2index_url)\n",
    "    word2index = r.json()\n",
    "    # load the index-to-word lookup map\n",
    "    r = requests.get(index2word_url)\n",
    "    index2word = r.json()\n",
    "    return Voc(corpus_name, word2index, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SazSyux7KFS"
   },
   "outputs": [],
   "source": [
    "# Helper functions for preprocessing and tokenizing text\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Tokenize the string using NLTK\n",
    "def tokenize(text):\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(pattern=r'\\w+|[^\\w\\s]')\n",
    "    # simplify the problem space by considering only ASCII data\n",
    "    cleaned_text = unicodeToAscii(text.lower())\n",
    "\n",
    "    # if the resulting string is empty, nothing else to do\n",
    "    if not cleaned_text.strip():\n",
    "        return []\n",
    "    \n",
    "    return tokenizer.tokenize(cleaned_text)\n",
    "\n",
    "# Given a ConvoKit conversation, preprocess each utterance's text by tokenizing and truncating.\n",
    "# Returns the processed dialog entry where text has been replaced with a list of\n",
    "# tokens, each no longer than MAX_LENGTH - 1 (to leave space for the EOS token)\n",
    "def processDialog(voc, dialog):\n",
    "    processed = []\n",
    "    for utterance in dialog.iter_utterances():\n",
    "        # skip the section header, which does not contain conversational content\n",
    "#         if utterance.meta['is_section_header']:\n",
    "#             continue\n",
    "        tokens = tokenize(utterance.text)\n",
    "        # replace out-of-vocabulary tokens\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] not in voc.word2index:\n",
    "                tokens[i] = \"UNK\"\n",
    "        processed.append({\"tokens\": tokens, \"id\": utterance.id})\n",
    "#         processed.append({\"tokens\": tokens, \"is_attack\": int(utterance.meta['comment_has_personal_attack']), \"id\": utterance.id})\n",
    "    return processed\n",
    "\n",
    "# Load context-reply pairs from the Corpus, optionally filtering to only conversations\n",
    "# from the specified split (train, val, or test).\n",
    "# Each conversation, which has N comments (not including the section header) will\n",
    "# get converted into N-1 comment-reply pairs, one pair for each reply \n",
    "# (the first comment does not reply to anything).\n",
    "# Each comment-reply pair is a tuple consisting of the conversational context\n",
    "# (that is, all comments prior to the reply), the reply itself, the label (that\n",
    "# is, whether the reply contained a derailment event), and the comment ID of the\n",
    "# reply (for later use in re-joining with the ConvoKit corpus).\n",
    "# The function returns a list of such pairs.\n",
    "def loadPairs(voc, corpus, split=None):\n",
    "    pairs = []\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # consider only conversations in the specified split of the data\n",
    "        if split is None or convo.meta['split'] == split:\n",
    "            dialog = processDialog(voc, convo)\n",
    "            for idx in range(1, len(dialog)):\n",
    "                reply = dialog[idx][\"tokens\"][:(MAX_LENGTH-1)]\n",
    "#                 label = dialog[idx][\"is_attack\"]\n",
    "                comment_id = dialog[idx][\"id\"]\n",
    "                # gather as context all utterances preceding the reply\n",
    "                context = [u[\"tokens\"][:(MAX_LENGTH-1)] for u in dialog[:idx]]\n",
    "                pairs.append((context, reply, comment_id))\n",
    "#                 pairs.append((context, reply, label, comment_id))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo4Mg7AEK-0Q"
   },
   "outputs": [],
   "source": [
    "# Helper functions for turning dialog and text sequences into tensors, and manipulating those tensors\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Takes a batch of dialogs (lists of lists of tokens) and converts it into a\n",
    "# batch of utterances (lists of tokens) sorted by length, while keeping track of\n",
    "# the information needed to reconstruct the original batch of dialogs\n",
    "def dialogBatch2UtteranceBatch(dialog_batch):\n",
    "    utt_tuples = [] # will store tuples of (utterance, original position in batch, original position in dialog)\n",
    "    for batch_idx in range(len(dialog_batch)):\n",
    "        dialog = dialog_batch[batch_idx]\n",
    "        for dialog_idx in range(len(dialog)):\n",
    "            utterance = dialog[dialog_idx]\n",
    "            utt_tuples.append((utterance, batch_idx, dialog_idx))\n",
    "    # sort the utterances in descending order of length, to remain consistent with pytorch padding requirements\n",
    "    utt_tuples.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    # return the utterances, original batch indices, and original dialog indices as separate lists\n",
    "    utt_batch = [u[0] for u in utt_tuples]\n",
    "    batch_indices = [u[1] for u in utt_tuples]\n",
    "    dialog_indices = [u[2] for u in utt_tuples]\n",
    "    return utt_batch, batch_indices, dialog_indices\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch, already_sorted=False):\n",
    "    if not already_sorted:\n",
    "        pair_batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    input_batch, output_batch, label_batch, id_batch = [], [], [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "#         label_batch.append(pair[2])\n",
    "        id_batch.append(pair[2])\n",
    "    dialog_lengths = torch.tensor([len(x) for x in input_batch])\n",
    "    input_utterances, batch_indices, dialog_indices = dialogBatch2UtteranceBatch(input_batch)\n",
    "    inp, utt_lengths = inputVar(input_utterances, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "#     label_batch = torch.FloatTensor(label_batch) if label_batch[0] is not None else None\n",
    "    return inp, dialog_lengths, utt_lengths, batch_indices, dialog_indices, id_batch, output, mask, max_target_len\n",
    "\n",
    "def batchIterator(voc, source_data, batch_size, shuffle=True):\n",
    "    cur_idx = 0\n",
    "    if shuffle:\n",
    "        random.shuffle(source_data)\n",
    "    while True:\n",
    "        if cur_idx >= len(source_data):\n",
    "            cur_idx = 0\n",
    "            if shuffle:\n",
    "                random.shuffle(source_data)\n",
    "        batch = source_data[cur_idx:(cur_idx+batch_size)]\n",
    "        # the true batch size may be smaller than the given batch size if there is not enough data left\n",
    "        true_batch_size = len(batch)\n",
    "        # ensure that the dialogs in this batch are sorted by length, as expected by the padding module\n",
    "        batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        # for analysis purposes, get the source dialogs and labels associated with this batch\n",
    "        batch_dialogs = [x[0] for x in batch]\n",
    "        batch_labels = [x[2] for x in batch]\n",
    "        # convert batch to tensors\n",
    "        batch_tensors = batch2TrainData(voc, batch, already_sorted=True)\n",
    "        yield (batch_tensors, batch_dialogs, batch_labels, true_batch_size) \n",
    "        cur_idx += batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_ev-7g-xsGQ"
   },
   "source": [
    "## Part 2: load the data\n",
    "\n",
    "Now we load the labeled Wikiconv corpus from ConvoKit, and run some transformations to prepare it for use with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y96SXcp4x1yj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-Cornell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.filter_conversations_by(lambda convo: convo.meta['timestamp'] >= 1538352000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPySt5a4yLId"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4388\n",
      "972\n",
      "634\n"
     ]
    }
   ],
   "source": [
    "# let's check some quick stats to verify that the corpus loaded correctly\n",
    "print(len(corpus.get_utterance_ids()))\n",
    "print(len(corpus.get_usernames()))\n",
    "print(len(corpus.get_conversation_ids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeNfs0A-yosu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_owner': <convokit.model.corpus.Corpus object at 0x12f5feac8>, '_id': '9kblzt', '_utterance_ids': ['9kblzt', 'e6xxenu', 'e6xxorn', 'e6yzs62', 'e6zaygs'], '_usernames': None, '_meta': {'title': 'Help cals credit 11?', 'num_comments': 4, 'domain': 'self.Cornell', 'timestamp': 1538354174, 'subreddit': 'Cornell', 'gilded': 0, 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0}, 'stickied': False, 'author_flair_text': ''}}\n",
      "Utterance({'id': '9kblzt', 'user': User([('name', 'Lilsquirtz')]), 'root': '9kblzt', 'reply_to': None, 'timestamp': 1538354174, 'text': 'Help what happens if I decide to drop a class, which means I go under 12 credits?', 'meta': {'score': 6, 'top_level_comment': None, 'retrieved_on': 1542537027, 'gilded': 0, 'gildings': {'gid_1': 0, 'gid_2': 0, 'gid_3': 0}, 'subreddit': 'Cornell', 'stickied': False, 'permalink': '/r/Cornell/comments/9kblzt/help_cals_credit_11/', 'author_flair_text': ''}})\n"
     ]
    }
   ],
   "source": [
    "# Let's also take a look at some example data to see what kinds of information/metadata are available to us\n",
    "print(list(corpus.iter_conversations())[0].__dict__)\n",
    "print(list(corpus.iter_utterances())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3QIoTtbzOfY"
   },
   "source": [
    "Now we can use the utilities defined in Part 1 to convert the ConvoKit conversational data into a tokenized form that can be straightforwardly turned into Tensors later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlpbT72FxK8W"
   },
   "outputs": [],
   "source": [
    "# First, we need to build the vocabulary so that we know how to map tokens to tensor indicies.\n",
    "# For the sake of replicating the paper results, we will load the pre-computed vocabulary objects used in the paper.\n",
    "voc = loadPrecomputedVoc(\"wikiconv\", WORD2INDEX_URL, INDEX2WORD_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "eRSAVrgSxhmD",
    "outputId": "8db8b58e-08c9-4f7c-b00a-4765151be5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50004\n",
      "[('UNK', 3), ('.', 4), ('the', 5), (\"'\", 6), (',', 7), ('to', 8), ('i', 9), ('of', 10), ('a', 11), ('and', 12)]\n",
      "[('0', 'PAD'), ('1', 'SOS'), ('2', 'EOS'), ('3', 'UNK'), ('4', '.'), ('5', 'the'), ('6', \"'\"), ('7', ','), ('8', 'to'), ('9', 'i')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the Voc object to make sure it loaded correctly\n",
    "print(voc.num_words) # expected vocab size is 50004: it was built using a fixed vocab size of 50k plus 4 spots for special tokens PAD, SOS, EOS, and UNK.\n",
    "print(list(voc.word2index.items())[:10])\n",
    "print(list(voc.index2word.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmaWfn7vyTjy"
   },
   "outputs": [],
   "source": [
    "# Convert the test set data into a list of input/label pairs. Each input will represent the conversation as a list of lists of tokens.\n",
    "test_pairs = loadPairs(voc, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "RIiQNvg4zBEi",
    "outputId": "a38e339f-8edf-4d60-c645-0239a97d2c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3754\n"
     ]
    }
   ],
   "source": [
    "# Validate the conversion by checking data size and some samples\n",
    "print(len(test_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 972\n",
      "Number of Utterances: 4388\n",
      "Number of Conversations: 634\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['help',\n",
       "   'what',\n",
       "   'happens',\n",
       "   'if',\n",
       "   'i',\n",
       "   'decide',\n",
       "   'to',\n",
       "   'drop',\n",
       "   'a',\n",
       "   'class',\n",
       "   ',',\n",
       "   'which',\n",
       "   'means',\n",
       "   'i',\n",
       "   'go',\n",
       "   'under',\n",
       "   '12',\n",
       "   'credits',\n",
       "   '?'],\n",
       "  ['you',\n",
       "   'have',\n",
       "   'to',\n",
       "   'petition',\n",
       "   'to',\n",
       "   'remain',\n",
       "   'a',\n",
       "   'full',\n",
       "   '-',\n",
       "   'time',\n",
       "   'student']],\n",
       " ['petition',\n",
       "  'before',\n",
       "  'dropping',\n",
       "  '.',\n",
       "  'this',\n",
       "  'is',\n",
       "  'a',\n",
       "  'case',\n",
       "  'where',\n",
       "  'permission',\n",
       "  'is',\n",
       "  'better',\n",
       "  'than',\n",
       "  'forgiveness',\n",
       "  '.'],\n",
       " 'e6xxorn')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghTVeRFK0CUe"
   },
   "source": [
    "## Part 3: define the model\n",
    "\n",
    "Next, we need to set up the individual components of the CRAFT framework: the utterance encoder, the context encoder, and the classification head (since we are only running inference on an already-trained model, we do not need to define the decoder here). Each component will be defined as a PyTorch `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzAgSRKi0p_I"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"This module represents the utterance encoder component of CRAFT, responsible for creating vector representations of utterances\"\"\"\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "class ContextEncoderRNN(nn.Module):\n",
    "    \"\"\"This module represents the context encoder component of CRAFT, responsible for creating an order-sensitive vector representation of conversation context\"\"\"\n",
    "    def __init__(self, hidden_size, n_layers=1, dropout=0):\n",
    "        super(ContextEncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # only unidirectional GRU for context encoding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=False)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "class SingleTargetClf(nn.Module):\n",
    "    \"\"\"This module represents the CRAFT classifier head, which takes the context encoding and uses it to make a forecast\"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(SingleTargetClf, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # initialize classifier\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer1_act = nn.LeakyReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.layer2_act = nn.LeakyReLU()\n",
    "        self.clf = nn.Linear(hidden_size // 2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, encoder_outputs, encoder_input_lengths):\n",
    "        # from stackoverflow (https://stackoverflow.com/questions/50856936/taking-the-last-state-from-bilstm-bigru-in-pytorch)\n",
    "        # First we unsqueeze seqlengths two times so it has the same number of\n",
    "        # of dimensions as output_forward\n",
    "        # (batch_size) -> (1, batch_size, 1)\n",
    "        lengths = encoder_input_lengths.unsqueeze(0).unsqueeze(2)\n",
    "        # Then we expand it accordingly\n",
    "        # (1, batch_size, 1) -> (1, batch_size, hidden_size) \n",
    "        lengths = lengths.expand((1, -1, encoder_outputs.size(2)))\n",
    "\n",
    "        # take only the last state of the encoder for each batch\n",
    "        last_outputs = torch.gather(encoder_outputs, 0, lengths-1).squeeze()\n",
    "        # forward pass through hidden layers\n",
    "        layer1_out = self.layer1_act(self.layer1(self.dropout(last_outputs)))\n",
    "        layer2_out = self.layer2_act(self.layer2(self.dropout(layer1_out)))\n",
    "        # compute and return logits\n",
    "        logits = self.clf(self.dropout(layer2_out)).squeeze()\n",
    "        return logits\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    \"\"\"This helper module encapsulates the CRAFT pipeline, defining the logic of passing an input through each consecutive sub-module.\"\"\"\n",
    "    def __init__(self, encoder, context_encoder, classifier):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.context_encoder = context_encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, input_batch, dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, batch_size, max_length):\n",
    "        # Forward input through encoder model\n",
    "        _, utt_encoder_hidden = self.encoder(input_batch, utt_lengths)\n",
    "        \n",
    "        # Convert utterance encoder final states to batched dialogs for use by context encoder\n",
    "        context_encoder_input = makeContextEncoderInput(utt_encoder_hidden, dialog_lengths_list, batch_size, batch_indices, dialog_indices)\n",
    "        \n",
    "        # Forward pass through context encoder\n",
    "        context_encoder_outputs, context_encoder_hidden = self.context_encoder(context_encoder_input, dialog_lengths)\n",
    "        \n",
    "        # Forward pass through classifier to get prediction logits\n",
    "        logits = self.classifier(context_encoder_outputs, dialog_lengths)\n",
    "        \n",
    "        # Apply sigmoid activation\n",
    "        \n",
    "        predictions = .sigmoid(logits)\n",
    "        return predictions\n",
    "\n",
    "def makeContextEncoderInput(utt_encoder_hidden, dialog_lengths, batch_size, batch_indices, dialog_indices):\n",
    "    \"\"\"The utterance encoder takes in utterances in combined batches, with no knowledge of which ones go where in which conversation.\n",
    "       Its output is therefore also unordered. We correct this by using the information computed during tensor conversion to regroup\n",
    "       the utterance vectors into their proper conversational order.\"\"\"\n",
    "    # first, sum the forward and backward encoder states\n",
    "    utt_encoder_summed = utt_encoder_hidden[-2,:,:] + utt_encoder_hidden[-1,:,:]\n",
    "    # we now have hidden state of shape [utterance_batch_size, hidden_size]\n",
    "    # split it into a list of [hidden_size,] x utterance_batch_size\n",
    "    last_states = [t.squeeze() for t in utt_encoder_summed.split(1, dim=0)]\n",
    "    \n",
    "    # create a placeholder list of tensors to group the states by source dialog\n",
    "    states_dialog_batched = [[None for _ in range(dialog_lengths[i])] for i in range(batch_size)]\n",
    "    \n",
    "    # group the states by source dialog\n",
    "    for hidden_state, batch_idx, dialog_idx in zip(last_states, batch_indices, dialog_indices):\n",
    "        states_dialog_batched[batch_idx][dialog_idx] = hidden_state\n",
    "        \n",
    "    # stack each dialog into a tensor of shape [dialog_length, hidden_size]\n",
    "    states_dialog_batched = [torch.stack(d) for d in states_dialog_batched]\n",
    "    \n",
    "    # finally, condense all the dialog tensors into a single zero-padded tensor\n",
    "    # of shape [max_dialog_length, batch_size, hidden_size]\n",
    "    return torch.nn.utils.rnn.pad_sequence(states_dialog_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eG1p_cH5fw9"
   },
   "source": [
    "## Part 4: define the evaluation procedure\n",
    "\n",
    "We're almost ready to run! The last component we need is some code for actually running the data through the full CRAFT pipeline. This includes actually converting the text input into tensors (we use the utilities defined in Part 1 to do this) then passing the resulting tensors through each CRAFT module batch-by-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZqEcwEJ5bqn"
   },
   "outputs": [],
   "source": [
    "def evaluateBatch(encoder, context_encoder, predictor, voc, input_batch, dialog_lengths, \n",
    "                  dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, batch_size, device, max_length=MAX_LENGTH):\n",
    "    # Set device options\n",
    "    input_batch = input_batch.to(device)\n",
    "    dialog_lengths = dialog_lengths.to(device)\n",
    "    utt_lengths = utt_lengths.to(device)\n",
    "    # Predict future attack using predictor\n",
    "    scores = predictor(input_batch, dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices, batch_size, max_length)\n",
    "    predictions = (scores > 0.5).float()\n",
    "    return predictions, scores\n",
    "\n",
    "def evaluateDataset(dataset, encoder, context_encoder, predictor, voc, batch_size, device):\n",
    "    # create a batch iterator for the given data\n",
    "    batch_iterator = batchIterator(voc, dataset, batch_size, shuffle=False)\n",
    "    # find out how many iterations we will need to cover the whole dataset\n",
    "    n_iters = len(dataset) // batch_size + int(len(dataset) % batch_size > 0)\n",
    "    output_df = {\n",
    "        \"id\": [],\n",
    "        \"prediction\": [],\n",
    "        \"score\": []\n",
    "    }\n",
    "    for iteration in range(1, n_iters+1):\n",
    "        batch, batch_dialogs, _, true_batch_size = next(batch_iterator)\n",
    "        # Extract fields from batch\n",
    "        input_variable, dialog_lengths, utt_lengths, batch_indices, dialog_indices, convo_ids, target_variable, mask, max_target_len = batch\n",
    "        dialog_lengths_list = [len(x) for x in batch_dialogs]\n",
    "        # run the model\n",
    "        # predictions, scores\n",
    "        predictions, scores = evaluateBatch(encoder, context_encoder, predictor, voc, input_variable,\n",
    "                                            dialog_lengths, dialog_lengths_list, utt_lengths, batch_indices, dialog_indices,\n",
    "                                            true_batch_size, device)\n",
    "\n",
    "        # format the output as a dataframe (which we can later re-join with the corpus)\n",
    "        for i in range(true_batch_size):\n",
    "            convo_id = convo_ids[i]\n",
    "            pred = predictions[i].item()\n",
    "            score = scores[i].item()\n",
    "            output_df[\"id\"].append(convo_id)\n",
    "            output_df[\"prediction\"].append(pred)\n",
    "            output_df[\"score\"].append(score)\n",
    "                \n",
    "        print(\"Iteration: {}; Percent complete: {:.1f}%\".format(iteration, iteration / n_iters * 100))\n",
    "\n",
    "    return pd.DataFrame(output_df).set_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Na5gjZGE-KA0"
   },
   "source": [
    "## Part 5: build and run the model\n",
    "\n",
    "We finally have all the components we need! Now we can instantiate the CRAFT model components, load the trained weights, and run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2551GR65-Wm5",
    "outputId": "d836b623-b9d2-4069-abf1-5c8848a282c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved parameters...\n",
      "\tDownloading trained CRAFT...\n",
      "\t...Done!\n",
      "Building encoders, decoder, and classifier...\n",
      "Models built and ready to go!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1; Percent complete: 1.7%\n",
      "Iteration: 2; Percent complete: 3.4%\n",
      "Iteration: 3; Percent complete: 5.1%\n",
      "Iteration: 4; Percent complete: 6.8%\n",
      "Iteration: 5; Percent complete: 8.5%\n",
      "Iteration: 6; Percent complete: 10.2%\n",
      "Iteration: 7; Percent complete: 11.9%\n",
      "Iteration: 8; Percent complete: 13.6%\n",
      "Iteration: 9; Percent complete: 15.3%\n",
      "Iteration: 10; Percent complete: 16.9%\n",
      "Iteration: 11; Percent complete: 18.6%\n",
      "Iteration: 12; Percent complete: 20.3%\n",
      "Iteration: 13; Percent complete: 22.0%\n",
      "Iteration: 14; Percent complete: 23.7%\n",
      "Iteration: 15; Percent complete: 25.4%\n",
      "Iteration: 16; Percent complete: 27.1%\n",
      "Iteration: 17; Percent complete: 28.8%\n",
      "Iteration: 18; Percent complete: 30.5%\n",
      "Iteration: 19; Percent complete: 32.2%\n",
      "Iteration: 20; Percent complete: 33.9%\n",
      "Iteration: 21; Percent complete: 35.6%\n",
      "Iteration: 22; Percent complete: 37.3%\n",
      "Iteration: 23; Percent complete: 39.0%\n",
      "Iteration: 24; Percent complete: 40.7%\n",
      "Iteration: 25; Percent complete: 42.4%\n",
      "Iteration: 26; Percent complete: 44.1%\n",
      "Iteration: 27; Percent complete: 45.8%\n",
      "Iteration: 28; Percent complete: 47.5%\n",
      "Iteration: 29; Percent complete: 49.2%\n",
      "Iteration: 30; Percent complete: 50.8%\n",
      "Iteration: 31; Percent complete: 52.5%\n",
      "Iteration: 32; Percent complete: 54.2%\n",
      "Iteration: 33; Percent complete: 55.9%\n",
      "Iteration: 34; Percent complete: 57.6%\n",
      "Iteration: 35; Percent complete: 59.3%\n",
      "Iteration: 36; Percent complete: 61.0%\n",
      "Iteration: 37; Percent complete: 62.7%\n",
      "Iteration: 38; Percent complete: 64.4%\n",
      "Iteration: 39; Percent complete: 66.1%\n",
      "Iteration: 40; Percent complete: 67.8%\n",
      "Iteration: 41; Percent complete: 69.5%\n",
      "Iteration: 42; Percent complete: 71.2%\n",
      "Iteration: 43; Percent complete: 72.9%\n",
      "Iteration: 44; Percent complete: 74.6%\n",
      "Iteration: 45; Percent complete: 76.3%\n",
      "Iteration: 46; Percent complete: 78.0%\n",
      "Iteration: 47; Percent complete: 79.7%\n",
      "Iteration: 48; Percent complete: 81.4%\n",
      "Iteration: 49; Percent complete: 83.1%\n",
      "Iteration: 50; Percent complete: 84.7%\n",
      "Iteration: 51; Percent complete: 86.4%\n",
      "Iteration: 52; Percent complete: 88.1%\n",
      "Iteration: 53; Percent complete: 89.8%\n",
      "Iteration: 54; Percent complete: 91.5%\n",
      "Iteration: 55; Percent complete: 93.2%\n",
      "Iteration: 56; Percent complete: 94.9%\n",
      "Iteration: 57; Percent complete: 96.6%\n",
      "Iteration: 58; Percent complete: 98.3%\n",
      "Iteration: 59; Percent complete: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Fix random state for reproducibility\n",
    "random.seed(2019)\n",
    "\n",
    "# Tell torch to use GPU. Note that if you are running this notebook in a non-GPU environment, you can change 'cuda' to 'cpu' to get the code to run.\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(\"Loading saved parameters...\")\n",
    "if not os.path.isfile(\"model.tar\"):\n",
    "    print(\"\\tDownloading trained CRAFT...\")\n",
    "    urlretrieve(MODEL_URL, \"model.tar\")\n",
    "    print(\"\\t...Done!\")\n",
    "# checkpoint = torch.load(\"model.tar\")\n",
    "# If running in a non-GPU environment, you need to tell PyTorch to convert the parameters to CPU tensor format.\n",
    "# To do so, replace the previous line with the following:\n",
    "checkpoint = torch.load(\"model.tar\", map_location=torch.device('cpu'))\n",
    "encoder_sd = checkpoint['en']\n",
    "context_sd = checkpoint['ctx']\n",
    "attack_clf_sd = checkpoint['atk_clf']\n",
    "embedding_sd = checkpoint['embedding']\n",
    "voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "print('Building encoders, decoder, and classifier...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "embedding.load_state_dict(embedding_sd)\n",
    "# Initialize utterance and context encoders\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "context_encoder = ContextEncoderRNN(hidden_size, context_encoder_n_layers, dropout)\n",
    "encoder.load_state_dict(encoder_sd)\n",
    "context_encoder.load_state_dict(context_sd)\n",
    "# Initialize classifier\n",
    "attack_clf = SingleTargetClf(hidden_size, dropout)\n",
    "attack_clf.load_state_dict(attack_clf_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "context_encoder = context_encoder.to(device)\n",
    "attack_clf = attack_clf.to(device)\n",
    "print('Models built and ready to go!')\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "context_encoder.eval()\n",
    "attack_clf.eval()\n",
    "\n",
    "# Initialize the pipeline\n",
    "predictor = Predictor(encoder, context_encoder, attack_clf)\n",
    "\n",
    "# Run the pipeline!\n",
    "forecasts_df = evaluateDataset(test_pairs, encoder, context_encoder, predictor, voc, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e8q95r5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8qfvt3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8qcoi9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8pjejh</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8q9pj0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8qbynq</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8qdaph</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8qe2p3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7b76jt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.979315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7rzhsu</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prediction     score\n",
       "id                           \n",
       "e8q95r5         1.0  0.983908\n",
       "e8qfvt3         1.0  0.981553\n",
       "e8qcoi9         1.0  0.980987\n",
       "e8pjejh         1.0  0.980831\n",
       "e8q9pj0         1.0  0.980752\n",
       "e8qbynq         1.0  0.980654\n",
       "e8qdaph         1.0  0.980264\n",
       "e8qe2p3         1.0  0.979371\n",
       "e7b76jt         1.0  0.979315\n",
       "e7rzhsu         1.0  0.977393"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts_df.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_scored_ids = forecasts_df.sort_values('score', ascending=False).head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s not my job to do your work for you. I asked for evidence, that’s all. If you can’t do the work then don’t make the claim.\n",
      "\n",
      "Obstruction and obstruction of justice are two different concepts. \n",
      "\n",
      "Evidence for a personal attack? \n",
      "\n",
      "You can simultaneously say “I would think about something differently if I were raised different” and “the way they think about this is immoral.” If something is wrong, it’s wrong\n",
      "\n",
      "Except you're the one asking for work to be done...\n",
      "\n",
      "Trump can't back up his claims on 90 percent of what he says and people still believe him. I'll just assume you're an aggie or spend too much time in Rand to understand how evidence works.\n",
      "\n",
      "No I’d be a hypocrite if I disagree with the claim. I never disagreed, I simply asked you for your evidence. You know, like when an attorney is certain their client is innocent but want to compile evidence anyway to make their case stronger. It doesn’t make the attorney a hypocrite for wanting all the evidence possible before making this own statement. It’s the wisest option to gather evidence before making a claim, not to make the claim then look for it.\n",
      "\n",
      "But whatever works for you I guess since you think yourself an exception.\n",
      "\n",
      "So I made a claim and had evidence and presented it when confronted, even though you also had the same knowledge and declined to put it forward. Sounds more like obstruction in your little world.\n",
      "\n",
      "I'm not sure why this is so confusing. Some people want to layer and get a cheaper jacket, that's fine. Some people want a certain brand or a certain type of parka that is more expensive, some can't afford something for $500 but can for $300. That is who this is for, not those who want an $80 jacket. It really isn't that intense\n",
      "\n",
      "[deleted]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for utt_id in highest_scored_ids:\n",
    "    print(corpus.get_utterance(utt_id).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_scored_ids = forecasts_df.sort_values('score', ascending=True).head(20).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e8s82ym</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8ryqad</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8rypin</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8rykx3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7tizoz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8ryhig</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8rydz0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8rycbu</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8rybro</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e82drcv</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e796nbp</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e86kaaz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8pbk7q</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7etyak</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7p2qq1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8pei12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8ry9lo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e81lo18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7grbua</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e8sr501</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prediction     score\n",
       "id                           \n",
       "e8s82ym         0.0  0.040840\n",
       "e8ryqad         0.0  0.041310\n",
       "e8rypin         0.0  0.042459\n",
       "e8rykx3         0.0  0.043252\n",
       "e7tizoz         0.0  0.044797\n",
       "e8ryhig         0.0  0.044992\n",
       "e8rydz0         0.0  0.046444\n",
       "e8rycbu         0.0  0.049498\n",
       "e8rybro         0.0  0.052667\n",
       "e82drcv         0.0  0.053639\n",
       "e796nbp         0.0  0.054059\n",
       "e86kaaz         0.0  0.055847\n",
       "e8pbk7q         0.0  0.056211\n",
       "e7etyak         0.0  0.056614\n",
       "e7p2qq1         0.0  0.058086\n",
       "e8pei12         0.0  0.058200\n",
       "e8ry9lo         0.0  0.058769\n",
       "e81lo18         0.0  0.060508\n",
       "e7grbua         0.0  0.064946\n",
       "e8sr501         0.0  0.065087"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts_df.sort_values('score', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Almost every comment in the last 12 hours has been harassing or insulting, locking the thread. \n",
      "\n",
      "Thanks!\n",
      "\n",
      "BSOC is in both A&amp;S and CALS, you can major in it in either college with the only difference being the college distribution requirements.\n",
      "\n",
      "I'd love to climb with someone as well! I have my belay cert and have a similar schedule as you :D\n",
      "\n",
      "No.  You apply through the app dev website at the start of the semester and it will start the second half of the semester.  Pre-enroll is only possible with courses that do not have applications (for example, none of the game dev courses allow pre-enroll either).\n",
      "\n",
      "&gt;Further, none of their citizens seem to give a fuck.\n",
      "\n",
      "Hey, I was born here in the US but lived in China for a couple of years. \u0014Chinese citizens are content with their government is because the standard of living has improved drastically over the past decades. People aren't dumb and do understand the faults in their government, but no one cares enough to risk themselves and their family. Avoiding trouble is deeply rooted in Chinese thinking and comes from thousands of years of emperor rule.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&gt;How many international Chinese students have you heard here shitting on the west, or spouting some rampant nationalistic garbage?\n",
      "\n",
      "On the other hand we're pretty proud of our identity as people of china, not people of the PRC. Both the U.S. and Chinese government has its faults. One aspect of China I'd like to root is that government officials aren't openly corrupt. If you fuck up in public, you'll likely be \"vanished\" or sent to prison. In the U.S. government officials can do whatever the fuck they want and as long as people vote for them, they remain in office. People used to praise the west and its strength, but now you have Trump in office and his image doesn't convey strength to the world.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "&gt;Some girl (from China) in a seminar discussion on China a few weeks ago was heard saying she thought that the internment and re-education of muslims was *okay.*\n",
      "\n",
      "This is another difference between Chinese and U.S. thinking. U.S. is a country built on immigrants and every religion is given equal status (at least on paper). This thinking is limited to the West and in China people don't give a shit about foreigners and their rights. In America, muslims are \"one of us\". In China, muslims are \"not Chinese\".\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "I'm no sociology major and my opinions are limited by my values, perspective, and information, but so is everyone else in this world. You might think Chinese people are cancer and rotten for disregarding muslims, spewing shit on the west, and that's because of your own perspective, perhaps influenced by the values you were told to accept as right, the media, etc. Chinese people grew up with different values and were fed different information. There is no right or wrong really. Again, I'm not even well-versed in this area, but I'm open to discussion.\u0014\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "Edit: formatting, spelling, and some extra words.\n",
      "\n",
      "combination of both - took 16-18 credits per semester my sophomore and junior years (w exception of semester abroad) and averaged like a 3.7. Took 14 credits each semester of senior year and averaged a 4.0. \n",
      "\n",
      "also helped that i was taking courses that i was genuinely interested in and that i stopped dicking around so much/rarely skipped class after freshman year. \n",
      "\n",
      "That's only offered in the fall and 1102 in the spring is way harder by comparison \n",
      "\n",
      "Not necessarily. You definitely need to study for it but it’s not as bad as the prelims \n",
      "\n",
      "The professor will write down what concepts he is covering and what textbook chapters it is from. I usually just started by reading the textbook, and then if there was something I didn't understand I just looked for videos/blog posts online that explained it. \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "Unfortunately, when I took it the professor didn't even have slides, he just typed pseudocode into a word document or excel... and I've heard slides from previous years have a lot of errors, so you're probably better off reading the textbook (which has the same pseudocode but better formatted)\n",
      "\n",
      "And I’m a 5’6” nerdy as fuck Asian who plays Pokémon Go in my free time.\n",
      "\n",
      "\n",
      "What the fuck is going on in this thread?!!??\n",
      "\n",
      "Probably a dumb question, but how do you check who’s a prof and who’s a grad student? I can’t figure it out from their profile pages on the English dept website\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for utt_id in lowest_scored_ids:\n",
    "    text = corpus.get_utterance(utt_id).text\n",
    "    if text == \"[removed]\": continue\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "colab_type": "code",
    "id": "lVK-1NWHEy7l",
    "outputId": "6cc4e2c8-9ac5-41ef-aa37-aa377104ce38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e76baep</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e750tzy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.570285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e721c5w</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e70gdct</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.587237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e70coh6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7acfbc</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6zy1hj</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7a4kqh</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e70jm55</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e721s2b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6zvwkq</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e70wqrf</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e72sqc0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.356380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6zaygs</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.754090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6zdxdf</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6z43m9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6zl1a9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6zsxme</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.559647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e70cffm</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.387807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e71o7bf</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.679522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         prediction     score\n",
       "id                           \n",
       "e76baep         0.0  0.495217\n",
       "e750tzy         1.0  0.570285\n",
       "e721c5w         0.0  0.488575\n",
       "e70gdct         1.0  0.587237\n",
       "e70coh6         0.0  0.491607\n",
       "e7acfbc         0.0  0.315236\n",
       "e6zy1hj         0.0  0.425934\n",
       "e7a4kqh         0.0  0.466913\n",
       "e70jm55         0.0  0.220556\n",
       "e721s2b         1.0  0.959971\n",
       "e6zvwkq         1.0  0.571487\n",
       "e70wqrf         0.0  0.458823\n",
       "e72sqc0         0.0  0.356380\n",
       "e6zaygs         1.0  0.754090\n",
       "e6zdxdf         0.0  0.210213\n",
       "e6z43m9         0.0  0.282691\n",
       "e6zl1a9         1.0  0.970225\n",
       "e6zsxme         1.0  0.559647\n",
       "e70cffm         0.0  0.387807\n",
       "e71o7bf         1.0  0.679522"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect some of the outputs as a sanity-check\n",
    "forecasts_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_EMZ7-SKtEP"
   },
   "source": [
    "## Part 6: merge predictions back into corpus and evaluate\n",
    "\n",
    "Now that the hard part is done, all that is left to do is to evaluate the predictions. Since the predictions are in no particular order, we will first merge each prediction back into the source corpus, and then evaluate each conversation according to the order of utterances within that conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vnjmtu-QLDVo"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b4184be37b6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconvo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_conversations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# only consider test set conversations (we did not make predictions for the other ones)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_utterances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforecasts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'split'"
     ]
    }
   ],
   "source": [
    "# We will add a metadata entry to each test-set utterance signifying whether it was FORECAST to be a derailment.\n",
    "# Note that there is an important subtlety in how this metadata field is to be interpreted - the forecast for a given\n",
    "# utterance is made BEFORE the model actually sees the utterance. That is, the forecast does not mean \"the model thinks\n",
    "# this utterance *is* a derailment\" but rather that \"based on the context of all preceding utterances, the model predicted,\n",
    "# prior to actually seeing this utterance, that this utterance *would be* a derailment\".\n",
    "for convo in corpus.iter_conversations():\n",
    "    # only consider test set conversations (we did not make predictions for the other ones)\n",
    "    if convo.meta['split'] == \"test\":\n",
    "        for utt in convo.iter_utterances():\n",
    "            if utt.id in forecasts_df.index:\n",
    "                utt.meta['forecast_score'] = forecasts_df.loc[utt.id].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FYxW_AuWszqX",
    "outputId": "b61bb3fe-37d5-4e59-879e-a0fc418906aa"
   },
   "outputs": [],
   "source": [
    "# Finally, we can use the forecast-annotated corpus to compute the forecast accuracy.\n",
    "# Though we have an individual forecast per utterance, ground truth is at the conversation level:\n",
    "# either a conversation derails or it does not. Thus, forecast accuracy is computed as follows:\n",
    "#   - True positives are cases that actually derail, for which the model made at least one positive forecast ANYTIME prior to derailment\n",
    "#   - False positives are cases that don't derail but for which the model made at least one positive forecast\n",
    "#   - False negatives are cases that derail but for which the model made no positive forecasts prior to derailment\n",
    "#   - True negatives are cases that don't derail, for which the model made no positive forecasts\n",
    "# Note that by construction, the last comment of each conversation is the one marked as derailment, and that our earlier code was therefore\n",
    "# set up to not look at the last comment, meaning that all forecasts we obtained are forecasts made prior to derailment. This simplifies\n",
    "# the computation of forecast accuracy as we now do not need to explicitly consider when a forecast was made.\n",
    "\n",
    "conversational_forecasts_df = {\n",
    "    \"convo_id\": [],\n",
    "    \"label\": [],\n",
    "    \"score\": [],\n",
    "    \"prediction\": []\n",
    "}\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta['split'] == \"test\":\n",
    "        conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "        conversational_forecasts_df['label'].append(int(convo.meta['conversation_has_personal_attack']))\n",
    "        forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "        conversational_forecasts_df['score'] = np.max(forecast_scores)\n",
    "        conversational_forecasts_df['prediction'].append(int(np.max(forecast_scores) > FORECAST_THRESH))\n",
    "\n",
    "conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "print((conversational_forecasts_df.label == conversational_forecasts_df.prediction).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "_49Yaz2FIo9S",
    "outputId": "b9a916f3-ab47-429e-8d80-c963ed92e381"
   },
   "outputs": [],
   "source": [
    "# in addition to accuracy, we can also consider applying other metrics at the conversation level, such as precision/recall\n",
    "def get_pr_stats(preds, labels):\n",
    "    tp = ((labels==1)&(preds==1)).sum()\n",
    "    fp = ((labels==0)&(preds==1)).sum()\n",
    "    tn = ((labels==0)&(preds==0)).sum()\n",
    "    fn = ((labels==1)&(preds==0)).sum()\n",
    "    print(\"Precision = {0:.4f}, recall = {1:.4f}\".format(tp / (tp + fp), tp / (tp + fn)))\n",
    "    print(\"False positive rate =\", fp / (fp + tn))\n",
    "    print(\"F1 =\", 2 / (((tp + fp) / tp) + ((tp + fn) / tp)))\n",
    "\n",
    "get_pr_stats(conversational_forecasts_df.prediction, conversational_forecasts_df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzBiI0dsW7WZ"
   },
   "source": [
    "## Part 7: model analysis: how early is early warning?\n",
    "\n",
    "The goal of CRAFT is to forecast outcomes in advance, but how far in advance does it typically make its prediction? Following the paper, we measure this in two ways: the number of *comments* between the first prediction and the actual derailment, and how much *elapsed time* that gap actually translates to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qfvl9k8Xesh"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'forecast_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-2f768526d51b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# now scan the utterances in order until we find the first derailment prediction (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mutts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'forecast_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mFORECAST_THRESH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;31m# recall that the forecast_score meta field specifies what CRAFT thought this comment would look like BEFORE it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# saw this comment. So the actual CRAFT forecast is made during the previous comment; we account for this by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'forecast_score'"
     ]
    }
   ],
   "source": [
    "comments_until_derail = {} # store the \"number of comments until derailment\" metric for each conversation\n",
    "time_until_derail = {} # store the \"time until derailment\" metric for each conversation\n",
    "\n",
    "for convo in corpus.iter_conversations():\n",
    "    # filter out the section header as usual\n",
    "    utts = list(convo.iter_utterances())\n",
    "    utts = [utt for utt in convo.iter_utterances() if not utt.id == utt.root]\n",
    "    # by construction, the last comment is the one with the personal attack\n",
    "    derail_idx = len(utts) - 1\n",
    "    # now scan the utterances in order until we find the first derailment prediction (if any)\n",
    "    for idx in range(1, len(utts)):\n",
    "        if utts[idx].meta['forecast_score'] > FORECAST_THRESH:\n",
    "            # recall that the forecast_score meta field specifies what CRAFT thought this comment would look like BEFORE it\n",
    "            # saw this comment. So the actual CRAFT forecast is made during the previous comment; we account for this by \n",
    "            # subtracting 1 from idx\n",
    "            comments_until_derail[convo.id] = derail_idx - (idx-1)\n",
    "            time_until_derail[convo.id] = utts[derail_idx].timestamp - utts[(idx-1)].timestamp\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IFXn4LrMhJ8W",
    "outputId": "e6476c9f-205e-4a3a-d562-1ae6d46875e5"
   },
   "outputs": [],
   "source": [
    "# compute some quick statistics about the distribution of the \"number of comments until derailment\" metric\n",
    "comments_until_derail_vals = np.asarray(list(comments_until_derail.values()))\n",
    "print(np.min(comments_until_derail_vals), np.max(comments_until_derail_vals), np.median(comments_until_derail_vals), np.mean(comments_until_derail_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7cTdzAuLhuHF",
    "outputId": "c1da699c-3eb6-4721-bc03-e1eadb07688e"
   },
   "outputs": [],
   "source": [
    "# compute some quick statistics about the distribution of the \"time until derailment\" metric\n",
    "# note that since timestamps are in seconds, we convert to hours by dividing by 3600, to make it more human readable\n",
    "time_until_derail_vals = np.asarray(list(time_until_derail.values())) / 3600\n",
    "print(np.min(time_until_derail_vals), np.max(time_until_derail_vals), np.median(time_until_derail_vals), np.mean(time_until_derail_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "_w3l6UxDiDAz",
    "outputId": "0e4ea2c4-7dcb-4655-eb8a-b7bd3106a5d5"
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of \"number of comments until derailment\" as a histogram (reproducing Figure 4 from the paper)\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "plt.rcParams['font.size'] = 24\n",
    "plt.hist(comments_until_derail_vals, bins=range(1, np.max(comments_until_derail_vals)), density=True)\n",
    "plt.xlim(1,10)\n",
    "plt.xticks(np.arange(1,10)+0.5, np.arange(1,10))\n",
    "plt.yticks(np.arange(0,0.25,0.05), np.arange(0,25,5))\n",
    "plt.xlabel(\"Number of comments elapsed\")\n",
    "plt.ylabel(\"% of conversations\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CRAFT inference demo using ConvoKit",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
