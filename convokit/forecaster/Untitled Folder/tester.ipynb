{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download('subreddit-Cornell'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 7568\n",
      "Number of Utterances: 74467\n",
      "Number of Conversations: 10744\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list = []\n",
    "idx = 0\n",
    "for utt in corpus.iter_utterances():\n",
    "    idx+=1\n",
    "    if idx == 10:\n",
    "        break\n",
    "    tokenized_list.append(word_tokenize(utt.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I',\n",
       "  'was',\n",
       "  'just',\n",
       "  'reading',\n",
       "  'about',\n",
       "  'the',\n",
       "  'Princeton',\n",
       "  'Mic-Check',\n",
       "  'and',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'getting',\n",
       "  '[',\n",
       "  'national',\n",
       "  'press',\n",
       "  ']',\n",
       "  '(',\n",
       "  'http',\n",
       "  ':',\n",
       "  '//www.bloomberg.com/news/2011-12-29/princeton-brews-trouble-for-us-1-percenters-commentary-by-michael-lewis.html',\n",
       "  ')',\n",
       "  '.',\n",
       "  'I',\n",
       "  'want',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'sense',\n",
       "  'of',\n",
       "  'what',\n",
       "  'people',\n",
       "  'felt',\n",
       "  'like',\n",
       "  'around',\n",
       "  'campus',\n",
       "  '.',\n",
       "  'Anything',\n",
       "  'interesting',\n",
       "  'happen',\n",
       "  '?',\n",
       "  'Anything',\n",
       "  'interesting',\n",
       "  'coming',\n",
       "  'up',\n",
       "  '?'],\n",
       " ['I',\n",
       "  'have',\n",
       "  'added',\n",
       "  'support',\n",
       "  'for',\n",
       "  'Cornell',\n",
       "  'to',\n",
       "  'courseoff.com',\n",
       "  '(',\n",
       "  'https',\n",
       "  ':',\n",
       "  '//cornell.courseoff.com',\n",
       "  ')',\n",
       "  '.',\n",
       "  'Courseoff',\n",
       "  'is',\n",
       "  'a',\n",
       "  'free',\n",
       "  'web',\n",
       "  'app',\n",
       "  'to',\n",
       "  'help',\n",
       "  'you',\n",
       "  'plan',\n",
       "  'your',\n",
       "  'semester',\n",
       "  'schedules',\n",
       "  '.',\n",
       "  'It',\n",
       "  'is',\n",
       "  'very',\n",
       "  'popular',\n",
       "  'with',\n",
       "  'students',\n",
       "  'at',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'other',\n",
       "  'schools',\n",
       "  'I',\n",
       "  'support',\n",
       "  '.',\n",
       "  'No',\n",
       "  'signup',\n",
       "  'is',\n",
       "  'required',\n",
       "  'to',\n",
       "  'use',\n",
       "  'it',\n",
       "  'so',\n",
       "  'feel',\n",
       "  'free',\n",
       "  'to',\n",
       "  'try',\n",
       "  'it',\n",
       "  'out',\n",
       "  '!',\n",
       "  'You',\n",
       "  'can',\n",
       "  'create',\n",
       "  'an',\n",
       "  'account',\n",
       "  'which',\n",
       "  'allows',\n",
       "  'multiple',\n",
       "  'schedules',\n",
       "  ',',\n",
       "  'saving',\n",
       "  'schedules',\n",
       "  ',',\n",
       "  'and',\n",
       "  'sharing',\n",
       "  'schedules',\n",
       "  '.',\n",
       "  'Let',\n",
       "  'me',\n",
       "  'know',\n",
       "  'what',\n",
       "  'you',\n",
       "  'guys',\n",
       "  'think',\n",
       "  '!',\n",
       "  'Any',\n",
       "  'feedback',\n",
       "  'is',\n",
       "  'always',\n",
       "  'appreciated',\n",
       "  '.',\n",
       "  'If',\n",
       "  'you',\n",
       "  'like',\n",
       "  'it',\n",
       "  ',',\n",
       "  'tell',\n",
       "  'your',\n",
       "  'friends',\n",
       "  ':',\n",
       "  ')',\n",
       "  'If',\n",
       "  'you',\n",
       "  'find',\n",
       "  'a',\n",
       "  'problem',\n",
       "  ',',\n",
       "  'let',\n",
       "  'me',\n",
       "  'know',\n",
       "  'as',\n",
       "  'well',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'have',\n",
       "  'a',\n",
       "  'facebook',\n",
       "  ',',\n",
       "  'so',\n",
       "  'we',\n",
       "  \"'d\",\n",
       "  'need',\n",
       "  'a',\n",
       "  'volunteer..',\n",
       "  'just',\n",
       "  'someone',\n",
       "  'to',\n",
       "  'let',\n",
       "  'cornell',\n",
       "  'on',\n",
       "  'facebook',\n",
       "  'know',\n",
       "  'that',\n",
       "  'we',\n",
       "  'have',\n",
       "  'a',\n",
       "  'presence',\n",
       "  'on',\n",
       "  'reddit..',\n",
       "  'perhaps',\n",
       "  'a',\n",
       "  'small',\n",
       "  'explanation',\n",
       "  'of',\n",
       "  'what',\n",
       "  'reddit',\n",
       "  'is',\n",
       "  '?',\n",
       "  'now',\n",
       "  'that',\n",
       "  'we',\n",
       "  'are',\n",
       "  'almost',\n",
       "  'beautiful',\n",
       "  'and',\n",
       "  'such..',\n",
       "  'we',\n",
       "  'need',\n",
       "  'more',\n",
       "  'redditors',\n",
       "  '!'],\n",
       " ['so',\n",
       "  ',',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'starting',\n",
       "  'to',\n",
       "  'mess',\n",
       "  'with',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'css',\n",
       "  'on',\n",
       "  'our',\n",
       "  'lovely',\n",
       "  'subreddit..',\n",
       "  'anyone',\n",
       "  'have',\n",
       "  'any',\n",
       "  'fun',\n",
       "  'suggestions',\n",
       "  'about',\n",
       "  'our',\n",
       "  'little',\n",
       "  'envelope',\n",
       "  '?',\n",
       "  'or',\n",
       "  'up/downvote',\n",
       "  'things',\n",
       "  '?',\n",
       "  'GO',\n",
       "  'NUTS',\n",
       "  '.'],\n",
       " [],\n",
       " ['Ever',\n",
       "  'since',\n",
       "  'SOPA',\n",
       "  'put',\n",
       "  'fear',\n",
       "  'into',\n",
       "  'the',\n",
       "  'hearts',\n",
       "  'of',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'loves',\n",
       "  'the',\n",
       "  'internet',\n",
       "  ',',\n",
       "  'it',\n",
       "  'looks',\n",
       "  'like',\n",
       "  '[',\n",
       "  'The',\n",
       "  'DarkNet',\n",
       "  'Plan',\n",
       "  ']',\n",
       "  '(',\n",
       "  'http',\n",
       "  ':',\n",
       "  '//www.reddit.com/r/darknetplan',\n",
       "  ')',\n",
       "  'has',\n",
       "  'grown',\n",
       "  'by',\n",
       "  'the',\n",
       "  'thousands',\n",
       "  'and',\n",
       "  'even',\n",
       "  'got',\n",
       "  '[',\n",
       "  'national',\n",
       "  'media',\n",
       "  'attention',\n",
       "  ']',\n",
       "  '(',\n",
       "  'http',\n",
       "  ':',\n",
       "  '//www.forbes.com/sites/andygreenberg/2011/11/23/wary-of-sopa-reddit-users-aim-to-build-a-new-censorship-free-internet/',\n",
       "  ')',\n",
       "  '.',\n",
       "  'What',\n",
       "  'is',\n",
       "  'the',\n",
       "  'feasibility',\n",
       "  'of',\n",
       "  'doing',\n",
       "  'that',\n",
       "  'for',\n",
       "  'our',\n",
       "  'big',\n",
       "  'red',\n",
       "  'campus',\n",
       "  '?',\n",
       "  'Relevant',\n",
       "  ':',\n",
       "  'I',\n",
       "  'miss',\n",
       "  'DC++'],\n",
       " [],\n",
       " [],\n",
       " ['i',\n",
       "  \"'m\",\n",
       "  'seriously',\n",
       "  'considering',\n",
       "  'cornell',\n",
       "  'for',\n",
       "  'law',\n",
       "  'school',\n",
       "  ',',\n",
       "  'and',\n",
       "  'this',\n",
       "  'intrigues',\n",
       "  'me',\n",
       "  '.',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'not',\n",
       "  'at',\n",
       "  'all',\n",
       "  'a',\n",
       "  'part',\n",
       "  'of',\n",
       "  'my',\n",
       "  'decision',\n",
       "  'making',\n",
       "  'process',\n",
       "  ',',\n",
       "  'but',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'cornell',\n",
       "  'was',\n",
       "  'at',\n",
       "  'some',\n",
       "  'point',\n",
       "  'by',\n",
       "  'some',\n",
       "  'people',\n",
       "  'known',\n",
       "  'as',\n",
       "  '``',\n",
       "  'godless',\n",
       "  'cornell',\n",
       "  \"''\",\n",
       "  '(',\n",
       "  'i',\n",
       "  'think',\n",
       "  'i',\n",
       "  'read',\n",
       "  'that',\n",
       "  'on',\n",
       "  'wikipedia',\n",
       "  ')',\n",
       "  'makes',\n",
       "  'me',\n",
       "  'smile',\n",
       "  '.',\n",
       "  'it',\n",
       "  'seems',\n",
       "  'like',\n",
       "  'most',\n",
       "  'of',\n",
       "  'the',\n",
       "  'older',\n",
       "  'schools',\n",
       "  'in',\n",
       "  'this',\n",
       "  'country',\n",
       "  'are',\n",
       "  'religious',\n",
       "  'and',\n",
       "  'cornell',\n",
       "  'is',\n",
       "  'non-sectarian',\n",
       "  '(',\n",
       "  'i',\n",
       "  'believe',\n",
       "  ')',\n",
       "  '.',\n",
       "  'how',\n",
       "  'does',\n",
       "  'this',\n",
       "  'play',\n",
       "  'out',\n",
       "  'at',\n",
       "  'the',\n",
       "  'school',\n",
       "  '?',\n",
       "  'edit',\n",
       "  ':',\n",
       "  'to',\n",
       "  'be',\n",
       "  'more',\n",
       "  'clear',\n",
       "  ',',\n",
       "  'i',\n",
       "  'guess',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'wondering',\n",
       "  'if',\n",
       "  'the',\n",
       "  'student',\n",
       "  'body',\n",
       "  'is',\n",
       "  'especially',\n",
       "  'agnostic/atheist',\n",
       "  'leaning',\n",
       "  'or',\n",
       "  'if',\n",
       "  'there',\n",
       "  \"'s\",\n",
       "  'some',\n",
       "  'sort',\n",
       "  'of',\n",
       "  'unstated',\n",
       "  'distaste',\n",
       "  'for',\n",
       "  'religion',\n",
       "  '.',\n",
       "  'i',\n",
       "  \"'m\",\n",
       "  'agnostic',\n",
       "  'atheist',\n",
       "  'FWIW',\n",
       "  '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CV(tokenizer=lambda x: x, preprocessor=lambda x: x, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3),\n",
       "                preprocessor=<function <lambda> at 0x1250a7b70>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function <lambda> at 0x1250a7a60>, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(raw_documents=tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
