{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/caleb/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import pickle\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, Conversation, Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_dir = '/sauna/fake-news'\n",
    "politics_dir = '/sauna/reddit_201810_raw/corpus/pokemontrades_banlist~-~politics/politics'\n",
    "donald_dir = '/sauna/reddit_201810_raw/corpus/TheTwoBeerQueers~-~The_Donald/The_Donald/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_urls = {'breitbart.com', 'dcgazette.com', \"dailymail.co.uk\", \n",
    "                  \"lewrockwell.com\", \"newswars.com\", \"gellerreport.com\",\n",
    "                  \"dcclothesline.com\", \"thegatewaypundit.com\", \"trueactivist.com\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_sites = ['BBC', 'Business Insider', 'Buzzfeed', 'CBS News', 'CNN', 'Daily Beast', \n",
    "                   'FT Westminster Blog', 'FiveThirtyEight', 'Fortune', 'Mercury News', 'NPR',\n",
    "                   'National Review', 'New Yorker', 'Newsweek', 'PBS', 'Politico', 'Real Clear Politics', 'Reuters',\n",
    "                   'Slate', 'The American Conservative', 'The Denver Post', 'The Guardian', 'The Hill', 'The New York Times',\n",
    "                   'The Verge', 'USA Today', 'Vox', 'WSJ Washington Wire', 'Washington Monthly', 'Washington Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_urls = {'bbc.com', 'businessinsider.com', 'buzzfeed.com', 'cbsnews.com', 'cnn.com', 'thedailybeast.com',\n",
    "                  'ft.com', 'fivethirtyeight.com', 'fortune.com', 'mercurynews.com', 'npr.org', 'nationalreview.com',\n",
    "                  'newyorker.com', 'newsweek.com', 'pbs.org', 'politico.com', 'realclearpol', 'reuters.com',\n",
    "                  'slate.com', 'theamericanconservative.com', 'denverpost.com', 'theguardian.com', 'thehill.com',\n",
    "                  'nytimes.com', 'theverge.com', 'usatoday.com', 'vox.com', 'wsj.com', 'washingtonmonthly.com', \n",
    "                  'washingtonpost.com'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_corpus = Corpus(filename=politics_dir)\n",
    "politics_corpus = Corpus(filename=os.path.join(fake_news_dir, 'politics_corpus_paired'))\n",
    "# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_valid_convos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time: 01 Oct 2017\n",
    "# End time: 01 Oct 2018\n",
    "start_time = 1506816000\n",
    "end_time = 1538352000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "real_news_convos = defaultdict(set)\n",
    "fake_news_convos = defaultdict(set)\n",
    "for convo in politics_corpus.iter_conversations():\n",
    "    if start_time <= convo.meta['timestamp'] <= end_time:\n",
    "        if convo.meta['domain'] in real_news_urls:\n",
    "            real_news_convos[convo.meta['domain']].add(convo.id)\n",
    "        elif convo.meta['domain'] in fake_news_urls:\n",
    "            fake_news_convos[convo.meta['domain']].add(convo.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washingtonpost.com: 136\n",
      "thehill.com: 234\n",
      "slate.com: 22\n",
      "cnn.com: 74\n",
      "thedailybeast.com: 57\n",
      "reuters.com: 35\n",
      "nytimes.com: 74\n",
      "businessinsider.com: 59\n",
      "politico.com: 59\n",
      "cbsnews.com: 25\n",
      "newsweek.com: 60\n",
      "vox.com: 27\n",
      "theamericanconservative.com: 3\n",
      "theguardian.com: 32\n",
      "bbc.com: 15\n",
      "wsj.com: 8\n",
      "usatoday.com: 21\n",
      "buzzfeed.com: 13\n",
      "newyorker.com: 4\n",
      "fivethirtyeight.com: 2\n",
      "npr.org: 14\n",
      "pbs.org: 4\n",
      "nationalreview.com: 6\n",
      "denverpost.com: 1\n",
      "theverge.com: 1\n",
      "washingtonmonthly.com: 2\n",
      "mercurynews.com: 1\n",
      "fortune.com: 1\n",
      "Total posts: 990\n"
     ]
    }
   ],
   "source": [
    "for k, v in real_news_convos.items():\n",
    "    print(\"{}: {}\".format(k, len(v)))\n",
    "    \n",
    "print(\"Total posts:\", sum([len(v) for v in real_news_convos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breitbart.com: 987\n",
      "dailymail.co.uk: 3\n",
      "Total posts: 990\n"
     ]
    }
   ],
   "source": [
    "for k, v in fake_news_convos.items():\n",
    "    print(\"{}: {}\".format(k, len(v)))\n",
    "    \n",
    "print(\"Total posts:\", sum([len(v) for v in fake_news_convos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "real_news_convos_ids = list(chain.from_iterable(real_news_convos.values()))\n",
    "fake_news_convos_ids = list(chain.from_iterable(fake_news_convos.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_convos_ids = set(real_news_convos_ids + fake_news_convos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 59141\n",
      "Number of Utterances: 278813\n",
      "Number of Conversations: 1980\n"
     ]
    }
   ],
   "source": [
    "\n",
    "politics_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_corpus.filter_conversations_by(func=lambda convo: convo.id in valid_convos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 59141\n",
      "Number of Utterances: 278813\n",
      "Number of Conversations: 1980\n"
     ]
    }
   ],
   "source": [
    "politics_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_corpus.dump('politics_corpus_valid_convos', base_path=fake_news_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfxs = politics_corpus.utterance_threads(prefix_len=8, include_root=False)\n",
    "convos_with_valid_len = set()\n",
    "convo_to_thread = defaultdict(list)\n",
    "for thread_id, utts in thread_pfxs.items():\n",
    "    if len(utts) >= 8:\n",
    "        convos_with_valid_len.add(utts[next(iter(utts))].root)\n",
    "        convo_to_thread[utts[next(iter(utts))].root].append(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "for convo, threads in convo_to_thread.items():\n",
    "    convo_to_thread[convo] = choice(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_to_convo = {v: k for k, v in convo_to_thread.items() if len(v) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_thread_ids = [convo_to_thread[convo_id] for convo_id in real_news_convos_ids if len(convo_to_thread[convo_id]) > 0]\n",
    "fake_news_thread_ids = [convo_to_thread[convo_id] for convo_id in fake_news_convos_ids if len(convo_to_thread[convo_id]) > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "real_news_thread_sample = sample(real_news_thread_ids, len(fake_news_thread_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_convos_sample = [thread_to_convo[thread_id] for thread_id in real_news_thread_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_long_convos = [thread_to_convo[thread_id] for thread_id in fake_news_thread_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_convos = set(real_news_convos_sample).union(fake_news_long_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_threads = fake_news_thread_ids + real_news_thread_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(balanced_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_corpus.filter_conversations_by(func=lambda convo: convo.id in paired_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_corpus.dump('politics_corpus_paired', base_path=fake_news_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_paired'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 59141\n",
      "Number of Utterances: 278813\n",
      "Number of Conversations: 1980\n"
     ]
    }
   ],
   "source": [
    "politics_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(min_thread_len=8, prefix_len=8, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_feats = hc.retrieve_motif_counts(politics_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_threads = set(balanced_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_feats = {k: v for k, v in motif_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_counts_df = pd.DataFrame.from_dict(motif_feats, orient='index')\n",
    "motif_feat_names = list(motif_counts_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caleb/Cornell-Conversational-Analysis-Toolkit/convokit/hyperconvo/hyperconvo.py:182: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \"norm.max\": lambda l: np.max(l) / np.sum(l),\n",
      "/home/caleb/Cornell-Conversational-Analysis-Toolkit/convokit/hyperconvo/hyperconvo.py:187: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if len(l) > 1 else np.nan,\n",
      "/home/caleb/miniconda/envs/venv/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/caleb/miniconda/envs/venv/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/caleb/miniconda/envs/venv/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:2614: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pk = 1.0*pk / np.sum(pk, axis=0)\n",
      "/home/caleb/Cornell-Conversational-Analysis-Toolkit/convokit/hyperconvo/hyperconvo.py:194: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if len(l) > 1 else np.nan\n"
     ]
    }
   ],
   "source": [
    "hyperconv_feats = hc.retrieve_feats(politics_corpus)\n",
    "hyperconv_feats = {k: v for k, v in hyperconv_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_df = pd.DataFrame.from_dict(hyperconv_feats, orient='index')\n",
    "hyperconv_feat_names = list(hyperconv_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feats = hc.retrieve_motif_pathway_stats(politics_corpus)\n",
    "path_feats = {k: v for k, v in path_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stats_df = pd.DataFrame.from_dict(path_feats, orient='index')\n",
    "columns = ['PATH-'+', '.join(filter(lambda x: type(x) == str, col)).strip() for col in path_stats_df.columns.values]\n",
    "path_stats_df.columns = columns\n",
    "path_feat_names = list(path_stats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df = pd.concat([hyperconv_df, path_stats_df, motif_counts_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df['volume'] = [len(set(utt.user.name for utt in thread_pfxs[thread_id].values())) for thread_id in feats_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_thread_ids = set(fake_news_thread_ids)\n",
    "y = [int(thread_id in fake_news_thread_ids) for thread_id in list(feats_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 262)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df_no_na = feats_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1907"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feats_df_no_na['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- hyperconvo, cv_accuracy: 0.5905\n",
      "- volume, cv_accuracy: 0.5338\n",
      "- hyperconv-volume, cv_accuracy: 0.5915\n",
      "- motifs, cv_accuracy: 0.5747\n",
      "- motifs-volume, cv_accuracy: 0.5810\n",
      "- motifpaths, cv_accuracy: 0.5994\n",
      "- motifpaths-volume, cv_accuracy: 0.6009\n",
      "- hyperconv-motif, cv_accuracy: 0.5863\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for feature_set, name in [(hyperconv_feat_names, \"hyperconvo\"),\n",
    "                        ([\"volume\"], \"volume\"),\n",
    "                        (hyperconv_feat_names + [\"volume\"], \"hyperconv-volume\"),\n",
    "                        (motif_feat_names, \"motifs\"),\n",
    "                        (motif_feat_names + [\"volume\"], \"motifs-volume\"),\n",
    "                        (path_feat_names, \"motifpaths\"),\n",
    "                        (path_feat_names + [\"volume\"], \"motifpaths-volume\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names, \"hyperconv-motif\"),\n",
    "                        (hyperconv_feat_names + path_feat_names, \"hyperconv-paths\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names + path_feat_names, \"hyperconvo-motifall\"),\n",
    "                       ]:\n",
    "    clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])      \n",
    "    loo = LeaveOneOut()\n",
    "    X = np.array(feats_df_no_na[feature_set])\n",
    "    y = np.array(feats_df_no_na['y'])\n",
    "    scores = cross_val_score(clf, X, y, cv=loo)\n",
    "    print(\"- {}, cv_accuracy: {:.4f}\".format(name, scores.mean()))\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     train_score = clf.score(X_train, y_train)\n",
    "#     test_score = clf.score(X_test, y_test)\n",
    "#     print(\"- {}, train: {:.4f}, test: {:.4f}\".format(name, train_score, test_score))\n",
    "#     print_extreme_coefs(clf, feature_set, num_features = 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfxs = politics_corpus.utterance_threads(prefix_len=8, include_root=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfx_texts = {k: \" \".join([utt.text for utt in v.values()]) for k, v in thread_pfxs.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(thread_id in real_news_thread_sample) for thread_id in list(feats_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids, y_train, y_test = train_test_split(list(balanced_threads), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=0.05, max_df=0.7, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv.fit_transform([thread_pfx_texts[train_id] for train_id in train_ids]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cv.transform([thread_pfx_texts[test_id] for test_id in test_ids]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BoW, train: 0.7740, test: 0.5404\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])      \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"- {}, train: {:.4f}, test: {:.4f}\".format(\"BoW\", train_score, test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feature_names, num_features: int = 5):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "\n",
    "    assert len(feature_names) == len(coefs)\n",
    "\n",
    "    feats_coefs = sorted(list(zip(feature_names, coefs)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print()\n",
    "    print(\"TOP {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[:num_features]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[-num_features:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 FEATURES\n",
      "right wing: 1.076\n",
      "https: 0.770\n",
      "http: 0.581\n",
      "www: 0.537\n",
      "of the: 0.446\n",
      "the right: 0.432\n",
      "going: 0.406\n",
      "years: 0.375\n",
      "on: 0.371\n",
      "said: 0.343\n",
      "\n",
      "BOTTOM 10 FEATURES\n",
      "org: -0.386\n",
      "against: -0.395\n",
      "talking about: -0.399\n",
      "https www: -0.401\n",
      "world: -0.407\n",
      "do with: -0.422\n",
      "right: -0.492\n",
      "going to: -0.532\n",
      "com: -0.815\n",
      "wing: -1.256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, cv.get_feature_names(), num_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
