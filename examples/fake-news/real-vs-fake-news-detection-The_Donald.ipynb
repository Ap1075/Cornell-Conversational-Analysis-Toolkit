{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/caleb/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import pickle\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, Conversation, Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_dir = '/sauna/fake-news'\n",
    "news_dir = '/sauna/reddit_201810_raw/corpus/newreddits_nsfw~-~news/news/'\n",
    "donald_dir = '/sauna/reddit_201810_raw/corpus/TheTwoBeerQueers~-~The_Donald/The_Donald/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_urls = {'breitbart.com', 'dcgazette.com', \"dailymail.co.uk\", \n",
    "                  \"lewrockwell.com\", \"newswars.com\", \"gellerreport.com\",\n",
    "                  \"dcclothesline.com\", \"thegatewaypundit.com\", \"trueactivist.com\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_sites = ['BBC', 'Business Insider', 'Buzzfeed', 'CBS News', 'CNN', 'Daily Beast', \n",
    "                   'FT Westminster Blog', 'FiveThirtyEight', 'Fortune', 'Mercury News', 'NPR',\n",
    "                   'National Review', 'New Yorker', 'Newsweek', 'PBS', 'Politico', 'Real Clear Politics', 'Reuters',\n",
    "                   'Slate', 'The American Conservative', 'The Denver Post', 'The Guardian', 'The Hill', 'The New York Times',\n",
    "                   'The Verge', 'USA Today', 'Vox', 'WSJ Washington Wire', 'Washington Monthly', 'Washington Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_urls = {'bbc.com', 'businessinsider.com', 'buzzfeed.com', 'cbsnews.com', 'cnn.com', 'thedailybeast.com',\n",
    "                  'ft.com', 'fivethirtyeight.com', 'fortune.com', 'mercurynews.com', 'npr.org', 'nationalreview.com',\n",
    "                  'newyorker.com', 'newsweek.com', 'pbs.org', 'politico.com', 'realclearpol', 'reuters.com',\n",
    "                  'slate.com', 'theamericanconservative.com', 'denverpost.com', 'theguardian.com', 'thehill.com',\n",
    "                  'nytimes.com', 'theverge.com', 'usatoday.com', 'vox.com', 'wsj.com', 'washingtonmonthly.com', \n",
    "                  'washingtonpost.com'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_corpus = Corpus(filename=donald_dir)\n",
    "# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_valid_convos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time: 01 Oct 2016\n",
    "# End time: 01 Oct 2018\n",
    "start_time = 1475280000\n",
    "end_time = 1538352000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "real_news_convos = defaultdict(set)\n",
    "fake_news_convos = defaultdict(set)\n",
    "for convo in donald_corpus.iter_conversations():\n",
    "    if start_time <= convo.meta['timestamp'] <= end_time:\n",
    "        if convo.meta['domain'] in real_news_urls:\n",
    "            real_news_convos[convo.meta['domain']].add(convo.id)\n",
    "        elif convo.meta['domain'] in fake_news_urls:\n",
    "            fake_news_convos[convo.meta['domain']].add(convo.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nytimes.com: 4071\n",
      "cbsnews.com: 2079\n",
      "thehill.com: 12938\n",
      "theguardian.com: 3337\n",
      "theverge.com: 383\n",
      "wsj.com: 2047\n",
      "cnn.com: 2835\n",
      "nationalreview.com: 1403\n",
      "vox.com: 510\n",
      "npr.org: 1052\n",
      "bbc.com: 3090\n",
      "washingtonpost.com: 4781\n",
      "politico.com: 3578\n",
      "businessinsider.com: 2324\n",
      "reuters.com: 5110\n",
      "newsweek.com: 1078\n",
      "usatoday.com: 2773\n",
      "mercurynews.com: 342\n",
      "buzzfeed.com: 673\n",
      "newyorker.com: 301\n",
      "thedailybeast.com: 1238\n",
      "pbs.org: 224\n",
      "slate.com: 479\n",
      "fortune.com: 461\n",
      "ft.com: 212\n",
      "theamericanconservative.com: 264\n",
      "denverpost.com: 205\n",
      "fivethirtyeight.com: 236\n",
      "washingtonmonthly.com: 21\n",
      "Total posts: 58045\n"
     ]
    }
   ],
   "source": [
    "for k, v in real_news_convos.items():\n",
    "    print(\"{}: {}\".format(k, len(v)))\n",
    "    \n",
    "print(\"Total posts:\", sum([len(v) for v in real_news_convos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dailymail.co.uk: 13903\n",
      "breitbart.com: 49274\n",
      "thegatewaypundit.com: 16219\n",
      "dcclothesline.com: 198\n",
      "lewrockwell.com: 227\n",
      "trueactivist.com: 28\n",
      "dcgazette.com: 4\n",
      "newswars.com: 392\n",
      "gellerreport.com: 269\n",
      "Total posts: 80514\n"
     ]
    }
   ],
   "source": [
    "for k, v in fake_news_convos.items():\n",
    "    print(\"{}: {}\".format(k, len(v)))\n",
    "    \n",
    "print(\"Total posts:\", sum([len(v) for v in fake_news_convos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "real_news_convos_ids = list(chain.from_iterable(real_news_convos.values()))\n",
    "fake_news_convos_ids = list(chain.from_iterable(fake_news_convos.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_convos_ids = set(real_news_convos_ids + fake_news_convos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 617191\n",
      "Number of Utterances: 38640598\n",
      "Number of Conversations: 3830155\n"
     ]
    }
   ],
   "source": [
    "donald_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_corpus.filter_conversations_by(func=lambda convo: convo.id in valid_convos_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 92476\n",
      "Number of Utterances: 1274684\n",
      "Number of Conversations: 138559\n"
     ]
    }
   ],
   "source": [
    "donald_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_corpus.dump('donald_corpus_valid_convos_2year', base_path=fake_news_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfxs = donald_corpus.utterance_threads(prefix_len=8, include_root=False)\n",
    "convos_with_valid_len = set()\n",
    "convo_to_thread = defaultdict(list)\n",
    "for thread_id, utts in thread_pfxs.items():\n",
    "    if len(utts) >= 8:\n",
    "        convos_with_valid_len.add(utts[next(iter(utts))].root)\n",
    "        convo_to_thread[utts[next(iter(utts))].root].append(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "for convo, threads in convo_to_thread.items():\n",
    "    convo_to_thread[convo] = choice(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_to_convo = {v: k for k, v in convo_to_thread.items() if len(v) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6oiwcb',\n",
       " '5wmjg3',\n",
       " '5u4uxc',\n",
       " '86i3r1',\n",
       " '6i2dzz',\n",
       " '5mroiq',\n",
       " '56swim',\n",
       " '5by9qm',\n",
       " '65c0f0',\n",
       " '6kilc8']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_news_convos_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_thread_ids = [convo_to_thread[convo_id] for convo_id in real_news_convos_ids if len(convo_to_thread[convo_id]) > 0]\n",
    "fake_news_thread_ids = [convo_to_thread[convo_id] for convo_id in fake_news_convos_ids if len(convo_to_thread[convo_id]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2241"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "fake_news_thread_sample = sample(fake_news_thread_ids, len(real_news_thread_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_convos_sample = [thread_to_convo[thread_id] for thread_id in fake_news_thread_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_long_convos = [thread_to_convo[thread_id] for thread_id in real_news_thread_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_convos = set(fake_news_convos_sample).union(real_news_long_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_threads = real_news_thread_ids + fake_news_thread_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4482"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(balanced_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_corpus.filter_conversations_by(func=lambda convo: convo.id in paired_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_corpus.dump('donald_corpus_paired_2year', base_path=fake_news_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_paired'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 62295\n",
      "Number of Utterances: 496334\n",
      "Number of Conversations: 4482\n"
     ]
    }
   ],
   "source": [
    "donald_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(min_thread_len=8, prefix_len=8, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_feats = hc.retrieve_motif_counts(donald_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_threads = set(balanced_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_feats = {k: v for k, v in motif_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_counts_df = pd.DataFrame.from_dict(motif_feats, orient='index')\n",
    "motif_feat_names = list(motif_counts_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caleb/Cornell-Conversational-Analysis-Toolkit/convokit/hyperconvo/hyperconvo.py:182: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  \"norm.max\": lambda l: np.max(l) / np.sum(l),\n",
      "/home/caleb/Cornell-Conversational-Analysis-Toolkit/convokit/hyperconvo/hyperconvo.py:187: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if len(l) > 1 else np.nan,\n",
      "/home/caleb/miniconda/envs/venv/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/caleb/miniconda/envs/venv/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/caleb/miniconda/envs/venv/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:2614: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pk = 1.0*pk / np.sum(pk, axis=0)\n",
      "/home/caleb/Cornell-Conversational-Analysis-Toolkit/convokit/hyperconvo/hyperconvo.py:194: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if len(l) > 1 else np.nan\n"
     ]
    }
   ],
   "source": [
    "hyperconv_feats = hc.retrieve_feats(donald_corpus)\n",
    "hyperconv_feats = {k: v for k, v in hyperconv_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_df = pd.DataFrame.from_dict(hyperconv_feats, orient='index')\n",
    "hyperconv_feat_names = list(hyperconv_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feats = hc.retrieve_motif_pathway_stats(donald_corpus)\n",
    "path_feats = {k: v for k, v in path_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stats_df = pd.DataFrame.from_dict(path_feats, orient='index')\n",
    "columns = ['PATH-'+', '.join(filter(lambda x: type(x) == str, col)).strip() for col in path_stats_df.columns.values]\n",
    "path_stats_df.columns = columns\n",
    "path_feat_names = list(path_stats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df = pd.concat([hyperconv_df, path_stats_df, motif_counts_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_thread_sample = set(fake_news_thread_sample)\n",
    "y = [int(thread_id in fake_news_thread_sample) for thread_id in list(feats_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4482, 261)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df_no_na = feats_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4443"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2224"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feats_df_no_na['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- hyperconvo, cv_accuracy: 0.5192\n",
      "- motifs, cv_accuracy: 0.5226\n",
      "- motifpaths, cv_accuracy: 0.5123\n",
      "- hyperconv-motif, cv_accuracy: 0.5138\n",
      "- hyperconv-paths, cv_accuracy: 0.5123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "for feature_set, name in [(hyperconv_feat_names, \"hyperconvo\"),\n",
    "                        (motif_feat_names, \"motifs\"),\n",
    "                        (path_feat_names, \"motifpaths\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names, \"hyperconv-motif\"),\n",
    "                        (hyperconv_feat_names + path_feat_names, \"hyperconv-paths\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names + path_feat_names, \"hyperconvo-motifall\"),\n",
    "                       ]:\n",
    "    clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])      \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    X = np.array(feats_df_no_na[feature_set])\n",
    "    y = np.array(feats_df_no_na['y'])\n",
    "    scores = cross_val_score(clf, X, y, cv=loo)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     train_score = clf.score(X_train, y_train)\n",
    "#     test_score = clf.score(X_test, y_test)\n",
    "#     print(\"- {}, train: {:.4f}, test: {:.4f}\".format(name, train_score, test_score))\n",
    "    print(\"- {}, cv_accuracy: {:.4f}\".format(name, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfxs = donald_corpus.utterance_threads(prefix_len=8, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfx_texts = {k: \" \".join([utt.text for utt in v.values()]) for k, v in thread_pfxs.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(thread_id in fake_news_thread_sample) for thread_id in list(feats_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids, y_train, y_test = train_test_split(list(balanced_threads), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=0.05, max_df=0.7, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv.fit_transform([thread_pfx_texts[train_id] for train_id in train_ids]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cv.transform([thread_pfx_texts[test_id] for test_id in test_ids]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BoW, train: 0.6485, test: 0.5162\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])      \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"- {}, train: {:.4f}, test: {:.4f}\".format(\"BoW\", train_score, test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feature_names, num_features: int = 5):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "\n",
    "    assert len(feature_names) == len(coefs)\n",
    "\n",
    "    feats_coefs = sorted(list(zip(feature_names, coefs)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print()\n",
    "    print(\"TOP {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[:num_features]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[-num_features:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 FEATURES\n",
      "you re: 0.319\n",
      "they re: 0.296\n",
      "able: 0.286\n",
      "trying: 0.223\n",
      "wouldn: 0.170\n",
      "hillary: 0.164\n",
      "we re: 0.163\n",
      "at least: 0.162\n",
      "with: 0.151\n",
      "first: 0.146\n",
      "\n",
      "BOTTOM 10 FEATURES\n",
      "going to: -0.137\n",
      "have the: -0.137\n",
      "least: -0.143\n",
      "be: -0.152\n",
      "bad: -0.156\n",
      "with the: -0.156\n",
      "trying to: -0.225\n",
      "able to: -0.293\n",
      "clinton: -0.472\n",
      "re: -0.482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, cv.get_feature_names(), num_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
