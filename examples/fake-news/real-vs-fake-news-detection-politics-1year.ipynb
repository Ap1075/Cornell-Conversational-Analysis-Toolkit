{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/caleb/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import pickle\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, Conversation, Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_dir = '/sauna/fake-news'\n",
    "politics_dir = '/sauna/reddit_201810_raw/corpus/pokemontrades_banlist~-~politics/politics'\n",
    "donald_dir = '/sauna/reddit_201810_raw/corpus/TheTwoBeerQueers~-~The_Donald/The_Donald/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_urls = {'breitbart.com', 'dcgazette.com', \"dailymail.co.uk\", \n",
    "                  \"lewrockwell.com\", \"newswars.com\", \"gellerreport.com\",\n",
    "                  \"dcclothesline.com\", \"thegatewaypundit.com\", \"trueactivist.com\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_sites = ['BBC', 'Business Insider', 'Buzzfeed', 'CBS News', 'CNN', 'Daily Beast', \n",
    "                   'FT Westminster Blog', 'FiveThirtyEight', 'Fortune', 'Mercury News', 'NPR',\n",
    "                   'National Review', 'New Yorker', 'Newsweek', 'PBS', 'Politico', 'Real Clear Politics', 'Reuters',\n",
    "                   'Slate', 'The American Conservative', 'The Denver Post', 'The Guardian', 'The Hill', 'The New York Times',\n",
    "                   'The Verge', 'USA Today', 'Vox', 'WSJ Washington Wire', 'Washington Monthly', 'Washington Post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_urls = {'bbc.com', 'businessinsider.com', 'buzzfeed.com', 'cbsnews.com', 'cnn.com', 'thedailybeast.com',\n",
    "                  'ft.com', 'fivethirtyeight.com', 'fortune.com', 'mercurynews.com', 'npr.org', 'nationalreview.com',\n",
    "                  'newyorker.com', 'newsweek.com', 'pbs.org', 'politico.com', 'realclearpol', 'reuters.com',\n",
    "                  'slate.com', 'theamericanconservative.com', 'denverpost.com', 'theguardian.com', 'thehill.com',\n",
    "                  'nytimes.com', 'theverge.com', 'usatoday.com', 'vox.com', 'wsj.com', 'washingtonmonthly.com', \n",
    "                  'washingtonpost.com'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aae0baab2847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# politics_corpus = Corpus(filename=os.path.join(fake_news_dir, 'politics_corpus_paired_2year'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolitics_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolitics_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_valid_convos'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Cornell-Conversational-Analysis-Toolkit/convokit/model/corpus.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, utterances, utterance_start_index, utterance_end_index, merge_lines, exclude_utterance_meta, exclude_conversation_meta, exclude_user_meta, exclude_overall_meta, version)\u001b[0m\n\u001b[1;32m     75\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mutterance_start_index\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mutterance_end_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                 \u001b[0mutterances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                             \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# politics_corpus = Corpus(filename=os.path.join(fake_news_dir, 'politics_corpus_paired_2year'))\n",
    "politics_corpus = Corpus(filename=politics_dir)\n",
    "# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_valid_convos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time: 01 Oct 2016\n",
    "# End time: 01 Oct 2018\n",
    "start_time = 1506816000\n",
    "end_time = 1538352000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "real_news_convos = defaultdict(lambda: defaultdict(list))\n",
    "fake_news_convos = defaultdict(lambda: defaultdict(list))\n",
    "for convo in politics_corpus.iter_conversations():\n",
    "    if start_time <= convo.meta['timestamp'] <= end_time:\n",
    "        if convo.meta['domain'] in real_news_urls:\n",
    "            real_news_convos[convo.meta['domain']][convo.meta['url']].append(convo.id)\n",
    "        elif convo.meta['domain'] in fake_news_urls:\n",
    "            fake_news_convos[convo.meta['domain']][convo.meta['url']].append(convo.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfxs = politics_corpus.utterance_threads(prefix_len=5, include_root=False)\n",
    "convos_with_valid_len = set()\n",
    "convo_to_thread = defaultdict(list)\n",
    "for thread_id, utts in thread_pfxs.items():\n",
    "    if len(utts) >= 5:\n",
    "        convos_with_valid_len.add(utts[next(iter(utts))].root)\n",
    "        convo_to_thread[utts[next(iter(utts))].root].append(thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "for convo, threads in convo_to_thread.items():\n",
    "    convo_to_thread[convo] = choice(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, urls in real_news_convos.items():\n",
    "    for url, convos in urls.items():\n",
    "        valid_convos = [c for c in convos if c in convo_to_thread]\n",
    "        urls[url] = choice(valid_convos) if len(valid_convos) > 0 else None\n",
    "\n",
    "for domain, urls in fake_news_convos.items():\n",
    "    for url, convos in urls.items():\n",
    "        valid_convos = [c for c in convos if c in convo_to_thread]\n",
    "        urls[url] = choice(valid_convos) if len(valid_convos) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, urls in real_news_convos.items():\n",
    "    real_news_convos[domain] = {url: convo_id for url, convo_id in urls.items() if convo_id is not None}\n",
    "\n",
    "for domain, urls in fake_news_convos.items():\n",
    "    fake_news_convos[domain] = {url: convo_id for url, convo_id in urls.items() if convo_id is not None}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_convo_ids = []\n",
    "fake_news_convo_ids = []\n",
    "for domain, urls in real_news_convos.items():\n",
    "    real_news_convo_ids.extend(list(urls.values()))\n",
    "for domain, urls in fake_news_convos.items():\n",
    "    fake_news_convo_ids.extend(list(urls.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_thread_ids = [convo_to_thread[convo_id] for convo_id in real_news_convo_ids]\n",
    "fake_news_thread_ids = [convo_to_thread[convo_id] for convo_id in fake_news_convo_ids]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in real_news_convos.items():\n",
    "    print(\"{}: {}\".format(k, len(v)))\n",
    "    \n",
    "print(\"Total posts:\", sum([len(v) for v in real_news_convos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in fake_news_convos.items():\n",
    "    print(\"{}: {}\".format(k, len(v)))\n",
    "    \n",
    "print(\"Total posts:\", sum([len(v) for v in fake_news_convos.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(real_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fake_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_fake_valid_ids = set(real_news_convo_ids).union(set(fake_news_convo_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_corpus.filter_conversations_by(func=lambda convo: convo.id in real_fake_valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_corpus.dump('politics_corpus_fake_real_valid_convos_1year', base_path=fake_news_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_thread_ids = [convo_to_thread[convo_id] for convo_id in real_news_convo_ids if len(convo_to_thread[convo_id]) > 0]\n",
    "fake_news_thread_ids = [convo_to_thread[convo_id] for convo_id in fake_news_convo_ids if len(convo_to_thread[convo_id]) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1577"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "real_news_thread_sample = sample(real_news_thread_ids, len(fake_news_thread_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_to_convo = {thread_id: convo_id for convo_id, thread_id in convo_to_thread.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news_convos_sample = [thread_to_convo[thread_id] for thread_id in real_news_thread_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_long_convos = [thread_to_convo[thread_id] for thread_id in fake_news_thread_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_convos = set(real_news_convos_sample).union(fake_news_long_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_to_thread_filtered = {k: v for k, v in convo_to_thread.items() if k in paired_convos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake_news_thread_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_threads = fake_news_thread_ids + real_news_thread_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(balanced_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paired_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_corpus.filter_conversations_by(func=lambda convo: convo.id in paired_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_corpus.dump('politics_corpus_paired_1year', base_path=fake_news_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donald_corpus = Corpus(filename=os.path.join(fake_news_dir, 'donald_corpus_paired'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 56013\n",
      "Number of Utterances: 260704\n",
      "Number of Conversations: 1928\n"
     ]
    }
   ],
   "source": [
    "politics_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(min_thread_len=8, prefix_len=8, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_feats = hc.retrieve_feats(politics_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_threads = set(balanced_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_feats = {k: v for k, v in motif_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_counts_df = pd.DataFrame.from_dict(motif_feats, orient='index')\n",
    "motif_feat_names = list(motif_counts_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_feats = hc.retrieve_feats(politics_corpus)\n",
    "hyperconv_feats = {k: v for k, v in hyperconv_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_df = pd.DataFrame.from_dict(hyperconv_feats, orient='index')\n",
    "hyperconv_feat_names = list(hyperconv_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feats = hc.retrieve_motif_pathway_stats(politics_corpus)\n",
    "path_feats = {k: v for k, v in path_feats.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stats = hc.retrieve_motif_pathway_stats(politics_corpus)\n",
    "path_stats = {k: v for k, v in path_stats.items() if k in balanced_threads}\n",
    "\n",
    "path_stats_df = pd.DataFrame.from_dict(path_stats, orient='index')\n",
    "columns = ['PATH-'+', '.join(filter(lambda x: type(x) == str, col)).strip() for col in path_stats_df.columns.values]\n",
    "path_stats_df.columns = columns\n",
    "\n",
    "path_stats_enum_df = pd.DataFrame()\n",
    "\n",
    "for path_stat in columns:\n",
    "    path_stats_enum_df['is-present[{}]'.format(path_stat)] = path_stats_df[path_stat] > 0\n",
    "    path_stats_enum_df['count[{}]'.format(path_stat)] = path_stats_df[path_stat]\n",
    "\n",
    "path_feat_names = list(path_stats_enum_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df = pd.concat([hyperconv_df, path_stats_enum_df, motif_counts_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df['volume'] = [len(set(utt.user.name for utt in thread_pfxs[thread_id].values())) for thread_id in feats_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_thread_ids = set(fake_news_thread_ids)\n",
    "y = [int(thread_id in fake_news_thread_ids) for thread_id in list(feats_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1928, 330)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df_no_na = feats_df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(feats_df_no_na['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feature_names, num_features: int = 5):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "\n",
    "    assert len(feature_names) == len(coefs)\n",
    "\n",
    "    feats_coefs = sorted(list(zip(feature_names, coefs)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print()\n",
    "    print(\"TOP {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[:num_features]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(num_features))\n",
    "    for ft, coef in feats_coefs[-num_features:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- hyperconvo, cv_accuracy: 0.5923\n",
      "- volume, cv_accuracy: 0.5078\n",
      "- motifs, cv_accuracy: 0.5923\n",
      "- motifpaths, cv_accuracy: 0.5960\n",
      "- motif-all, cv_accuracy: 0.6058\n",
      "- hyperconv-motif, cv_accuracy: 0.5928\n",
      "- hyperconv-paths, cv_accuracy: 0.6058\n",
      "- hyperconvo-motifall, cv_accuracy: 0.6053\n",
      "- hyperconvo-motifall-volume, cv_accuracy: 0.6048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for feature_set, name in [(hyperconv_feat_names, \"hyperconvo\"),\n",
    "                        ([\"volume\"], \"volume\"),\n",
    "                        (motif_feat_names, \"motifs\"),\n",
    "                        (path_feat_names, \"motifpaths\"),\n",
    "                        (motif_feat_names + path_feat_names, \"motif-all\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names, \"hyperconv-motif\"),\n",
    "                        (hyperconv_feat_names + path_feat_names, \"hyperconv-paths\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names + path_feat_names, \"hyperconvo-motifall\"),\n",
    "                        (hyperconv_feat_names + motif_feat_names + path_feat_names + [\"volume\"], \"hyperconvo-motifall-volume\")\n",
    "                       ]:\n",
    "    clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])      \n",
    "    loo = LeaveOneOut()\n",
    "    X = np.array(feats_df_no_na[feature_set])\n",
    "    y = np.array(feats_df_no_na['y'])\n",
    "    scores = cross_val_score(clf, X, y, cv=loo)\n",
    "    print(\"- {}, cv_accuracy: {:.4f}\".format(name, scores.mean()))\n",
    "#     print(\"- {}, train: {:.4f}, test: {:.4f}\".format(name, train_score, test_score))\n",
    "#     print_extreme_coefs(clf, feature_set, num_features = 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfxs = politics_corpus.utterance_threads(prefix_len=8, include_root=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pfx_texts = {k: \" \".join([utt.text for utt in v.values()]) for k, v in thread_pfxs.items() if k in balanced_threads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(thread_id in real_news_thread_sample) for thread_id in list(feats_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids, y_train, y_test = train_test_split(list(balanced_threads), y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=0.05, max_df=0.7, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv.fit_transform([thread_pfx_texts[train_id] for train_id in train_ids]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = cv.transform([thread_pfx_texts[test_id] for test_id in test_ids]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BoW, train: 0.6358, test: 0.5049\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])      \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(\"- {}, train: {:.4f}, test: {:.4f}\".format(\"BoW\", train_score, test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 FEATURES\n",
      "http www: 0.280\n",
      "https www: 0.271\n",
      "fake: 0.141\n",
      "get: 0.137\n",
      "will: 0.132\n",
      "all: 0.131\n",
      "have been: 0.130\n",
      "kind of: 0.119\n",
      "the gop: 0.114\n",
      "than: 0.106\n",
      "\n",
      "BOTTOM 10 FEATURES\n",
      "gop: -0.105\n",
      "information: -0.107\n",
      "to get: -0.120\n",
      "out: -0.122\n",
      "would: -0.123\n",
      "kind: -0.127\n",
      "http: -0.132\n",
      "https: -0.136\n",
      "been: -0.215\n",
      "www: -0.372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, cv.get_feature_names(), num_features = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
