{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos the `phrasing_motifs` module, which extracts representations of utterances in terms of how they are phrased. \n",
    "\n",
    "This is a really clear example of a method which reflects both good (we think) ideas and ad-hoc implementation decisions. As such, there are lots of options and potential variations to consider (beyond the deeper question of what phrasings even are) -- I'll detail these as I go along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the corpus (for now I'm using tennis, since it's a bit faster to run, but in the release demo I'll probably use parliament)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kitchen/convokit_corpora/tennis-corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the corpus, plus some pre-computed dependency parses (see `todo: scripts to do this parsing` for how to get these parses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus(ROOT_DIR)\n",
    "corpus.load_info('utterance',['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VERBOSITY = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our specific goal, which we'll use ConvoKit to accomplish, is to produce an abstract representation of questions asked by reporters to players after tennis matches, in terms of how they are phrased: what phrasing, or lexico-syntatic \"motif\", does a question have? \n",
    "\n",
    "Here's an example of an utterance by a reporter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_utt_id = '5188_0.q'\n",
    "utt = corpus.get_utterance(test_utt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do you feel? Watching the Australian Open it was very scary watching your ankle buckle. How does your ankle feel now?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each _sentence_ that has a question (the first and the third), we want to come up with a representation of that sentence's phrasing. Intuitively, both questions sound like they could be abstractly thought of as \"how do/does X feel?\" -- that is this is a query that could be asked of things beyond \"you\" or \"your ankle\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, if we want to get at this higher level of abstraction, we might want to start by looking at the structural \"skeleton\" of the sentence, i.e., its dependency parse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As as starting point, we're going to provide a representation of questions in terms of their dependency parse by extracting all the parent-to-child token edges, or \"arcs\". We will use the `TextToArcs` class to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextToArcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_arcs` is a transformer (actually a `TextProcessor`) that will read the dependency parse of an utterance and write the resultant arcs to a field called `'arcs'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n"
     ]
    }
   ],
   "source": [
    "get_arcs = TextToArcs('arcs', verbosity=VERBOSITY)\n",
    "corpus = get_arcs.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'arcs'` is a list where each element corresponds to a sentence in the utterance. Each sentence is represented in terms of its arcs, in a space-separated string. \n",
    "\n",
    "Each arc, in turn, can be read as follows:\n",
    "\n",
    "* `x_y` means that `x` is the parent and `y` is the child token (e.g., `feel_do` = `feel --> do`)\n",
    "* `x_*` means that `x` is a token with at least one descendant, which we do not resolve (this is roughly like bigrams backing off to unigrams)\n",
    "* `x>y` means that `x` and `y` are the first two tokens in the sentence (the decision here was that how the sentence starts is a signal of \"phrasing structure\" on par with the dependency tree structure)\n",
    "* `x>*` means that `x` is the first token in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do_* feel_* feel_do feel_how feel_you how>* how>do how_* you_*',\n",
       " 'ankle_* australian_* buckle_* buckle_ankle buckle_your it_* open_* open_australian open_the scary_* scary_very the_* very_* was_* was_it was_scary was_watching watching>* watching>the watching_* watching_buckle watching_open your_*',\n",
       " 'ankle_* ankle_your does_* feel_* feel_ankle feel_does feel_how feel_now how>* how>does how_* now_* your_*']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('arcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further preprocessing: cleaned-up arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, while we've got the methodology to start making sense of the dependency tree, we arguably haven't progressed beyond producing fancy bigram representations of sentences. One problem is perhaps that the default arc extraction is a bit too permissive -- it gives us _all_ of the arcs. We might not want this for a few reasons:\n",
    "\n",
    "* We only want to learn about question phrasings; we don't actually care about non-question sentences.\n",
    "* The structure of a question might be best encapsulated by the arcs that go out of the _root_ of the tree; as you get further down we might end up with less structural and more content-specific representations.\n",
    "* Likewise, the particular _nouns_ used (e.g., `australian`) might not be good descriptions of the more abstract phrasing pattern.\n",
    "\n",
    "All of these points are debatable, and the resultant modules I'll show below hopefully allow you to play around with them. Taking these point as is for now, though, we'll do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.phrasing_motifs import CensorNouns, QuestionSentences\n",
    "from convokit.convokitPipeline import ConvokitPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will actually create a pipeline to extract the arcs we want. This pipeline has the following components, in order:\n",
    "\n",
    "* `CensorNouns`: a transformer that removes all the nouns and pronouns from a dependency parse. This transformer also collapses constructions like `What tournament [was it]` into `What [was it]`.\n",
    "* `TextToArcs`: calling the arc extractor from above with an extra parameter: `root_only=True` which will only extract arcs attached to the root (in addition to the first two tokens, though this is also tunable by passing in parameter `use_start=True`).\n",
    "* `QuestionSentences`: a transformer that, given utterance fields consisting of a list of sentences, removes all the sentences which contain question marks. Here, we pass an extra parameter `input_filter=question_filter`, telling it to ignore utterances which aren't listed in the Corpus as questions (i.e., if a player asks a question, we'll discount this, since it's not labeled in the Corpus as a reporter question). \n",
    "    * (you may wonder how this transformer can tell whether a sentence has a question mark in it, given that the output of `TextToArcs` doesn't have any punctuation. Under the hood, `QuestionSentences` looks at the dependency parse of the sentence and checks whether the last token is a question.)\n",
    "    * `QuestionSentences` also omits any sentences which don't begin in capital letters. To turn this off, pass parameter `use_caps=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_filter(utt, aux_input={}):\n",
    "    return utt.meta['is_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_arc_pipe = ConvokitPipeline([\n",
    "    ('censor_nouns', CensorNouns('parsed_censored', verbosity=VERBOSITY)),\n",
    "    ('shallow_arcs', TextToArcs('arcs_censored', input_field='parsed_censored', \n",
    "                               root_only=True, verbosity=VERBOSITY)),\n",
    "    ('question_sentence_filter', QuestionSentences('question_arcs', input_field='arcs_censored',\n",
    "                                         input_filter=question_filter, verbosity=VERBOSITY))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n",
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n",
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = q_arc_pipe.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline results in a more minimalistic representation of utterances, in terms of just the arcs at the root of dependency trees, just the questions, and no nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel_* feel_do feel_how how>* how>do',\n",
       " 'feel_* feel_does feel_how feel_now how>* how>does']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('question_arcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrasing Motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to arrive at our final representation of phrasings, we can go one further level of abstraction. In short, `feel_does` doesn't feel yet like a fully-specified phrasing; we might instead want to return `how does __ feel`. To do this, our intuition is to think of phrasings as frequently-cooccurring sets of multiple arcs. \n",
    "\n",
    "To extract these frequent arc-sets (which may remind you of the data mining idea of extracting frequent itemsets) we will use the `PhrasingMotifs` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.phrasing_motifs import PhrasingMotifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm_model = PhrasingMotifs('question_motifs','question_arcs',min_support=50,fit_filter=question_filter,\n",
    "                          verbosity=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `pm_model` will:\n",
    "\n",
    "* extract all sets of arcs, as read from the `question_arcs` field, which occur at least 50 times in a corpus. These frequently-occurring arc sets will constitute the set, or \"vocabulary\", of phrasings.\n",
    "* write the resultant output -- the phrasings that an utterance contains -- to a field called `question_motifs`. \n",
    "\n",
    "On the latter point, `pm_model` will only transform (i.e., label phrasings for) utterances which are questions, i.e., `question_filter(utterance) = True`. That is, in both the train and transform steps, we totally ignore non-questions.\n",
    "\n",
    "Note that the phrasings learned by `pm_model` are therefore _corpus-specific_ -- different corpora may have different frequently-occurring sets, resulting in different vocabularies of phrasings. For instance, you wouldn't expect people in the British House of Commons to ask questions that sound like questions asked to tennis players. In this respect, think of `PhrasingMotifs` like models from scikit learn (e.g., `LogisticRegression`) -- it is fit to a particular dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting frequent itemsets for 81911 sets\n",
      "\tfirst pass: counting itemsets up to and including 5 items large\n",
      "\tfirst pass: 10000/81911 sets processed\n",
      "\tfirst pass: 20000/81911 sets processed\n",
      "\tfirst pass: 30000/81911 sets processed\n",
      "\tfirst pass: 40000/81911 sets processed\n",
      "\tfirst pass: 50000/81911 sets processed\n",
      "\tfirst pass: 60000/81911 sets processed\n",
      "\tfirst pass: 70000/81911 sets processed\n",
      "\tfirst pass: 80000/81911 sets processed\n",
      "\tsecond pass: counting itemsets more than 5 items large\n",
      "\tsecond pass: checking 5897 sets for itemsets of length 6\n",
      "\tsecond pass: checking 1728 sets for itemsets of length 7\n",
      "making itemset tree for 3525 itemsets\n",
      "deduplicating itemsets\n",
      "\tcounting itemset cooccurrences for 10000/78794 collections\n",
      "\tcounting itemset cooccurrences for 20000/78794 collections\n",
      "\tcounting itemset cooccurrences for 30000/78794 collections\n",
      "\tcounting itemset cooccurrences for 40000/78794 collections\n",
      "\tcounting itemset cooccurrences for 50000/78794 collections\n",
      "\tcounting itemset cooccurrences for 60000/78794 collections\n",
      "\tcounting itemset cooccurrences for 70000/78794 collections\n",
      "\tfinding supersets\n"
     ]
    }
   ],
   "source": [
    "pm_model.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most common phrasings and how often they occur in the data (in # of sentences). Note that `('*',)` denotes the null phrasing -- i.e., it encapsulates sentences with _any_ root word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('*',) 81911\n",
      "('what>*',) 12510\n",
      "('is_*',) 10314\n",
      "('how>*',) 9354\n",
      "('do>*',) 8844\n",
      "('think_*',) 7148\n",
      "('think_do',) 6169\n",
      "('think_*', 'think_do') 6169\n",
      "('is>*',) 5879\n",
      "('feel_*',) 5768\n",
      "('was_*',) 5467\n",
      "('is>*', 'is_*') 5325\n",
      "('did>*',) 4686\n",
      "('are_*',) 4031\n",
      "('feel_do',) 3367\n",
      "('feel_*', 'feel_do') 3367\n",
      "('are>*',) 2931\n",
      "('do>*', 'think_*') 2816\n",
      "('do>*', 'think_do') 2799\n",
      "('do>*', 'think_*', 'think_do') 2799\n",
      "('have_*',) 2670\n",
      "('can>*',) 2598\n",
      "('was>*',) 2430\n",
      "('what>do',) 2345\n",
      "('what>*', 'what>do') 2345\n",
      "('was>*', 'was_*') 2188\n",
      "('were_*',) 2109\n",
      "('how>do',) 2072\n",
      "('how>*', 'how>do') 2072\n",
      "('when>*',) 1986\n",
      "('do>*', 'feel_*') 1887\n",
      "('do>*', 'feel_do') 1881\n",
      "('do>*', 'feel_*', 'feel_do') 1881\n",
      "('have>*',) 1767\n",
      "(\"'s_*\",) 1744\n",
      "('are>*', 'are_*') 1685\n",
      "('does>*',) 1620\n",
      "('feel_did',) 1596\n",
      "('feel_*', 'feel_did') 1596\n",
      "('think_what',) 1575\n",
      "('think_*', 'think_what') 1575\n",
      "('feel_how',) 1529\n",
      "('feel_*', 'feel_how') 1529\n",
      "('were>*',) 1511\n",
      "('going_*',) 1478\n",
      "('have_do',) 1458\n",
      "('have_*', 'have_do') 1458\n",
      "('feel_*', 'how>*') 1438\n",
      "('think_*', 'what>*') 1422\n",
      "('so>*',) 1368\n"
     ]
    }
   ],
   "source": [
    "pm_model.print_top_phrasings(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having \"trained\", or fitted our model, we can then use it to annotate each (question) utterance in the corpus with the phrasings this utterance contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = pm_model.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PhrasingMotifs` will actually annotate utterances with _two_ fields. `question_motifs` lists all the phrasings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel_* feel_*__feel_do feel_*__feel_do__feel_how feel_*__feel_do__how>* feel_*__feel_how feel_*__how>* how>* how>*__how>do',\n",
       " 'feel_* feel_*__feel_does feel_*__feel_does__feel_how feel_*__feel_how feel_*__feel_how__feel_now feel_*__feel_now feel_*__feel_now__how>* feel_*__how>* how>* how>*__how>does']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('question_motifs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if we think of phrasings as sets of arcs, then a sentence that has phrasing `(arc1, arc2, arc3)` will also have phrasing `(arc1, arc2)`. Intuitively, more finely-specified phrasings (i.e., the 3-arc case) more closely specify the phrasing embodied by a sentence. As such, we list only the most finely-specified phrasings in `question_motifs__sink`. (while this roughly corresponds to the number of arcs in the phrasing, a more detailed description can be found in the paper and code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel_*__feel_do__how>*',\n",
       " 'feel_*__feel_does__feel_how feel_*__feel_now__how>*']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('question_motifs__sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may still object and say that a \"how _do_ X feel?\" and a \"how _does_ X feel?\" question still are roughly of the same abstract type -- this is just a grammatical difference, and the phrasing algorithm won't capture it because the root words of the sentences (do vs does) are different. There are ways to associate these two sentences, but here we must use some linear algebra (see TODO, the other notebook for details).\n",
    "\n",
    "\n",
    "For now, we'll save a subset of our output to disk, potentially for use in a later transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.dump_info('utterance', ['question_motifs', 'question_motifs__sink', 'arcs_censored'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing itemset counts\n",
      "writing downlinks\n",
      "writing itemset to ids\n",
      "writing meta information\n"
     ]
    }
   ],
   "source": [
    "pm_model.dump_model(os.path.join(ROOT_DIR, 'pm_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downlinks.json\titemset_counts.json  itemset_to_ids.json  meta.json\r\n"
     ]
    }
   ],
   "source": [
    "pm_model_dir = os.path.join(ROOT_DIR, 'pm_model')\n",
    "!ls $pm_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pm_model = PhrasingMotifs('question_motifs_new','question_arcs',min_support=50,fit_filter=question_filter,\n",
    "                          verbosity=VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading itemset counts\n",
      "reading downlinks\n",
      "reading itemset to ids\n",
      "reading meta information\n"
     ]
    }
   ],
   "source": [
    "new_pm_model.load_model(os.path.join(ROOT_DIR, 'pm_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt = new_pm_model.transform_utterance(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel_*__feel_do__how>*',\n",
       " 'feel_*__feel_does__feel_how feel_*__feel_now__how>*']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('question_motifs__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel_*__feel_do__how>*',\n",
       " 'feel_*__feel_does__feel_how feel_*__feel_now__how>*']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('question_motifs_new__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_arc_pipe_full = ConvokitPipeline([\n",
    "    ('shallow_arcs_full', TextToArcs('root_arcs', input_field='parsed', \n",
    "                               root_only=True, verbosity=VERBOSITY)),\n",
    "    ('question_sentence_filter', QuestionSentences('question_arcs_full', input_field='root_arcs',\n",
    "                                         input_filter=question_filter, verbosity=VERBOSITY)),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n",
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = q_arc_pipe_full.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noun_pm_model = PhrasingMotifs('question_motifs_full','question_arcs_full',min_support=50,\n",
    "                               fit_filter=question_filter, max_naive_itemset_size=4,\n",
    "                          verbosity=VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting frequent itemsets for 81911 sets\n",
      "\tfirst pass: counting itemsets up to and including 4 items large\n",
      "\tfirst pass: 10000/81911 sets processed\n",
      "\tfirst pass: 20000/81911 sets processed\n",
      "\tfirst pass: 30000/81911 sets processed\n",
      "\tfirst pass: 40000/81911 sets processed\n",
      "\tfirst pass: 50000/81911 sets processed\n",
      "\tfirst pass: 60000/81911 sets processed\n",
      "\tfirst pass: 70000/81911 sets processed\n",
      "\tfirst pass: 80000/81911 sets processed\n",
      "\tsecond pass: counting itemsets more than 4 items large\n",
      "\tsecond pass: checking 29918 sets for itemsets of length 5\n",
      "\tsecond pass: checked 10000/29918 sets for itemsets of length 5\n",
      "\tsecond pass: checked 20000/29918 sets for itemsets of length 5\n",
      "\tsecond pass: checking 21232 sets for itemsets of length 6\n",
      "\tsecond pass: checked 10000/21232 sets for itemsets of length 6\n",
      "\tsecond pass: checked 20000/21232 sets for itemsets of length 6\n",
      "\tsecond pass: checking 9206 sets for itemsets of length 7\n",
      "\tsecond pass: checking 1570 sets for itemsets of length 8\n",
      "making itemset tree for 8953 itemsets\n",
      "deduplicating itemsets\n",
      "\tcounting itemset cooccurrences for 10000/81892 collections\n",
      "\tcounting itemset cooccurrences for 20000/81892 collections\n",
      "\tcounting itemset cooccurrences for 30000/81892 collections\n",
      "\tcounting itemset cooccurrences for 40000/81892 collections\n",
      "\tcounting itemset cooccurrences for 50000/81892 collections\n",
      "\tcounting itemset cooccurrences for 60000/81892 collections\n",
      "\tcounting itemset cooccurrences for 70000/81892 collections\n",
      "\tcounting itemset cooccurrences for 80000/81892 collections\n",
      "\tfinding supersets\n"
     ]
    }
   ],
   "source": [
    "noun_pm_model.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('*',) 81911\n",
      "('what>*',) 12573\n",
      "('is_*',) 10333\n",
      "('how>*',) 9427\n",
      "('do>*',) 8863\n",
      "('do>you',) 8605\n",
      "('do>*', 'do>you') 8605\n",
      "('think_*',) 7158\n",
      "('think_you',) 7000\n",
      "('think_*', 'think_you') 7000\n",
      "('think_do',) 6179\n",
      "('think_*', 'think_do') 6179\n",
      "('think_do', 'think_you') 6164\n",
      "('think_*', 'think_do', 'think_you') 6164\n",
      "('is>*',) 5897\n",
      "('feel_*',) 5780\n",
      "('was_*',) 5469\n",
      "('is>*', 'is_*') 5325\n",
      "('feel_you',) 4928\n",
      "('feel_*', 'feel_you') 4928\n",
      "('did>*',) 4756\n",
      "('are_*',) 4038\n",
      "('did>you',) 3539\n",
      "('did>*', 'did>you') 3539\n",
      "('is_it',) 3456\n",
      "('is_*', 'is_it') 3456\n",
      "('feel_do',) 3369\n",
      "('feel_*', 'feel_do') 3369\n",
      "('feel_do', 'feel_you') 3355\n",
      "('feel_*', 'feel_do', 'feel_you') 3355\n",
      "('are>*',) 2934\n",
      "('do>*', 'think_*') 2816\n",
      "('do>*', 'think_do') 2799\n",
      "('do>*', 'think_*', 'think_do') 2799\n",
      "('do>*', 'think_you') 2798\n",
      "('do>*', 'think_*', 'think_you') 2798\n",
      "('do>*', 'think_do', 'think_you') 2794\n",
      "('do>*', 'think_*', 'think_do', 'think_you') 2794\n",
      "('do>you', 'think_*') 2784\n",
      "('do>*', 'do>you', 'think_*') 2784\n",
      "('do>you', 'think_you') 2769\n",
      "('do>*', 'do>you', 'think_you') 2769\n",
      "('do>you', 'think_*', 'think_you') 2769\n",
      "('do>*', 'do>you', 'think_*', 'think_you') 2769\n",
      "('do>you', 'think_do') 2767\n",
      "('do>*', 'do>you', 'think_do') 2767\n",
      "('do>you', 'think_*', 'think_do') 2767\n",
      "('do>*', 'do>you', 'think_*', 'think_do') 2767\n",
      "('do>you', 'think_do', 'think_you') 2765\n",
      "('do>*', 'do>you', 'think_do', 'think_you') 2765\n"
     ]
    }
   ],
   "source": [
    "noun_pm_model.print_top_phrasings(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/163948 utterances processed\n",
      "20000/163948 utterances processed\n",
      "30000/163948 utterances processed\n",
      "40000/163948 utterances processed\n",
      "50000/163948 utterances processed\n",
      "60000/163948 utterances processed\n",
      "70000/163948 utterances processed\n",
      "80000/163948 utterances processed\n",
      "90000/163948 utterances processed\n",
      "100000/163948 utterances processed\n",
      "110000/163948 utterances processed\n",
      "120000/163948 utterances processed\n",
      "130000/163948 utterances processed\n",
      "140000/163948 utterances processed\n",
      "150000/163948 utterances processed\n",
      "160000/163948 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = noun_pm_model.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel_*__feel_do__how>* feel_*__feel_you feel_*__feel_you__how>* feel_*__how>* how>* how>*__how>do',\n",
       " 'feel_* feel_*__feel_does feel_*__feel_does__feel_how feel_*__feel_how feel_*__feel_how__feel_now feel_*__feel_now feel_*__feel_now__how>* feel_*__how>* how>* how>*__how>does']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('question_motifs_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
