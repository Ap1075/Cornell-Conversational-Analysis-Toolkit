{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_2019-10-30_\n",
    "\n",
    "This notebook demos a few proposed additions/changes to ConvoKit:\n",
    "\n",
    "* A `TextProcessor` base class that maps per-utterance attributes to per-utterance outputs;\n",
    "* A `TextParser` class that does dependency parsing;\n",
    "* Selective and decoupled data storage and loading;\n",
    "* Per-utterance calls to a transformer;\n",
    "* Pipelining transformers. \n",
    "\n",
    "I've also left a bunch of questions and uncertainties (ctrl-f `Question:`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: loading an existing corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we load a clean version of a corpus. For speed I'm using a 200-utterance subset of the tennis corpus (in a release we'd go through the extra steps of subsetting tennis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kitchen/convokit_corpora/tennis-mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the original (and familiar) directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversations.json  corpus.json  index.json  users.json  utterances.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "ls $ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 9\n",
      "Number of Utterances: 200\n",
      "Number of Conversations: 100\n"
     ]
    }
   ],
   "source": [
    "corpus = convokit.Corpus(ROOT_DIR)\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the following example utterance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_utt_id = '1681_14.a'\n",
    "utt = corpus.get_utterance(test_utt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yeah, but many friends went with me, Japanese guy. So I wasn't -- I wasn't like homesick. But now sometimes I get homesick.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, `utt.meta` contains only the fields that we released with the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_answer': True, 'is_question': False, 'pair_idx': '1681_14'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and nothing else.\n",
    "\n",
    "The following call is equivalent to `utt.meta.get('parsed', None)`. In other words, `get_info` is a wrapper on top of directly accessing the `meta` dictionary, and its default behaviour returns `None` when the particular field doesn't exist.\n",
    "\n",
    "Having this wrapper and its counterpart, `utt.set_info(key, value)` hopefully enables any changes to how data is organized in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt.get_info('parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TextProcessor class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our transformers are per-utterance mappings of one attribute of an utterance to another. To facilitate this, I implemented a `TextProcessor` class that inherits from `Transformer`. \n",
    "\n",
    "`TextProcessor` is initialized with the following arguments:\n",
    "\n",
    "* `proc_fn`: the mapping function. Supports one of two function signatures: `proc_fn(input)` and `proc_fn(input, auxiliary_info)`. \n",
    "* `input_field`: the attribute of the utterance that `proc_fn` will take as input. If set to `None`, will default to reading `utt.text`, as seems to be presently done.\n",
    "* `output_field`: the name of the attribute that the output of `proc_fn` will be written to. \n",
    "* `aux_input`: any auxiliary input that `proc_fn` needs (e.g., a pre-loaded model); passed in as a dict.\n",
    "* `input_filter`: a boolean function of signature `input_filter(utterance, aux_input)`, where `aux_input` is again passed as a dict. If this returns `False` then the particular utterance will be skipped; by default it will always return `True`.\n",
    "\n",
    "Both `input_field` and `output_field` support multiple items -- that is, `proc_fn` could take in multiple attributes of an utterance and output multiple attributes. I'll show how this works below.\n",
    "\n",
    "\"Attribute\" is a deliberately generic term. `TextProcessor` could produce \"features\" as we may conventionally think of them (e.g., wordcount, politeness strategies). It can also be used to pre-process text, i.e., generate alternate representations of the text. The line between what's counted as feature and representation is blurry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: design of function calls\n",
    "\n",
    "Note that one option is for the function signatures of `proc_fn` and `input_filter` to match -- i.e., both take in entire utterances, rather than `proc_fn` taking in only select attributes. The perhaps hacky rationale for why this isn't presently the case is as follows:\n",
    "\n",
    "* `proc_fn` could in principle be called on stand-alone input that has nothing to do with a Corpus: as long as you have _any_ text -- albeit formatted correctly (e.g., you'd still have to know when to pass in a dependency parse, versus a raw string) -- you can make a call to `proc_fn`, without going through `TextProcessor`. Consequently, you should also be able to write a `proc_fn` without knowing what ConvoKit is.\n",
    "* `input_filter` is a decision you make that's contingent on the Corpus: for instance, you may not want to parse things with some corpus-specific metadata. As such, we'd actually want to see the Utterance object in this case. \n",
    "* `input_filter` is an advanced use case; most people will just interact with `proc_fn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple example -- cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, suppose we want to remove hyphens `--` from the text as a preprocessing step. To use `TextProcessor` to do this for us, we'd define the following as a `proc_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(' -- ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we initialize `prep`, a `TextProcessor` object that will run `preprocess_text` on each utterance.\n",
    "\n",
    "When we call `prep.transform()`, the following will occur:\n",
    "\n",
    "* Because we didn't specify an input field, `prep` will pass `utterance.text` into `preprocess_text`\n",
    "* It will write the output -- the text minus the hyphens -- to a field called `clean_text`. Under the surface, we are calling `utt.set_info('clean_text', <output>)`. Currently this `set_info` wrapper is equivalent to `utt.meta['clean_text'] = <output>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep = TextProcessor(proc_fn=preprocess_text, output_field='clean_text')\n",
    "corpus = prep.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as desired, we now have a new field in `utt` -- presently stored as an entry in `utt.meta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yeah, but many friends went with me, Japanese guy. So I wasn't I wasn't like homesick. But now sometimes I get homesick.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('clean_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some advanced usage: playing around with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of the following is to demonstrate more elaborate calls to `TextProcessor`, and also to show that `TextProcessor` is agnostic to whether we are producing a representation or a feature. I'll demo these points by way of wordcount.\n",
    "\n",
    "First, we'll initialize a `TextProcessor` that does wordcounts (i.e., `len(x.split())`) on just the raw text (`utt.text`), writing output to field `wc_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc_raw = TextProcessor(proc_fn=lambda x: len(x.split()), output_field='wc_raw')\n",
    "corpus = wc_raw.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('wc_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead wanted to wordcount our preprocessed text, with the hyphens removed, we can specify `input_field='clean_text'` -- as such, the `TextProcessor` will read from `utt.get_info('clean_text')` instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc = TextProcessor(proc_fn=lambda x: len(x.split()), output_field='wc', input_field='clean_text')\n",
    "corpus = wc.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we are no longer counting the extra hyphen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('wc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can count characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = TextProcessor(proc_fn=lambda x: len(x), output_field='ch', input_field='clean_text')\n",
    "corpus = chars.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some reason we now wanted to calculate:\n",
    "\n",
    "* characters per word\n",
    "* words per character (the reciprocal)\n",
    "\n",
    "This requires:\n",
    "\n",
    "* a `TextProcessor` that takes in multiple input fields, `'ch'` and `'wc'`;\n",
    "* and that writes to multiple output fields, `'char_per_word'` and `'word_per_char'`.\n",
    "\n",
    "Here's how the resultant object, `char_per_word`, handles this:\n",
    "\n",
    "* in `transform()`, we pass `proc_fn` a dict mapping input field name to value, e.g., `{'wc': 22, 'ch': 120}`\n",
    "* `proc_fn` will be written to return a tuple, where each element of that tuple corresponds to each element of the list we've passed to `output_field`, e.g., \n",
    "\n",
    "```out0, out1 = proc_fn(input)\n",
    "utt.set_info('char_per_word', out0) \n",
    "utt.set_info('word_per_char', out1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_per_word = TextProcessor(proc_fn=lambda x: (x['ch']/x['wc'], x['wc']/x['ch']), \n",
    "                              output_field=['char_per_word', 'word_per_char'], input_field=['ch','wc'])\n",
    "corpus = char_per_word.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.454545454545454"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('char_per_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18333333333333332"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('word_per_char')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a bunch of new fields pertaining to the attributes we've computed, presently stored as entries in `utt.meta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is_answer', 'is_question', 'pair_idx', 'clean_text', 'wc_raw', 'wc', 'ch', 'char_per_word', 'word_per_char'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.meta.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: default behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present, `TextProcessing` basically does no error handling. Here's some cases that come to mind:\n",
    "\n",
    "* If an utterance does not contain `input_field`, then we silently skip over the utterance. As such, utterances that are missing this attribute will not contain the resultant `output_field` (this is _not the same_ as the `output_field` existing and being set to `None`)\n",
    "* An utterance must contain _all_ of the `input_field`s if we've passed in multiple. Otherwise, the silent skipping-over behavior occurs. \n",
    "\n",
    "One nice behavior might be for `transform(corpus)` to throw an error on corpora where we know that `input_field` doesn't exist for _any_ utterance. This would require the corpus to maintain a registry of metadata and fields which have been computed before (something we're talking about already)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing text with the TextParser class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common utterance-level thing we want to do is parse the text. In practice, in increasing order of (computational) difficulty, this typically entails:\n",
    "\n",
    "* proper tokenizing of words and sentences;\n",
    "* POS-tagging;\n",
    "* dependency-parsing. \n",
    "\n",
    "As such, we provide a `TextParser` class that inherits from `TextProcessor` to do all of this, taking in the following arguments:\n",
    "\n",
    "* `output_field`: defaults to `'parsed'`\n",
    "* `input_field`\n",
    "* `mode`: whether we want to go through all of the above steps (which may be expensive) or stop mid-way through. Supports the following options: `'tokenize'`, `'tag'`, `'parse'` (the default).\n",
    "\n",
    "Under the surface, `TextParser` actually uses two separate models: a `spacy` object that does word tokenization, tagging and parsing _per sentence_, and `nltk`'s sentence tokenizer. The rationale is:\n",
    "\n",
    "* `spacy` doesn't support sentence tokenization without dependency-parsing, and we often want sentence tokenization without having to go through the effort of parsing.\n",
    "* We want to be consistent (as much as possible, given changes to spacy and nltk) in the tokenizations we produce, between runs where we don't want parsing and runs where we do.\n",
    "\n",
    "If we've pre-loaded these models, we can pass them into the constructor too, as:\n",
    "\n",
    "* `spacy_nlp`\n",
    "* `sent_tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = TextParser(input_field='clean_text', verbosity=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050/200 utterances processed\n",
      "100/200 utterances processed\n",
      "150/200 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = parser.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_parse = utt.get_info('parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse output\n",
    "\n",
    "A parse produced by `TextParser` is serialized in text form (to avoid several memory and ease-of-use difficulties with spacy binaries). It is a list consisting of sentences, where each sentence is a dict with\n",
    "\n",
    "* `toks`: a list of tokens (i.e., words) in the sentence;\n",
    "* `rt`: the index of the root of the dependency tree (i.e., `sentence['toks'][sentence['rt']` gives the root)\n",
    "\n",
    "Each token, in turn, contains the following:\n",
    "\n",
    "* `tok`: the text of the token;\n",
    "* `tag`: the tag;\n",
    "* `up`: the index of the parent of the token in the dependency tree (no entry for the root);\n",
    "* `down`: the indices of the children of the token;\n",
    "* `dep`: the dependency of the edge between the token and its parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rt': 5,\n",
       " 'toks': [{'dep': 'intj', 'dn': [], 'tag': 'UH', 'tok': 'Yeah', 'up': 5},\n",
       "  {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 5},\n",
       "  {'dep': 'cc', 'dn': [], 'tag': 'CC', 'tok': 'but', 'up': 5},\n",
       "  {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'many', 'up': 4},\n",
       "  {'dep': 'nsubj', 'dn': [3, 10], 'tag': 'NNS', 'tok': 'friends', 'up': 5},\n",
       "  {'dep': 'ROOT', 'dn': [0, 1, 2, 4, 6, 8, 11], 'tag': 'VBD', 'tok': 'went'},\n",
       "  {'dep': 'prep', 'dn': [7], 'tag': 'IN', 'tok': 'with', 'up': 5},\n",
       "  {'dep': 'pobj', 'dn': [], 'tag': 'PRP', 'tok': 'me', 'up': 6},\n",
       "  {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 5},\n",
       "  {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'Japanese', 'up': 10},\n",
       "  {'dep': 'appos', 'dn': [9], 'tag': 'NN', 'tok': 'guy', 'up': 4},\n",
       "  {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '.', 'up': 5}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parse[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we didn't want to go through the trouble of dependency-parsing (which could be expensive) we could initialize `TextParser` with `mode='tag'`, which only POS-tags tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texttagger = TextParser(output_field='tagged', input_field='clean_text', mode='tag')\n",
    "corpus = texttagger.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter the mode, the parse produced will follow the same structure as if we had run the entire pipeline (i.e., a list of sentences which are dicts); it will just be missing a few fields (like `rt`). \n",
    "\n",
    "In principle, this is to maintain consistency between different runs of the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toks': [{'tag': 'UH', 'tok': 'Yeah'},\n",
       "  {'tag': ',', 'tok': ','},\n",
       "  {'tag': 'CC', 'tok': 'but'},\n",
       "  {'tag': 'JJ', 'tok': 'many'},\n",
       "  {'tag': 'NNS', 'tok': 'friends'},\n",
       "  {'tag': 'VBD', 'tok': 'went'},\n",
       "  {'tag': 'IN', 'tok': 'with'},\n",
       "  {'tag': 'PRP', 'tok': 'me'},\n",
       "  {'tag': ',', 'tok': ','},\n",
       "  {'tag': 'JJ', 'tok': 'Japanese'},\n",
       "  {'tag': 'NN', 'tok': 'guy'},\n",
       "  {'tag': '.', 'tok': '.'}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('tagged')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some advanced usage: input filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the sake of demonstration, suppose we wished to save some computation time and only parse the questions in a corpus. We can do this by specifying `input_filter` (which, recall discussion above, takes as argument an `Utterance` object). (We note that especially if the corpus comes from an institutional setting, there may be official definitions for what a question is, beyond the presence or absence of a question mark in the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_question(utt, aux={}):\n",
    "    return utt.meta['is_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qparser = TextParser(output_field='qparsed', input_field='clean_text', input_filter=is_question, verbosity=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050/200 utterances processed\n",
      "100/200 utterances processed\n",
      "150/200 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = qparser.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our test utterance is not a question, `qparser.transform()` will skip over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt.get_info('qparsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we take the question that triggered the answer, we see that it is indeed parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_utt_id = '1681_14.q'\n",
    "q_utt = corpus.get_utterance(q_utt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How hard was it for you when, 13 years, left your parents, left Japan to go to the States. Was it a big step for you?'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_utt.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rt': 2,\n",
       "  'toks': [{'dep': 'advmod', 'dn': [], 'tag': 'WRB', 'tok': 'How', 'up': 1},\n",
       "   {'dep': 'acomp', 'dn': [0], 'tag': 'JJ', 'tok': 'hard', 'up': 2},\n",
       "   {'dep': 'ROOT', 'dn': [1, 3, 4, 11, 22], 'tag': 'VBD', 'tok': 'was'},\n",
       "   {'dep': 'nsubj', 'dn': [], 'tag': 'PRP', 'tok': 'it', 'up': 2},\n",
       "   {'dep': 'prep', 'dn': [5], 'tag': 'IN', 'tok': 'for', 'up': 2},\n",
       "   {'dep': 'pobj', 'dn': [], 'tag': 'PRP', 'tok': 'you', 'up': 4},\n",
       "   {'dep': 'advmod', 'dn': [], 'tag': 'WRB', 'tok': 'when', 'up': 11},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 11},\n",
       "   {'dep': 'nummod', 'dn': [], 'tag': 'CD', 'tok': '13', 'up': 9},\n",
       "   {'dep': 'npadvmod', 'dn': [8], 'tag': 'NNS', 'tok': 'years', 'up': 11},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 11},\n",
       "   {'dep': 'advcl',\n",
       "    'dn': [6, 7, 9, 10, 13, 14, 15],\n",
       "    'tag': 'VBD',\n",
       "    'tok': 'left',\n",
       "    'up': 2},\n",
       "   {'dep': 'poss', 'dn': [], 'tag': 'PRP$', 'tok': 'your', 'up': 13},\n",
       "   {'dep': 'dobj', 'dn': [12], 'tag': 'NNS', 'tok': 'parents', 'up': 11},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 11},\n",
       "   {'dep': 'conj', 'dn': [16, 18], 'tag': 'VBD', 'tok': 'left', 'up': 11},\n",
       "   {'dep': 'dobj', 'dn': [], 'tag': 'NNP', 'tok': 'Japan', 'up': 15},\n",
       "   {'dep': 'aux', 'dn': [], 'tag': 'TO', 'tok': 'to', 'up': 18},\n",
       "   {'dep': 'xcomp', 'dn': [17, 19], 'tag': 'VB', 'tok': 'go', 'up': 15},\n",
       "   {'dep': 'prep', 'dn': [21], 'tag': 'IN', 'tok': 'to', 'up': 18},\n",
       "   {'dep': 'det', 'dn': [], 'tag': 'DT', 'tok': 'the', 'up': 21},\n",
       "   {'dep': 'pobj', 'dn': [20], 'tag': 'NNPS', 'tok': 'States', 'up': 19},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '.', 'up': 2}]},\n",
       " {'rt': 0,\n",
       "  'toks': [{'dep': 'ROOT', 'dn': [1, 4, 7], 'tag': 'VBD', 'tok': 'Was'},\n",
       "   {'dep': 'nsubj', 'dn': [], 'tag': 'PRP', 'tok': 'it', 'up': 0},\n",
       "   {'dep': 'det', 'dn': [], 'tag': 'DT', 'tok': 'a', 'up': 4},\n",
       "   {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'big', 'up': 4},\n",
       "   {'dep': 'attr', 'dn': [2, 3, 5], 'tag': 'NN', 'tok': 'step', 'up': 0},\n",
       "   {'dep': 'prep', 'dn': [6], 'tag': 'IN', 'tok': 'for', 'up': 4},\n",
       "   {'dep': 'pobj', 'dn': [], 'tag': 'PRP', 'tok': 'you', 'up': 5},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '?', 'up': 0}]}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_utt.get_info('qparsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downstream application of parses: getting arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fully-parsed utterances, one thing we can do is extract all of the dependency-tree arcs -- effectively, fancy bigrams. To facilitate this, we include the following class `TextToArcs`, which also inherits from `TextProcessor`. By default, `TextToArcs` will use the parse as its input field (and hence requires the parse to exist). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextToArcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_arcs = TextToArcs('arcs')\n",
    "corpus = get_arcs.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextToArcs` returns a list of sentences, where each sentence is represented as a space-separated string of arcs. The main takeaway is that sometimes we might want utterances to be represented in a segmented way -- e.g., as a list of sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends_* friends_guy friends_many guy_* guy_japanese japanese_* many_* me_* went_* went_friends went_with went_yeah with_* with_me yeah>* yeah_*',\n",
       " 'i_* so>* so>i so_* was_* was_i was_so',\n",
       " 'homesick_* i>* i_* like_* was_* was_homesick was_i was_like',\n",
       " 'but>* but>now get_* get_homesick get_i get_now get_sometimes homesick_* i_* now_* sometimes_*']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('arcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: segmenting utterances\n",
    "\n",
    "I can see a lot of use cases where we actually compute things over _segments_ of utterances -- sentences, but also paragraphs, subsections, etc. Down the line, this could get a bit hard to manage, if different `TextProcessor` objects are expecting to find segmented versus unsegmented utterances; also note that calling `TextParser` is effectively a precondition to having sentences (that could be used in future transformers).\n",
    "\n",
    "Right now, many of the things I implement that inherit from `TextProcessor` down the line do this:\n",
    "\n",
    "* If the output is per-sentence, it will return a list where each entry of that list is a sentence. In principle, this facilitates calls where we want to correspond a sentence-level attribute to the original text of the parse. \n",
    "* If the input requires per-sentence segmentation, the transformer will just not work if you give it a string instead. If the input doesn't care, the transformer will check if you've given it a string or a list of strings, and in the latter case call `'\\n'.join(input)`. This motivates the next question:\n",
    "\n",
    "### Question: what do TextProcessors expect as input?\n",
    "\n",
    "Currently it's up to whoever writes the particular `TextProcessor`, and its in their head, what is the specification of the input (i.e., whether they're expecting a list of dicts, a list of strings, or a flat string, etc.) Maybe there's no way around this, or maybe there are conventions that one could set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and loading corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now computed a bunch of utterance-level attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is_answer', 'is_question', 'pair_idx', 'clean_text', 'wc_raw', 'wc', 'ch', 'char_per_word', 'word_per_char', 'parsed', 'tagged', 'arcs'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.meta.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, calling `corpus.dump` will write all of these attributes to disk, within the file that stores utterances; later calling `corpus.load` will load all of these attributes back into a new corpus. This is messy for a few reasons:\n",
    "\n",
    "* For big objects like parses, this incurs a high computational burden (especially if in a later use case you might not even need to look at parses)\n",
    "* There are lots of nonsense attributes we created around the way -- maybe as intermediate output, or as experiments we aren't committed to keeping. We don't necessarily want to enshrine these as a part of the corpus in the future.\n",
    "* It's just really messy to cram everything into `utterance.meta`. In particular, maybe there ought to be a clear distinction between fields that we believe are \"core\" to the corpus (like an institutional annotation for what constitutes a question), and things that we compute on top of the corpus, that perhaps generalize across different corpora (like parses). \n",
    "\n",
    "Note that these types of considerations generalize beyond Utterances to the other types of Corpus objects, Users and Conversations.\n",
    "\n",
    "I'll leave the last point open for now, since actually trying to taxonomize things we can compute on top of an utterance is hard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first two points, I've made a few tweaks to corpus.\n",
    "\n",
    "First, `corpus.dump` now takes an optional argument `fields_to_skip`, which is a dict of object type (`'utterance'`, `'conversation'`, `'user'`, `'corpus'`) to a list of fields that we do not want to write to disk. As is presently implemented, these fields to skip will also not be logged to `corpus.meta_index`.\n",
    "\n",
    "The following call will _only_ write the following new fields: `arcs` and `clean_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.dump('./', save_to_existing_path=True, \n",
    "            fields_to_skip={'utterance': ['parsed','tagged','wc_raw','ch',\n",
    "                                         'char_per_word','word_per_char','qparsed']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, to deal with fields that we'd like to keep around, but that we don't want to read and write to disk in a big batch with all the other corpus data, `corpus.dump_info` will dump fields of a Corpus object into separate files. This takes the following arguments as input:\n",
    "\n",
    "* `obj_type`: which type of Corpus object you're dealing with.\n",
    "* `fields`: a list of the fields to write. \n",
    "* `dir_name`: which directory to write to; by default will write to the directory you read the corpus from.\n",
    "\n",
    "This function will write each field in `fields` to a separate file called `info.<field>.jsonl` where each line of the file is a json-serialized dict: `{\"id\": <ID of object>, \"value\": <object.get_info(field)>}`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.dump_info('utterance',['parsed','tagged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we now have the following files in our directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversations.json  index.json         info.tagged.jsonl  utterances.jsonl\r\n",
      "corpus.json         info.parsed.jsonl  users.json\r\n"
     ]
    }
   ],
   "source": [
    "ls $ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now initialize a new corpus by reading from this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = convokit.Corpus(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_utt = new_corpus.get_utterance(test_utt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that things that we've omitted in the `corpus.dump` call will not be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is_answer', 'is_question', 'pair_idx', 'clean_text', 'wc', 'arcs'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_utt.meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends_* friends_guy friends_many guy_* guy_japanese japanese_* many_* me_* went_* went_friends went_with went_yeah with_* with_me yeah>* yeah_*',\n",
       " 'i_* so>* so>i so_* was_* was_i was_so',\n",
       " 'homesick_* i>* i_* like_* was_* was_homesick was_i was_like',\n",
       " 'but>* but>now get_* get_homesick get_i get_now get_sometimes homesick_* i_* now_* sometimes_*']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_utt.get_info('arcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a counterpart to `corpus.dump_info` we can also load auxiliary information on-demand. Here, this call will look for `info.<field>.jsonl` in the directory of `new_corpus` (or an optionally-specified `dir_name`) and attach the value specified in each line of the file to the utterance with the associated id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus.load_info('utterance',['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rt': 5,\n",
       "  'toks': [{'dep': 'intj', 'dn': [], 'tag': 'UH', 'tok': 'Yeah', 'up': 5},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 5},\n",
       "   {'dep': 'cc', 'dn': [], 'tag': 'CC', 'tok': 'but', 'up': 5},\n",
       "   {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'many', 'up': 4},\n",
       "   {'dep': 'nsubj', 'dn': [3, 10], 'tag': 'NNS', 'tok': 'friends', 'up': 5},\n",
       "   {'dep': 'ROOT', 'dn': [0, 1, 2, 4, 6, 8, 11], 'tag': 'VBD', 'tok': 'went'},\n",
       "   {'dep': 'prep', 'dn': [7], 'tag': 'IN', 'tok': 'with', 'up': 5},\n",
       "   {'dep': 'pobj', 'dn': [], 'tag': 'PRP', 'tok': 'me', 'up': 6},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 5},\n",
       "   {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'Japanese', 'up': 10},\n",
       "   {'dep': 'appos', 'dn': [9], 'tag': 'NN', 'tok': 'guy', 'up': 4},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '.', 'up': 5}]},\n",
       " {'rt': 2,\n",
       "  'toks': [{'dep': 'advmod', 'dn': [], 'tag': 'RB', 'tok': 'So', 'up': 2},\n",
       "   {'dep': 'nsubj', 'dn': [], 'tag': 'PRP', 'tok': 'I', 'up': 2},\n",
       "   {'dep': 'ROOT', 'dn': [0, 1, 3], 'tag': 'VBD', 'tok': 'was'},\n",
       "   {'dep': 'neg', 'dn': [], 'tag': 'RB', 'tok': \"n't\", 'up': 2}]},\n",
       " {'rt': 1,\n",
       "  'toks': [{'dep': 'nsubj', 'dn': [], 'tag': 'PRP', 'tok': 'I', 'up': 1},\n",
       "   {'dep': 'ROOT', 'dn': [0, 2, 3, 4, 5], 'tag': 'VBD', 'tok': 'was'},\n",
       "   {'dep': 'neg', 'dn': [], 'tag': 'RB', 'tok': \"n't\", 'up': 1},\n",
       "   {'dep': 'prep', 'dn': [], 'tag': 'UH', 'tok': 'like', 'up': 1},\n",
       "   {'dep': 'acomp', 'dn': [], 'tag': 'NN', 'tok': 'homesick', 'up': 1},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '.', 'up': 1}]},\n",
       " {'rt': 4,\n",
       "  'toks': [{'dep': 'cc', 'dn': [], 'tag': 'CC', 'tok': 'But', 'up': 4},\n",
       "   {'dep': 'advmod', 'dn': [], 'tag': 'RB', 'tok': 'now', 'up': 4},\n",
       "   {'dep': 'advmod', 'dn': [], 'tag': 'RB', 'tok': 'sometimes', 'up': 4},\n",
       "   {'dep': 'nsubj', 'dn': [], 'tag': 'PRP', 'tok': 'I', 'up': 4},\n",
       "   {'dep': 'ROOT', 'dn': [0, 1, 2, 3, 5, 6], 'tag': 'VBP', 'tok': 'get'},\n",
       "   {'dep': 'acomp', 'dn': [], 'tag': 'NN', 'tok': 'homesick', 'up': 4},\n",
       "   {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '.', 'up': 4}]}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_utt.get_info('parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Error handling\n",
    "\n",
    "If `dump_info` is called on a non-existent field, then it will write an entire file consisting of `{'id': <ID>, 'value': None}`, one line per number of objects in the Corpus. (It could in principle skip objects which don't have that field and hence write an empty file, though see next question.) Here again, we could facilitate error-checking with a Corpus-level metadata registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Partial loading\n",
    "\n",
    "Ideally, we'd like `load_info` to behave exactly like the corpus constructor when we are only reading a subset of utterances -- that is, passing the same `utterance_start_index` and `utterance_end_index` to `load_info` as to the corpus constructor should return auxiliary information corresponding to the subset of utterances that were loaded. \n",
    "\n",
    "The issue here is that I don't think a corpus currently has a \"canonical\" order in which utterances are listed (I see calls to `list(corpus.get_utterance_ids())` which aren't guaranteed to be deterministic). And of course, this partial loading would have to be implemented in `load_info`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Default behavior of corpus.dump\n",
    "\n",
    "This function call looks terrible, especially if you realize that after the dust has settled we've only written two additional fields to disk.\n",
    "\n",
    "```corpus.dump('./', save_to_existing_path=True, \n",
    "            fields_to_skip={'utterance': ['parsed','tagged','wc_raw','ch',\n",
    "             'char_per_word','word_per_char','qparsed']})\n",
    "```\n",
    "\n",
    "\n",
    "Another consideration is that when we dump corpora, we want to make sure that the metadata we shipped the corpus with get dumped as well (e.g., the `is_question` field should always be dumped). So one alternative, making use of the hypothetical metadata registry, could be as follows:\n",
    "\n",
    "* The registry keeps track of the \"canonical\" fields to dump by default;\n",
    "* When we create a new transformer where we can specify the output field, we should also have an option to specify whether this new field should be registered in the registry as a \"dump-by-default\" field, an \"ad-hoc\" field, or a \"probably dump me separately\" field. I don't know what the default behaviour of this should be, but clearly we'd need to set a default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: interoperability with merging corpora\n",
    "\n",
    "I have no idea whether this is the case right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-utterance calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextProcessor` objects also support calls per-utterance via `TextProcessor.transform_utterance()` (in principle, it could also take a list of utterances, but that's not implemented yet). However, in general, these calls require some assumptions about the nature of the utterance you pass in (see pipelining below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, if the `TextProcessor` in question has the input field defaulting to `utt.text` (as is the case with our preprocessor that removes hyphens), then it can either take in a string or an utterance. In either case, it will return an utterance with the new output field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_str = \"I played -- a tennis match.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': None, 'user': None, 'root': None, 'reply_to': None, 'timestamp': None, 'text': 'I played -- a tennis match.', 'meta': {'clean_text': 'I played a tennis match.'}})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.transform_utterance(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.model import Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adhoc_utt = Utterance(text=test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': None, 'user': None, 'root': None, 'reply_to': None, 'timestamp': None, 'text': 'I played -- a tennis match.', 'meta': {'clean_text': 'I played a tennis match.'}})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.transform_utterance(adhoc_utt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise for `wc_raw`, which wordcounts the raw utterance text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': None, 'user': None, 'root': None, 'reply_to': None, 'timestamp': None, 'text': 'I played -- a tennis match.', 'meta': {'wc_raw': 6}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_raw.transform_utterance(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the `TextProcessor` is expecting a particular input field, then it will not take strings as input. This is the case for `wc`, which wordcounts de-hyphenated text, and is hence expecting a field called `clean_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expecting utterance, not string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-019fe1916e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/convokit-2.1.0-py3.6.egg/convokit/text_processing/textProcessor.py\u001b[0m in \u001b[0;36mtransform_utterance\u001b[0;34m(self, utt, override_input_filter)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expecting utterance, not string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverride_input_filter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expecting utterance, not string"
     ]
    }
   ],
   "source": [
    "wc.transform_utterance(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate behavior would be to wrap the string in an utterance but not subsequently annotate the utterance with the output field. This actually works better with the behavior for utterances, which is that if the utterance does not have the desired input field, it will return the utterance with no additional fields annotated. So, uh, TODO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: default behaviors\n",
    "\n",
    "First, recall that `TextProcessor` is initialized with an optional `input_filter` argument. Right now, if a single utterance passed to `transform_utterance` doesn't satisfy the input filter, then the call is a no-op, unless optional argument `override_input_filter` is set to `True`.\n",
    "\n",
    "Second, there's the question of whether erroneous calls should be no-ops, or whether it should raise errors (in the latter case, it's then on the end-user to do the error handling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can string together multiple transformers, and hence `TextProcessors`, into a pipeline. This actually works right out of the box if we use scikit-learn's `Pipeline` class; I've wrapped this up in a `ConvokitPipeline` class which also supports the `transform_utterance` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.convokitPipeline import ConvokitPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose we want to go from raw text to outputting dependency parse arcs. We can chain the required steps to get there by initializing `ConvokitPipeline` with a list of steps, represented as a tuple of `(<step name>, initialized transformer-like object)`:\n",
    "\n",
    "* `'prep'`, our de-hyphenator\n",
    "* `'parse'`, our parser\n",
    "* `'arcs'`, our arc extractor.\n",
    "\n",
    "Step `k` expects that step `k-1` will hand it a corpus (or utterance) with the requisite input fields computed. Note that there's some redundancy here with specifying which input and output fields to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_pipe = ConvokitPipeline([('prep', TextProcessor(preprocess_text, 'clean_text_pipe')),\n",
    "                ('parse', TextParser('parsed_pipe', input_field='clean_text_pipe',\n",
    "                                    verbosity=50)),\n",
    "                ('arcs', TextToArcs('arcs_pipe', input_field='parsed_pipe'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050/200 utterances processed\n",
      "100/200 utterances processed\n",
      "150/200 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = arc_pipe.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends_* friends_guy friends_many guy_* guy_japanese japanese_* many_* me_* went_* went_friends went_with went_yeah with_* with_me yeah>* yeah_*',\n",
       " 'i_* so>* so>i so_* was_* was_i was_so',\n",
       " 'homesick_* i>* i_* like_* was_* was_homesick was_i was_like',\n",
       " 'but>* but>now get_* get_homesick get_i get_now get_sometimes homesick_* i_* now_* sometimes_*']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('arcs_pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, the pipeline also works to transform utterances, assuming that the utterance you've passed in agrees with the input that the very first transformer in the pipeline is expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': None, 'user': None, 'root': None, 'reply_to': None, 'timestamp': None, 'text': 'I played -- a tennis match.', 'meta': {'clean_text_pipe': 'I played a tennis match.', 'parsed_pipe': [{'rt': 1, 'toks': [{'tok': 'I', 'tag': 'PRP', 'dep': 'nsubj', 'up': 1, 'dn': []}, {'tok': 'played', 'tag': 'VBD', 'dep': 'ROOT', 'dn': [0, 4, 5]}, {'tok': 'a', 'tag': 'DT', 'dep': 'det', 'up': 4, 'dn': []}, {'tok': 'tennis', 'tag': 'NN', 'dep': 'compound', 'up': 4, 'dn': []}, {'tok': 'match', 'tag': 'NN', 'dep': 'dobj', 'up': 1, 'dn': [2, 3]}, {'tok': '.', 'tag': '.', 'dep': 'punct', 'up': 1, 'dn': []}]}], 'arcs_pipe': ['a_* i>* i_* match_* match_a match_tennis played_* played_i played_match tennis_*']}})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_pipe.transform_utterance(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: robustness\n",
    "\n",
    "Note that I haven't totally thought through when this basically direct application of `sklearn.pipeline` might not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Pipelining and per-utterance calls\n",
    "\n",
    "Per the implementation here, per-utterance calls on a transformer won't work (either they'll crash if you've passed in a string and we were expecting an input field, or they will silently do nothing) unless we pass in an utterance where all the requisite inpute fields are already there. My proposed way to handle this is:\n",
    "\n",
    "* For individual transformers, it is indeed up to the end-user to provide a well-formated utterance.\n",
    "* If we actually want to implement transformers where the user can start at the very beginning, by passing in a raw string, we would actually need to use a pipeline to get the raw string to the well-formed utterance.\n",
    "\n",
    "Note this isn't quite what Cristian was proposing:\n",
    "\n",
    "* For individual transformers, check if well-formatted. If not, somehow have the transformer keep track of the full antecedent list of transformers, and run the input string through all of them -- in other words, include pipelining with all transformers by default.\n",
    "\n",
    "I disfavor this approach since I feel that it makes actually writing custom transformers really annoying, and undermines the flexibility of having transformers potentially take in different input fields (e.g., being able to specify which variant of a pre-processed text you want to further transform). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
