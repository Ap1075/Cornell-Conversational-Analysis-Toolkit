{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demo some ideas I had about storing spacy parses as text, and more broadly about storing processed versions of utterance text that can be accessed in a fast, resource-friendly way.\n",
    "\n",
    "The basic problem is that often we substantively preprocess the text of an utterance -- for instance, by removing words, or by performing some sort of transformation such as extracting bigrams. These aren't quite like computing highly-compressed features as with Transformers -- often you are storing text that's equal to or even greater in size than the original; while certain Transformers might require that such preprocessed versions be loaded, not all pipelines will require it. A good example is dependency parses, though any sort of text cleaning procedure that you want to precompute also fits under this category. In the case of the QuestionTypology module for instance, we might explore different ways to preprocess the text, like extracting dependency parse arcs.\n",
    "\n",
    "My solution is to add an additional data structure to Corpus objects, called `processed_text`. (See [link](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/prompt-types/convokit/model/corpus.py) and ctrl+f `processed_text` for modifications to the code.) This is a dict of dicts, where each sub-dict maps utterance IDs to processed versions of the text. \n",
    "\n",
    "e.g.:\n",
    "`{'cleaned_text': {utterance_id: clean version of text}, 'parsed': {utterance_id: serialized spacy parse}`\n",
    "\n",
    "This dict isn't loaded by default, but the end-user, or a Transformer, can load particular fields (see the bottom of this notebook for a demonstration); they can also choose to write particular fields to disk. \n",
    "\n",
    "This may be more general than storing processed text, in which case processed_text is a bad name.\n",
    "And yes, there is some ambiguity wrt what constitutes a \"feature\" versus \"preprocessed text\". in the extreme, someone might decide to store all features, even the float-valued ones, in this data structure -- not that we'd recommend it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll demo the functionality on a subset of 100 conversations in the Tennis corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus('/kitchen/convokit_corpora/tennis-mini/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's the original directory structure. this should look familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversations.json  corpus.json  index.json  users.json  utterances.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "ls '/kitchen/convokit_corpora/tennis-mini/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the corpus object is initialized with an empty `processed_text` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's the example utterance we'll work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_utt_id = '1681_14.a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yeah, but many friends went with me, Japanese guy. So I wasn't -- I wasn't like homesick. But now sometimes I get homesick.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_utterance(test_utt_id).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextProcessor` is the main class that is in charge of preprocessing utterance texts -- i.e., converting text that either is stored in the utterance object or in a field of `processed_text`, to processed text that is stored in another field of `processed_text`.\n",
    "\n",
    "It inherits from `Transformer` objects, but rather than annotating utterances, it stores output in the relevant field of `processed_text`. The arguments it needs to take in are:\n",
    "\n",
    "* `proc_fn`: a function for processing text that takes in string text and an optional dict `aux_input` of auxiliary inputs. (I couldn't figure out how to get `**kwargs` to work but maybe that's a better way)\n",
    "* `output_field`: the field in `processed_text` that the processed text is stored in.\n",
    "\n",
    "Other arguments:\n",
    "\n",
    "* By default, `TextProcessor` will read text from the utterance object itself (i.e., `utterance.text`). However, specifying argument `input_field` will instead have it read text from that field of `processed_text`.\n",
    "* `aux_input` stores any auxiliary arguments to `proc_fn`. Useful if we need to load something like a `spacy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a basic example of a preprocessing function: removing instances of `--`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.replace(' -- ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_prep = TextProcessor(preprocess_text, 'text')\n",
    "corpus = text_prep.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now have a new field in `processed_text`, which can be accessed with the `corpus.get_processed_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.processed_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yeah, but many friends went with me, Japanese guy. So I wasn't I wasn't like homesick. But now sometimes I get homesick.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_processed_text(test_utt_id, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented some classes that further inherit from `TextProcessor` to handle some particular preprocessing methods. In particular, `TextParser` will dependency parse the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want the output of the parse to be written to the `parsed` field of `processed_text`, and we want the parser to use our preprocessed input in the `text` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050/200 utterances processed\n",
      "100/200 utterances processed\n",
      "150/200 utterances processed\n"
     ]
    }
   ],
   "source": [
    "textparser = TextParser('parsed', input_field='text', verbosity=50)\n",
    "corpus = textparser.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'parsed'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.processed_text.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what a spacy parse, serialized in text form, looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parse = corpus.get_processed_text(test_utt_id, 'parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse is a list consisting of sentences. For each sentence, the `rt` entry denotes the index of the root of the sentence, and the `toks` entry lists the tokens within the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rt': 5,\n",
       " 'toks': [{'dep': 'intj', 'dn': [], 'tag': 'UH', 'tok': 'Yeah', 'up': 5},\n",
       "  {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 5},\n",
       "  {'dep': 'cc', 'dn': [], 'tag': 'CC', 'tok': 'but', 'up': 5},\n",
       "  {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'many', 'up': 4},\n",
       "  {'dep': 'nsubj', 'dn': [3, 10], 'tag': 'NNS', 'tok': 'friends', 'up': 5},\n",
       "  {'dep': 'ROOT', 'dn': [0, 1, 2, 4, 6, 8, 11], 'tag': 'VBD', 'tok': 'went'},\n",
       "  {'dep': 'prep', 'dn': [7], 'tag': 'IN', 'tok': 'with', 'up': 5},\n",
       "  {'dep': 'pobj', 'dn': [], 'tag': 'PRP', 'tok': 'me', 'up': 6},\n",
       "  {'dep': 'punct', 'dn': [], 'tag': ',', 'tok': ',', 'up': 5},\n",
       "  {'dep': 'amod', 'dn': [], 'tag': 'JJ', 'tok': 'Japanese', 'up': 10},\n",
       "  {'dep': 'appos', 'dn': [9], 'tag': 'NN', 'tok': 'guy', 'up': 4},\n",
       "  {'dep': 'punct', 'dn': [], 'tag': '.', 'tok': '.', 'up': 5}]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parse[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token in `toks` contains \n",
    "\n",
    "* `tok`: the text of the token\n",
    "* `tag`: the POS tag of the token\n",
    "* `up`: the index of the parent of the token in the dependency tree.\n",
    "* `dn`: the indices of the children of the token in the dependency tree.\n",
    "* `dep`: the dependency in the edge between the token and its parent.\n",
    "\n",
    "Note that `dn` is really just there for convenience; if we wanted to further save space we could remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want the entire dependency parse, `TextParser` can also be run in other modes:\n",
    "\n",
    "* `tag`, which POS tags the input\n",
    "* `tokenize`, which only tokenizes the input.\n",
    "\n",
    "In both cases, the nltk sentence tokenizer is used to tokenize sentences, and each sentence is then passed into a spacy object. A bit annoying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttagger = TextParser('tagged', 'tag', input_field='text')\n",
    "corpus = texttagger.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, we see that only the POS tags are available for each token. Across all modes, `TextParser` output is in a similar format, in case we later change our minds and decide to dependency-parse it after all. (note that this isn't implemented, and kind of relies on spacy tokenization being consistent across runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'toks': [{'tag': 'UH', 'tok': 'Yeah'},\n",
       "   {'tag': ',', 'tok': ','},\n",
       "   {'tag': 'CC', 'tok': 'but'},\n",
       "   {'tag': 'JJ', 'tok': 'many'},\n",
       "   {'tag': 'NNS', 'tok': 'friends'},\n",
       "   {'tag': 'VBD', 'tok': 'went'},\n",
       "   {'tag': 'IN', 'tok': 'with'},\n",
       "   {'tag': 'PRP', 'tok': 'me'},\n",
       "   {'tag': ',', 'tok': ','},\n",
       "   {'tag': 'JJ', 'tok': 'Japanese'},\n",
       "   {'tag': 'NN', 'tok': 'guy'},\n",
       "   {'tag': '.', 'tok': '.'}]},\n",
       " {'toks': [{'tag': 'RB', 'tok': 'So'},\n",
       "   {'tag': 'PRP', 'tok': 'I'},\n",
       "   {'tag': 'VBD', 'tok': 'was'},\n",
       "   {'tag': 'RB', 'tok': \"n't\"},\n",
       "   {'tag': 'PRP', 'tok': 'I'},\n",
       "   {'tag': 'VBD', 'tok': 'was'},\n",
       "   {'tag': 'RB', 'tok': \"n't\"},\n",
       "   {'tag': 'UH', 'tok': 'like'},\n",
       "   {'tag': 'NN', 'tok': 'homesick'},\n",
       "   {'tag': '.', 'tok': '.'}]},\n",
       " {'toks': [{'tag': 'CC', 'tok': 'But'},\n",
       "   {'tag': 'RB', 'tok': 'now'},\n",
       "   {'tag': 'RB', 'tok': 'sometimes'},\n",
       "   {'tag': 'PRP', 'tok': 'I'},\n",
       "   {'tag': 'VBP', 'tok': 'get'},\n",
       "   {'tag': 'NN', 'tok': 'homesick'},\n",
       "   {'tag': '.', 'tok': '.'}]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_processed_text(test_utt_id, 'tagged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing I've found myself frequently doing is to store a subset of the information returned by the preprocessing as a string that contains only the tokens (but properly sentence-and-word tokenized). This allows me to quickly analyze the data when I don't actually need the full parse. I felt this merited yet another `TextProcessor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TokensToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_to_str = TokensToString('tok_str')\n",
    "corpus = tok_to_str.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TokensToString` will access the `parsed` field of `processed_text` and output text, where sentences are newline separated: (note that here, spacy gets a bit confused about sentence boundaries by virtue of the speaker's disfluency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah , but many friends went with me , Japanese guy .\n",
      "So I was n't\n",
      "I was n't like homesick .\n",
      "But now sometimes I get homesick .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.get_processed_text(test_utt_id, 'tok_str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can also format the outputted tokens in different ways, or only output a subset. Here, I'll ignore all punctuation and output the token and its tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_to_str = TokensToString('tok_tag', token_formatter=lambda x: '%s_%s' % (x['tok'].lower(), x['tag']),\n",
    "                           token_filter=lambda x: sum(ch.isalpha() for ch in x['tok'])>0)\n",
    "corpus = tag_to_str.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah_UH but_CC many_JJ friends_NNS went_VBD with_IN me_PRP japanese_JJ guy_NN\n",
      "so_RB i_PRP was_VBD n't_RB\n",
      "i_PRP was_VBD n't_RB like_UH homesick_NN\n",
      "but_CC now_RB sometimes_RB i_PRP get_VBP homesick_NN\n"
     ]
    }
   ],
   "source": [
    "print(corpus.get_processed_text(test_utt_id, 'tok_tag'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a transformation that is used in the QuestionTypology paper is to extract arcs from the dependency parse of the text (fancy bigrams, basically). Here's the `TextProcessor` object that does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextToArcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_to_arc = TextToArcs('arcs')\n",
    "corpus = text_to_arc.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this stores a list of sentences, where each sentence is a list of arcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['friends_*',\n",
       "  'friends_guy',\n",
       "  'friends_many',\n",
       "  'guy_*',\n",
       "  'guy_japanese',\n",
       "  'japanese_*',\n",
       "  'many_*',\n",
       "  'me_*',\n",
       "  'went_*',\n",
       "  'went_friends',\n",
       "  'went_with',\n",
       "  'went_yeah',\n",
       "  'with_*',\n",
       "  'with_me',\n",
       "  'yeah_*'],\n",
       " ['i_*', 'so>i', 'so_*', 'was_*', 'was_i', 'was_so'],\n",
       " ['homesick_*', 'i_*', 'like_*', 'was_*', 'was_homesick', 'was_i', 'was_like'],\n",
       " ['but>now',\n",
       "  'get_*',\n",
       "  'get_homesick',\n",
       "  'get_i',\n",
       "  'get_now',\n",
       "  'get_sometimes',\n",
       "  'homesick_*',\n",
       "  'i_*',\n",
       "  'now_*',\n",
       "  'sometimes_*']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_processed_text(test_utt_id, 'arcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextToArcs` comes with other options: we may only take arcs that come out of the root (`root_only`) or omit nouns (`censor_nouns`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_to_arc_mini = TextToArcs('arcs_mini', censor_nouns=True, root_only=True)\n",
    "corpus = text_to_arc_mini.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['went_*', 'went_with', 'went_yeah'],\n",
       " ['was_*', 'was_so'],\n",
       " ['was_*', 'was_like'],\n",
       " ['but>now', 'get_*', 'get_now', 'get_sometimes']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_processed_text(test_utt_id, 'arcs_mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since storing lists of lists might be clunky, we might instead want to serialize these things as strings -- where, as above, sentences are separated by newlines and arcs are separated by spaces. Also, I will use this opportunity to demonstrate a more involved `TextProcessor`. \n",
    "\n",
    "this function will serialize lists of lists of strings, and optionally takes in arguments for how to delimit sentences and tokens (i.e., elements of the list). note that these arguments are passed in by way of the argument `aux_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_tokens_and_sentences(sents, aux_input={'sent_sep': '\\n', 'tok_sep': ' '}):\n",
    "    return aux_input.get('sent_sep','\\n').join(aux_input.get('tok_sep',' ')\n",
    "                         .join(sent) for sent in sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for fun, suppose we wanted to separate arcs by commas instead of spaces. This is what the corresponding `TextProcessor` call, and output, look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arc_to_string = TextProcessor(proc_fn=join_tokens_and_sentences, \n",
    "                              output_field='arc_string', input_field='arcs',\n",
    "                             aux_input={'sent_sep': '\\n', 'tok_sep': ', '})\n",
    "corpus = arc_to_string.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friends_*, friends_guy, friends_many, guy_*, guy_japanese, japanese_*, many_*, me_*, went_*, went_friends, went_with, went_yeah, with_*, with_me, yeah_*\n",
      "i_*, so>i, so_*, was_*, was_i, was_so\n",
      "homesick_*, i_*, like_*, was_*, was_homesick, was_i, was_like\n",
      "but>now, get_*, get_homesick, get_i, get_now, get_sometimes, homesick_*, i_*, now_*, sometimes_*\n"
     ]
    }
   ],
   "source": [
    "print(corpus.get_processed_text(test_utt_id, 'arc_string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've now accumulated a bunch of different preprocessed versions of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'parsed', 'tagged', 'tok_str', 'tok_tag', 'arcs', 'arcs_mini', 'arc_string'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.processed_text.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can save all of them to disk as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.dump_processed_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this stores each field of processed_text as a separate jsonlist. (note that I should correct the file ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversations.json              processed_text.tagged.json\r\n",
      "corpus.json                     processed_text.text.json\r\n",
      "index.json                      processed_text.tok_str.json\r\n",
      "processed_text.arcs.json        processed_text.tok_tag.json\r\n",
      "processed_text.arcs_mini.json   users.json\r\n",
      "processed_text.arc_string.json  utterances.jsonl\r\n",
      "processed_text.parsed.json\r\n"
     ]
    }
   ],
   "source": [
    "ls '/kitchen/convokit_corpora/tennis-mini/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we'd only wanted to dump a subset, we could specify a list of fields to dump:\n",
    "\n",
    ">> corpus.dump_processed_text(['arcs', 'tok_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, here's a demo of how this expanded corpus might be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = convokit.Corpus('/kitchen/convokit_corpora/tennis-mini/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by default, _none_ of the processed fields are loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can specify a subset of fields to load (if no argument is provided, the expected behaviour is that it will load all the fields it can find in the directory -- note that this isn't yet implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus.load_processed_text(['arcs', 'tok_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arcs', 'tok_str'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.processed_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['friends_*',\n",
       "  'friends_guy',\n",
       "  'friends_many',\n",
       "  'guy_*',\n",
       "  'guy_japanese',\n",
       "  'japanese_*',\n",
       "  'many_*',\n",
       "  'me_*',\n",
       "  'went_*',\n",
       "  'went_friends',\n",
       "  'went_with',\n",
       "  'went_yeah',\n",
       "  'with_*',\n",
       "  'with_me',\n",
       "  'yeah_*'],\n",
       " ['i_*', 'so>i', 'so_*', 'was_*', 'was_i', 'was_so'],\n",
       " ['homesick_*', 'i_*', 'like_*', 'was_*', 'was_homesick', 'was_i', 'was_like'],\n",
       " ['but>now',\n",
       "  'get_*',\n",
       "  'get_homesick',\n",
       "  'get_i',\n",
       "  'get_now',\n",
       "  'get_sometimes',\n",
       "  'homesick_*',\n",
       "  'i_*',\n",
       "  'now_*',\n",
       "  'sometimes_*']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.get_processed_text(test_utt_id, 'arcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah , but many friends went with me , Japanese guy .\n",
      "So I was n't\n",
      "I was n't like homesick .\n",
      "But now sometimes I get homesick .\n"
     ]
    }
   ],
   "source": [
    "print(new_corpus.get_processed_text(test_utt_id, 'tok_str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
