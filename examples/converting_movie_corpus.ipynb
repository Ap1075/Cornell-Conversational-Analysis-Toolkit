{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Cornell Movie-Dialogs Corpus into ConvoKit format \n",
    "\n",
    "This notebook is a demonstration of how custom datasets can be converted into Corpus with ConvoKit. \n",
    "\n",
    "The original version of the Cornell Movie-Dialogs Corpus can be downloaded from:  https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html. It contains the following files:\n",
    "\n",
    "* __movie_characters_metadata.txt__ contains information about each movie character\n",
    "* __movie_lines.txt contains__ the actual text of each utterance\n",
    "* __movie_conversations.txt__ contains the structure of the conversations\n",
    "* __movie_titles_metadata.txt__ contains information about each movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "/home/jonathan/.conda/envs/ck_2_3_testing/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Corpus from a list of Utterances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus can be constructed from a list of utterances with:\n",
    "\n",
    "    corpus = Corpus(utterances= custom_utterance_list)\n",
    "    \n",
    "Our goal is to convert the original dataset into this \"custom_utterance_list\", and let ConvoKit will do the rest of the conversion for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creating users\n",
    "\n",
    "Each character in a movie is considered a user, and there are 9,035 characters in total in this dataset. We will read off metadata for each user from __movie_characters_metadata.txt__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the directory with where your downloaded cornell movie dialogs corpus is saved\n",
    "data_dir = \"cornell-movie-dialogs-corpus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"movie_characters_metadata.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    user_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we would directly use the name of the user as the name. However, in our case, since only the first name of the movie character is given, these names may not uniquely map to a character. We will instead use user_id provided in the original dataset as username, whereas the actual charatcter name will be saved in user metadata.\n",
    "\n",
    "For this dataset, we include the following information for each user:  \n",
    "* name of the character.\n",
    "* idx and name of the movie this charater is from\n",
    "* gender(available for 3,774 characters)\n",
    "* position on movie credits (3,321 characters available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_meta = {}\n",
    "for user in user_data:\n",
    "    user_info = [info.strip() for info in user.split(\"+++$+++\")]\n",
    "    user_meta[user_info[0]] = {\"character_name\": user_info[1],\n",
    "                               \"movie_idx\": user_info[2],\n",
    "                               \"movie_name\": user_info[3],\n",
    "                               \"gender\": user_info[4],\n",
    "                               \"credit_pos\": user_info[5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, an user object can be initiated with `User(name = <user_name>, meta = <user_metadata>)`. The following example shows how we create an User object for each unique character in the dataset, which will be used to create Utterances objects later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_users = {k: User(name = k, meta = v) for k,v in user_meta.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checking use-level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users in the data = 9035\n"
     ]
    }
   ],
   "source": [
    "print(\"number of users in the data = {}\".format(len(corpus_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character_name': 'BIANCA',\n",
       " 'movie_idx': 'm0',\n",
       " 'movie_name': '10 things i hate about you',\n",
       " 'gender': 'f',\n",
       " 'credit_pos': '4'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_users['u0'].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Creating utterance objects\n",
    "Utterances can be found in __movie_lines.txt__. There are 304,713 utterances in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"movie_lines.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    utterance_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate an utterance object, we generally need the following information (all ids should be of type string):\n",
    "- id: representing the unique id of the utterance. \n",
    "- user: a ConvoKit User object representing the user giving the utterance.\n",
    "- root: the id of the root utterance of the conversation.\n",
    "- reply_to: id of the utterance this was a reply to.\n",
    "- timestamp: timestamp of the utterance. \n",
    "- text: text of the utterance.\n",
    "\n",
    "Additional information associated with the utterance may be saved as utterance level metadata. In this case, we consider the movie_id from which this utterance is extracted as an example for metadata. \n",
    "\n",
    "An utterance possessing all the above information may be initiated by `Utterance(id=..., user =..., root =..., rely_to=..., timestamp=..., text =..., meta =...)`\n",
    "\n",
    "We now create such Utterance objects for the utterances in our dataset. Note that normally we would provide `root` and `reply_to` information at the time of instantiation, but we will defer it to later as such information need to be retrieved from a different file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304713/304713 [00:03<00:00, 90094.76it/s] \n"
     ]
    }
   ],
   "source": [
    "utterance_corpus = {}\n",
    "\n",
    "count = 0\n",
    "for utterance in tqdm(utterance_data):\n",
    "    \n",
    "    utterance_info = [info.strip() for info in utterance.split(\"+++$+++\")]\n",
    "    if len(utterance_info) < 4:\n",
    "        print(utterance_info)\n",
    "    \n",
    "    # ignoring character name since User object already has information\n",
    "    try:\n",
    "        idx, user, movie_id, text = utterance_info[0], utterance_info[1], utterance_info[2], utterance_info[4]\n",
    "    except:\n",
    "        print(utterance_info)\n",
    "    \n",
    "        meta = {'movie_id': movie_id}\n",
    "    \n",
    "    # root & reply_to will be updated later, timestamp is not applicable \n",
    "    utterance_corpus[idx] = Utterance(id=idx, user=corpus_users[user], text=text, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304713"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(utterance_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checking on the status of the utterance objects, they should now contain an id, the users who said them, the actual texts, as well as the movie ids as the metadata: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', '_owner': None, 'meta': {'movie_id': 'm0'}, '_id': 'L1044', 'user': User({'obj_type': 'user', '_owner': None, 'meta': {'character_name': 'CAMERON', 'movie_idx': 'm0', 'movie_name': '10 things i hate about you', 'gender': 'm', 'credit_pos': '3'}, '_id': 'u2', '_name': 'u2'}), 'root': None, 'reply_to': None, 'timestamp': None, 'text': 'They do to!'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_corpus['L1044'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating root and reply_to information to utterances\n",
    "__movie_conversations.txt__ provides the structure of conversations that organizes the above utterances. This will allow us to add the missing root and reply_to information to individual utterances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"movie_conversations.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    convo_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:02<00:00, 33615.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for info in tqdm(convo_data):\n",
    "        \n",
    "    user1, user2, m, convo = [info.strip() for info in info.split(\"+++$+++\")]\n",
    "\n",
    "    convo_seq = ast.literal_eval(convo)\n",
    "    \n",
    "    # update utterance\n",
    "    root = convo_seq[0]\n",
    "    \n",
    "    # convo_seq is a list of utterances ids, arranged in conversational order\n",
    "    for i, line in enumerate(convo_seq):\n",
    "        \n",
    "        # sanity checking: user giving the utterance is indeed in the pair of characters provided\n",
    "        if utterance_corpus[line].user.name not in [user1, user2]:\n",
    "            print(\"user mismatch in line {0}\".format(i))\n",
    "        \n",
    "        utterance_corpus[line].root = root\n",
    "        \n",
    "        if i == 0:\n",
    "            utterance_corpus[line].reply_to = None\n",
    "        else:\n",
    "            utterance_corpus[line].reply_to = convo_seq[i-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checking on the status of utterances. After updating root and reply_to information, they should now contain all mandatory fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', '_owner': None, 'meta': {'movie_id': 'm616'}, '_id': 'L666499', 'user': User({'obj_type': 'user', '_owner': None, 'meta': {'character_name': 'COGHILL', 'movie_idx': 'm616', 'movie_name': 'zulu dawn', 'gender': '?', 'credit_pos': '?'}, '_id': 'u9028', '_name': 'u9028'}), 'root': 'L666497', 'reply_to': 'L666498', 'timestamp': None, 'text': 'How quickly can you move your artillery forward?'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_corpus['L666499']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating corpus from list of utterances\n",
    "We are now ready to create the movie-corpus. Note that we can specify a version number for a corpus, making it easier for us to keep track of which corpus we are working with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_list = [utterance for k,utterance in utterance_corpus.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in actual use, create the appropriate version number\n",
    "movie_corpus = Corpus(utterances=utterance_list, version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvoKit will automatically help us create conversations based on the information about the utterances we provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset = 83097\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset = {}\".format(len(movie_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample conversation 0:\n",
      "['L1045', 'L1044']\n",
      "sample conversation 1:\n",
      "['L985', 'L984']\n",
      "sample conversation 2:\n",
      "['L925', 'L924']\n",
      "sample conversation 3:\n",
      "['L872', 'L871', 'L870']\n",
      "sample conversation 4:\n",
      "['L869', 'L868', 'L867', 'L866']\n"
     ]
    }
   ],
   "source": [
    "convo_ids = movie_corpus.get_conversation_ids()\n",
    "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
    "    print(\"sample conversation {}:\".format(i))\n",
    "    print(movie_corpus.get_conversation(convo_idx).get_utterance_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Updating Corpus level metadata:\n",
    "In this dataset, there are a few sets of additional information about a total of 617 movies from which these conversations are drawn. For instance, genres, release year, url from which the raw sources are retrieved are included in the original dataset. These may be saved as Corpus level metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding urls information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"raw_script_urls.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    urls = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_meta = {}\n",
    "for movie in urls:\n",
    "    movie_id, title, url = [info.strip() for info in movie.split(\"+++$+++\")]\n",
    "    movie_meta[movie_id] = {'title': title, \"url\": url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adding more movie meta from movie_titles_metadata.txt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + \"movie_titles_metadata.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    movie_extra = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movie_extra:\n",
    "    movie_id, title, year, rating, votes, genre  = [info.strip() for info in movie.split(\"+++$+++\")]\n",
    "    movie_meta[movie_id]['release_year'] = year\n",
    "    movie_meta[movie_id]['rating'] = rating\n",
    "    movie_meta[movie_id]['votes'] = votes\n",
    "    movie_meta[movie_id]['genre'] = genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checking for a random movie in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'the avengers',\n",
       " 'url': 'http://www.dailyscript.com/scripts/Avengers.html',\n",
       " 'release_year': '1998',\n",
       " 'rating': '3.40',\n",
       " 'votes': '21519',\n",
       " 'genre': \"['action', 'adventure', 'thriller']\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_meta['m23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_corpus.meta['movie_metadata'] = movie_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can also the original name of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_corpus.meta['name'] = \"Cornell Movie-Dialogs Corpus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Processing utterance texts \n",
    "\n",
    "We can also \"annotate\" the utterances, e.g., getting dependency parses for them, and save the resultant parses. Here is an example of how this can be done, more examples related to text processing can be found at https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/text-processing/text_preprocessing_demo.ipynb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = TextParser(verbosity=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/304713 utterances processed\n",
      "20000/304713 utterances processed\n",
      "30000/304713 utterances processed\n",
      "40000/304713 utterances processed\n",
      "50000/304713 utterances processed\n",
      "60000/304713 utterances processed\n",
      "70000/304713 utterances processed\n",
      "80000/304713 utterances processed\n",
      "90000/304713 utterances processed\n",
      "100000/304713 utterances processed\n",
      "110000/304713 utterances processed\n",
      "120000/304713 utterances processed\n",
      "130000/304713 utterances processed\n",
      "140000/304713 utterances processed\n",
      "150000/304713 utterances processed\n",
      "160000/304713 utterances processed\n",
      "170000/304713 utterances processed\n",
      "180000/304713 utterances processed\n",
      "190000/304713 utterances processed\n",
      "200000/304713 utterances processed\n",
      "210000/304713 utterances processed\n",
      "220000/304713 utterances processed\n",
      "230000/304713 utterances processed\n",
      "240000/304713 utterances processed\n",
      "250000/304713 utterances processed\n",
      "260000/304713 utterances processed\n",
      "270000/304713 utterances processed\n",
      "280000/304713 utterances processed\n",
      "290000/304713 utterances processed\n",
      "300000/304713 utterances processed\n",
      "304713/304713 utterances processed\n"
     ]
    }
   ],
   "source": [
    "movie_corpus = parser.transform(movie_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parses are saved under 'parsed' in utterance meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rt': 4,\n",
       "  'toks': [{'tok': 'How', 'tag': 'WRB', 'dep': 'advmod', 'up': 1, 'dn': []},\n",
       "   {'tok': 'quickly', 'tag': 'RB', 'dep': 'advmod', 'up': 4, 'dn': [0]},\n",
       "   {'tok': 'can', 'tag': 'MD', 'dep': 'aux', 'up': 4, 'dn': []},\n",
       "   {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 4, 'dn': []},\n",
       "   {'tok': 'move', 'tag': 'VB', 'dep': 'ROOT', 'dn': [1, 2, 3, 6, 7, 8]},\n",
       "   {'tok': 'your', 'tag': 'PRP$', 'dep': 'poss', 'up': 6, 'dn': []},\n",
       "   {'tok': 'artillery', 'tag': 'NN', 'dep': 'dobj', 'up': 4, 'dn': [5]},\n",
       "   {'tok': 'forward', 'tag': 'RB', 'dep': 'advmod', 'up': 4, 'dn': []},\n",
       "   {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 4, 'dn': []}]}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_corpus.get_utterance('L666499').get_info('parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving created datasets\n",
    "To complete the final step of dataset conversion, we want to save the dataset such that it can be loaded later for reuse. You may want to specify a name. The default location to find the saved datasets will be __./convokit/saved-copora__ in your home directory, but you can also specify where you want the saved corpora to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_corpus.dump(\"movie-corpus\", base_path = <specify where you prefer to save it to>)\n",
    "# the following would save the Corpus to the default location, i.e., ./convokit/saved-corpora\n",
    "movie_corpus.dump(\"movie-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving, the available info from dataset can be checked directly, without loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import meta_index\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterances-index': {'movie_id': \"<class 'str'>\", 'parsed': \"<class 'list'>\"},\n",
       " 'users-index': {'character_name': \"<class 'str'>\",\n",
       "  'movie_idx': \"<class 'str'>\",\n",
       "  'movie_name': \"<class 'str'>\",\n",
       "  'gender': \"<class 'str'>\",\n",
       "  'credit_pos': \"<class 'str'>\"},\n",
       " 'conversations-index': {},\n",
       " 'overall-index': {},\n",
       " 'version': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_index(filename = os.path.join(os.path.expanduser(\"~\"), \".convokit/saved-corpora/movie-corpus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways of conversion\n",
    "\n",
    "The above method is only one way to convert the dataset. Alternatively, one may follow strictly with the specifications of the expected data format described [here](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/doc/source/data_format.rst) and write out the component files directly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
