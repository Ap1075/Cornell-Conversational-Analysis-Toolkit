{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politeness prediction with ConvoKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a simple classifier to predict the politeness level of a request by considering the politeness strategies used, as seen in the paper [A computational approach to politeness with application to social factors](https://www.cs.cornell.edu/~cristian/Politeness.html), using ConvoKit. Note that this notebook is *not* intended to reproduce the paper results: legacy code for reproducibility is available at this [repository](https://github.com/sudhof/politeness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from typing import List, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: load annotated dataset\n",
    "\n",
    "We will be using the wikipedia annotations from the [Stanford Politeness Corpus](https://www.cs.cornell.edu/~cristian/Politeness.html). \n",
    "\n",
    "Code below demonstrates how to convert the original CSV file into the corpus format expected by ConvoKit, but this resultant corpus can also be directly downloaded using the helper function `download(\"wiki-politeness-annotated\")`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: annotate the corpus with politeness strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get politeness strategies for each utterance, we will first obtain dependency parses for the utterances, and then check for strategy use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Parser, PolitenessStrategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding dependency parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/wiki-politeness-annotated\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus = Corpus(convokit.download(\"wiki-politeness-annotated\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = Parser()\n",
    "wiki_corpus = annotator.fit_transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding strategy information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PolitenessStrategies()\n",
    "wiki_corpus = ps.transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how a processed utterance now look. Dependency parses are stored in `parsed`, and politeness strategies are in `politeness_strategies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': 629705, 'user': User([('name', 'wiki_user')]), 'root': 629705, 'reply_to': None, 'timestamp': 'NOT_RECORDED', 'text': \"Where did you learn English? How come you're taking on a third language?\", 'meta': {'Normalized Score': -1.1200492637766977, 'Binary': -1, 'Annotations': {'A2UFD1I8ZO1V4G': 13, 'A2YFPO0N4GIS25': 9, 'AYG3MF094634L': 11, 'A38WUWONC7EXTO': 11, 'A15DM9BMKZZJQ6': 5}, 'parsed': Where did you learn English? How come you're taking on a third language?, 'politeness_strategies': {'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 0, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 0, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 1, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 0, 'feature_politeness_==HASHEDGE==': 0, 'feature_politeness_==HASPOSITIVE==': 0, 'feature_politeness_==HASNEGATIVE==': 0}}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_corpus.get_utterance(629705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to save the corpus by doing `wiki_corpus.dump(\"wiki-politeness-annotated\")` for further exploration. Note that if you do not specify a base path, data will be saved to `.convokit/saved-corpora` in your home directory by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. predict politeness \n",
    "\n",
    "We will see how a simple classifier considering the use of politeness strategies perform. Note that this is only for demonstration, and not geared towards achieving best performance. \n",
    "\n",
    "(Most of the code below are adapted from [here](https://github.com/sudhof/politeness/blob/master/scripts/train_model.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this prediction task, we will only consider the polite vs. impolite group (i.e., those with \"Binary\" field being either +1 or -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_corpus = Corpus(utterances=[utt for utt in wiki_corpus.iter_utterances() if utt.meta[\"Binary\"] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from \"documents2feature_vectors\"\n",
    "def corpus2feature_vectors(corpus: Corpus, ids: List[int]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        corpus {Corpus} -- The corpus being converted. \n",
    "                        Requires pre-computed politeness_strategies in the utterance meta field. \n",
    "                        \n",
    "        ids {List[int]} -- ids being considered \n",
    "    \"\"\"\n",
    "    \n",
    "    fks = False\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for utt_id in ids:\n",
    "        \n",
    "        utt = corpus.get_utterance(utt_id)\n",
    "        fs = utt.meta[\"politeness_strategies\"]\n",
    "        if not fks:\n",
    "            fks = sorted(fs.keys())\n",
    "        fv = [fs[f] for f in fks]\n",
    "        \n",
    "        # the utterance is regarded as polite if its score is in the top 75% percentile (i.e., Binary = 1)\n",
    "        # and it is regarded as impolite is its score lies in the bottom 25% percentile (i.e., Binary = -1)\n",
    "        l = 1 if utt.meta[\"Binary\"] == 1 else 0\n",
    "        \n",
    "        X.append(fv)\n",
    "        y.append(l)\n",
    "        \n",
    "    X = csr_matrix(np.asarray(X))\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from \"train_svm\"\n",
    "def train_svm(corpus: Corpus, ntesting: int = 500, rseed:int = 123):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        corpus: annotated training corpus\n",
    "        ntesting:  number of docs to reserve for testing\n",
    "    \"\"\"\n",
    "\n",
    "    utt_ids = corpus.get_utterance_ids()\n",
    "    random.seed(rseed)\n",
    "    random.shuffle(utt_ids)\n",
    "\n",
    "    testing_ids = utt_ids[-ntesting:]\n",
    "    training_ids = utt_ids[:-ntesting]\n",
    "    \n",
    "    print(\"sample test_ids:\", testing_ids[-5:])\n",
    "\n",
    "    X, y = corpus2feature_vectors(corpus, training_ids)\n",
    "    Xtest, ytest = corpus2feature_vectors(corpus, testing_ids)\n",
    "\n",
    "    print(\"Fitting\")\n",
    "    clf = svm.SVC(C=0.02, kernel='linear', probability=True)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Test\n",
    "    y_pred = clf.predict(Xtest)\n",
    "    print(\"accuracy = {}\".format(np.mean(y_pred == ytest)))\n",
    "    print(classification_report(ytest, y_pred, labels = [1, 0], target_names=[\"polite\", \"impolite\"]))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = corpus2feature_vectors(binary_corpus, binary_corpus.get_utterance_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample test_ids: [485293, 252109, 623560, 328144, 627508]\n",
      "Fitting\n",
      "accuracy = 0.734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      polite       0.75      0.69      0.72       249\n",
      "    impolite       0.72      0.78      0.75       251\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.74      0.73      0.73       500\n",
      "weighted avg       0.74      0.73      0.73       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = train_svm(binary_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this classifier to predict politeness labels for Utterances. As an example, we will use some test utterances, but you can also consider use this classifier to predict on new utterances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [485293, 252109, 623560, 328144, 627508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unlabeled utterances, you will need to featurize slightly differently (e.g., by commenting out lines relevant to y), as y wouldn't be available\n",
    "Xtest, y = corpus2feature_vectors(binary_corpus, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predicting for test utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)\n",
    "yprob = clf.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to check predicted politeness label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test utterance:\n",
      "Blocked, templated.  Next?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.81650082 0.18349918]\n",
      "\n",
      "1\n",
      "test utterance:\n",
      "Stephan, what did you mean by ''\"Is English your native language? You seem to fill in a lot of things not said with your assumptions.\"'' on my talk?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.7367566 0.2632434]\n",
      "\n",
      "2\n",
      "test utterance:\n",
      "I see you created a nonsense article yesterday because you were bored. If I unblock you will you disrupt more?\n",
      "------------------------\n",
      "Result: polite, probability estimates = [0.37764129 0.62235871]\n",
      "\n",
      "3\n",
      "test utterance:\n",
      "I have no need to search the interwebs, all that matters is it offends people and is a violation of NPOV and MoS. \"All Wikipedia articles and other encyclopedic content must be written from a neutral point of view (NPOV), representing fairly and without bias all significant views (that have been published by reliable sources)\" - ess-eff is a bias term for a minority group of fans, put it this way: is there an SF-channel, or how often is the clichxe9 ess-eff used outside fan-groups?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.51761742 0.48238258]\n",
      "\n",
      "4\n",
      "test utterance:\n",
      "I responded to your talk page comment, encouraging you to try your hand at reworking it.  Good analysis of a potential problem, so why not take a whack at it?\n",
      "------------------------\n",
      "Result: polite, probability estimates = [0.32961796 0.67038204]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2label = {1: \"polite\", 0: \"impolite\"}\n",
    "\n",
    "for i, idx in enumerate(test_ids):\n",
    "    print(i)\n",
    "    test_utt = binary_corpus.get_utterance(idx)\n",
    "    print(\"test utterance:\\n{}\".format(test_utt.text))\n",
    "    print(\"------------------------\")\n",
    "    print(\"Result: {}, probability estimates = {}\\n\".format(pred2label[ypred[i]], yprob[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this is an implementation of a politeness classifier is trained on a specific dataset (wikipedia) and on a specific binarization of politeness classes. Depending on your scenario, you might find it is preferable to directly use the politeness strategies, as exemplified in the [conversations gone awry example](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb), rather than a politeness label/score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
