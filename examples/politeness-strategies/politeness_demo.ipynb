{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politeness prediction with ConvoKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a simple classifier to predict the politeness level of a request by considering the politeness strategies used, as seen in the paper [A computational approach to politeness with application to social factors](https://www.cs.cornell.edu/~cristian/Politeness.html), using ConvoKit. Note that this notebook is *not* intended to reproduce the paper results: legacy code for reproducibility is available at this [repository](https://github.com/sudhof/politeness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../github/Cornell-Conversational-Analysis-Toolkit/convokit/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liye/miniconda3/work/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"../github/Cornell-Conversational-Analysis-Toolkit/\")\n",
    "\n",
    "# import convokit\n",
    "# print(convokit.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from typing import List, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Loading (and converting) annotated dataset\n",
    "\n",
    "We will be using the wikipedia annotations from the [Stanford Politeness Corpus](https://www.cs.cornell.edu/~cristian/Politeness.html). \n",
    "\n",
    "Code below demonstrates how to convert the original CSV file into the corpus format expected by ConvoKit, but this resultant corpus can also be directly downloaded using the helper function `download(\"wiki-politeness-annotated\")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to modify the filepath depending on where your downloaded version is stored \n",
    "df = pd.read_csv(\"Stanford_politeness_corpus/wikipedia.annotated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Id</th>\n",
       "      <th>Request</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>Score3</th>\n",
       "      <th>Score4</th>\n",
       "      <th>Score5</th>\n",
       "      <th>TurkId1</th>\n",
       "      <th>TurkId2</th>\n",
       "      <th>TurkId3</th>\n",
       "      <th>TurkId4</th>\n",
       "      <th>TurkId5</th>\n",
       "      <th>Normalized Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>629705</td>\n",
       "      <td>Where did you learn English? How come you're t...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>A2UFD1I8ZO1V4G</td>\n",
       "      <td>A2YFPO0N4GIS25</td>\n",
       "      <td>AYG3MF094634L</td>\n",
       "      <td>A38WUWONC7EXTO</td>\n",
       "      <td>A15DM9BMKZZJQ6</td>\n",
       "      <td>-1.120049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>244336</td>\n",
       "      <td>Thanks very much for your edit to the &lt;url&gt; ar...</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>A2QN0EGBRGJU1M</td>\n",
       "      <td>A2GSW5RBAT5LQ5</td>\n",
       "      <td>AO5E3LWBYM72K</td>\n",
       "      <td>A2ULMYRKQMNNFG</td>\n",
       "      <td>A3TFQK7QK8X6LM</td>\n",
       "      <td>1.313955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Community      Id                                            Request  \\\n",
       "0  Wikipedia  629705  Where did you learn English? How come you're t...   \n",
       "1  Wikipedia  244336  Thanks very much for your edit to the <url> ar...   \n",
       "\n",
       "   Score1  Score2  Score3  Score4  Score5         TurkId1         TurkId2  \\\n",
       "0      13       9      11      11       5  A2UFD1I8ZO1V4G  A2YFPO0N4GIS25   \n",
       "1      23      16      24      21      25  A2QN0EGBRGJU1M  A2GSW5RBAT5LQ5   \n",
       "\n",
       "         TurkId3         TurkId4         TurkId5  Normalized Score  \n",
       "0  AYG3MF094634L  A38WUWONC7EXTO  A15DM9BMKZZJQ6         -1.120049  \n",
       "1  AO5E3LWBYM72K  A2ULMYRKQMNNFG  A3TFQK7QK8X6LM          1.313955  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to convert it to the format ConvoKit expects. Here is a simple helper function that does the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_corpus(df: DataFrame, id_col: str, text_col: str, meta_cols: List[str]) -> Corpus:\n",
    "    \n",
    "    \"\"\" Helper function to convert data to Corpus format\n",
    "     \n",
    "    Arguments:\n",
    "        df {DataFrame} -- Actual data, in a pandas Dataframe\n",
    "        id_col {str} -- name of the column that corresponds to utterances ids \n",
    "        text_col {str} -- name of the column that stores texts of the utterances  \n",
    "        meta_cols {List[str]} -- set of columns that stores relevant metadata \n",
    "    \n",
    "    Returns:\n",
    "        Corpus -- the converted corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # in this particular case, user, reply_to, and timestamp information are all not applicable \n",
    "    # and we will simply either create a placeholder entry, or leave it as None \n",
    "        \n",
    "    user = User(\"wiki_user\")\n",
    "    time = \"NOT_RECORDED\"\n",
    "\n",
    "    utterance_list = []    \n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        \n",
    "        # extracting meta data\n",
    "        metadata = {}\n",
    "        for meta_col in meta_cols:\n",
    "            metadata[meta_col] = row[meta_col]\n",
    "        \n",
    "        utterance_list.append(Utterance(str(row[id_col]), user, row[id_col], None, time, \\\n",
    "                                        row[text_col], meta=metadata))\n",
    "    \n",
    "    return Corpus(utterances = utterance_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For meta data, we will include the normalized score, its corresponding binary label (based on a 75% vs. 25% percentile cutoff -- technically there are three classes, but we will only look at the two ends, thus \"binary\"), as well as all original annotations with turker information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding detailed annotations information to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4353/4353 [00:10<00:00, 421.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# for simplicity, we will condense the turker information together\n",
    "df[\"Annotations\"] = [dict(zip([df.iloc[i][\"TurkId{}\".format(j)] for j in range(1,6)], \\\n",
    "                             [df.iloc[i][\"Score{}\".format(j)] for j in range(1,6)])) for i in tqdm(range(len(df)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- obtaining polite vs. impolite label (note that we are only interested in labels that are either +1 or -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the binary label based on Normalized score\n",
    "top = np.percentile(df['Normalized Score'], 75)\n",
    "bottom = np.percentile(df[\"Normalized Score\"], 25)\n",
    "df['Binary'] = [int(score >= top) - int(score <= bottom) for score in df['Normalized Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting dataframe to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4353it [00:00, 4521.61it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus = convert_df_to_corpus(df, \"Id\", \"Request\", [\"Normalized Score\", \"Binary\", \"Annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you were to download the data directly, here is how: \n",
    "# from convokit import download\n",
    "# wiki_corpus = Corpus(download(\"wiki-politeness-annotated\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Annotate the corpus with politeness strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get politeness strategies for each utterance, we will first obtain dependency parses for the utterances, and then check for strategy use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding dependency parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import TextParser\n",
    "parser = TextParser(verbosity=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050/4353 utterances processed\n",
      "100/4353 utterances processed\n",
      "150/4353 utterances processed\n",
      "200/4353 utterances processed\n",
      "250/4353 utterances processed\n",
      "300/4353 utterances processed\n",
      "350/4353 utterances processed\n",
      "400/4353 utterances processed\n",
      "450/4353 utterances processed\n",
      "500/4353 utterances processed\n",
      "550/4353 utterances processed\n",
      "600/4353 utterances processed\n",
      "650/4353 utterances processed\n",
      "700/4353 utterances processed\n",
      "750/4353 utterances processed\n",
      "800/4353 utterances processed\n",
      "850/4353 utterances processed\n",
      "900/4353 utterances processed\n",
      "950/4353 utterances processed\n",
      "1000/4353 utterances processed\n",
      "1050/4353 utterances processed\n",
      "1100/4353 utterances processed\n",
      "1150/4353 utterances processed\n",
      "1200/4353 utterances processed\n",
      "1250/4353 utterances processed\n",
      "1300/4353 utterances processed\n",
      "1350/4353 utterances processed\n",
      "1400/4353 utterances processed\n",
      "1450/4353 utterances processed\n",
      "1500/4353 utterances processed\n",
      "1550/4353 utterances processed\n",
      "1600/4353 utterances processed\n",
      "1650/4353 utterances processed\n",
      "1700/4353 utterances processed\n",
      "1750/4353 utterances processed\n",
      "1800/4353 utterances processed\n",
      "1850/4353 utterances processed\n",
      "1900/4353 utterances processed\n",
      "1950/4353 utterances processed\n",
      "2000/4353 utterances processed\n",
      "2050/4353 utterances processed\n",
      "2100/4353 utterances processed\n",
      "2150/4353 utterances processed\n",
      "2200/4353 utterances processed\n",
      "2250/4353 utterances processed\n",
      "2300/4353 utterances processed\n",
      "2350/4353 utterances processed\n",
      "2400/4353 utterances processed\n",
      "2450/4353 utterances processed\n",
      "2500/4353 utterances processed\n",
      "2550/4353 utterances processed\n",
      "2600/4353 utterances processed\n",
      "2650/4353 utterances processed\n",
      "2700/4353 utterances processed\n",
      "2750/4353 utterances processed\n",
      "2800/4353 utterances processed\n",
      "2850/4353 utterances processed\n",
      "2900/4353 utterances processed\n",
      "2950/4353 utterances processed\n",
      "3000/4353 utterances processed\n",
      "3050/4353 utterances processed\n",
      "3100/4353 utterances processed\n",
      "3150/4353 utterances processed\n",
      "3200/4353 utterances processed\n",
      "3250/4353 utterances processed\n",
      "3300/4353 utterances processed\n",
      "3350/4353 utterances processed\n",
      "3400/4353 utterances processed\n",
      "3450/4353 utterances processed\n",
      "3500/4353 utterances processed\n",
      "3550/4353 utterances processed\n",
      "3600/4353 utterances processed\n",
      "3650/4353 utterances processed\n",
      "3700/4353 utterances processed\n",
      "3750/4353 utterances processed\n",
      "3800/4353 utterances processed\n",
      "3850/4353 utterances processed\n",
      "3900/4353 utterances processed\n",
      "3950/4353 utterances processed\n",
      "4000/4353 utterances processed\n",
      "4050/4353 utterances processed\n",
      "4100/4353 utterances processed\n",
      "4150/4353 utterances processed\n",
      "4200/4353 utterances processed\n",
      "4250/4353 utterances processed\n",
      "4300/4353 utterances processed\n",
      "4350/4353 utterances processed\n"
     ]
    }
   ],
   "source": [
    "corpus = parser.transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding strategy information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import PolitenessStrategies\n",
    "ps = PolitenessStrategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus = ps.transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how a processed utterance now look. Dependency parses are stored in `parsed`, and politeness strategies are in `politeness_strategies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': '629705', 'user': User([('name', 'wiki_user')]), 'root': 629705, 'reply_to': None, 'timestamp': 'NOT_RECORDED', 'text': \"Where did you learn English? How come you're taking on a third language?\", 'meta': {'Normalized Score': -1.1200492637766977, 'Binary': -1, 'Annotations': {'A2UFD1I8ZO1V4G': 13, 'A2YFPO0N4GIS25': 9, 'AYG3MF094634L': 11, 'A38WUWONC7EXTO': 11, 'A15DM9BMKZZJQ6': 5}, 'parsed': [{'rt': 3, 'toks': [{'tok': 'Where', 'tag': 'WRB', 'dep': 'advmod', 'up': 3, 'dn': []}, {'tok': 'did', 'tag': 'VBD', 'dep': 'aux', 'up': 3, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 3, 'dn': []}, {'tok': 'learn', 'tag': 'VB', 'dep': 'ROOT', 'dn': [0, 1, 2, 4, 5]}, {'tok': 'English', 'tag': 'NNP', 'dep': 'dobj', 'up': 3, 'dn': []}, {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 3, 'dn': []}]}, {'rt': 4, 'toks': [{'tok': 'How', 'tag': 'WRB', 'dep': 'advmod', 'up': 4, 'dn': []}, {'tok': 'come', 'tag': 'VB', 'dep': 'aux', 'up': 4, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 4, 'dn': []}, {'tok': \"'re\", 'tag': 'VBP', 'dep': 'aux', 'up': 4, 'dn': []}, {'tok': 'taking', 'tag': 'VBG', 'dep': 'ROOT', 'dn': [0, 1, 2, 3, 5, 8, 9]}, {'tok': 'on', 'tag': 'RP', 'dep': 'prt', 'up': 4, 'dn': []}, {'tok': 'a', 'tag': 'DT', 'dep': 'det', 'up': 8, 'dn': []}, {'tok': 'third', 'tag': 'JJ', 'dep': 'amod', 'up': 8, 'dn': []}, {'tok': 'language', 'tag': 'NN', 'dep': 'dobj', 'up': 4, 'dn': [6, 7]}, {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 4, 'dn': []}]}], 'politeness_strategies': {'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 0, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 0, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 1, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 0, 'feature_politeness_==HASHEDGE==': 0, 'feature_politeness_==HASPOSITIVE==': 0, 'feature_politeness_==HASNEGATIVE==': 0}}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_corpus.get_utterance('629705')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to save the corpus by doing `wiki_corpus.dump(\"wiki-politeness-annotated\")` for further exploration. Note that if you do not specify a base path, data will be saved to `.convokit/saved-corpora` in your home directory by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] To get a glimpse of the overall use of politeness strategies in this corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.summarize(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Predict Politeness \n",
    "\n",
    "We will see how a simple classifier considering the use of politeness strategies perform, using `Classifier` (note that this is only for demonstration, and not geared towards achieving best performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a prestep, we subset the corpus as we will only consider the polite vs. impolite class for prediction (i.e., those with \"Binary\" field being either +1 or -1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_corpus = Corpus(utterances=[utt for utt in wiki_corpus.iter_utterances() if utt.meta[\"Binary\"] != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Direct evaluation\n",
    "\n",
    "If you are interested in how effectiveness are these politeness strategies, `Classifier` provides evaluation with both train/test splits, as well as with cross validaton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cross validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus objects...\n",
      "Running a cross-validated evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../github/Cornell-Conversational-Analysis-Toolkit/convokit/classifier/util.py:58: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  X_y_df = pd.concat([X_df, y_df], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.7706422 , 0.74770642, 0.73394495, 0.75172414, 0.73793103])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cv = Classifier(obj_type=\"utterance\", \n",
    "                        pred_feats=[\"politeness_strategies\"], \n",
    "                        labeller=lambda utt: utt.meta['Binary'] == 1)\n",
    "\n",
    "clf_cv.evaluate_with_cv(binary_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus objects...\n",
      "Running a train-test-split evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../github/Cornell-Conversational-Analysis-Toolkit/convokit/classifier/util.py:58: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  X_y_df = pd.concat([X_df, y_df], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7270642201834863, array([[169,  39],\n",
       "        [ 80, 148]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_split = Classifier(obj_type=\"utterance\", \n",
    "                        pred_feats=[\"politeness_strategies\"], \n",
    "                        labeller=lambda utt: utt.meta['Binary'] == 1)\n",
    "\n",
    "clf_split.evaluate_with_train_test_split(binary_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Training a classifier to predict on other utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 2078, test size = 100\n"
     ]
    }
   ],
   "source": [
    "test_ids = binary_corpus.get_utterance_ids()[-100:]\n",
    "train_corpus = Corpus(utterances=[utt for utt in binary_corpus.iter_utterances() if utt.id not in test_ids])\n",
    "test_corpus = Corpus(utterances=[utt for utt in binary_corpus.iter_utterances() if utt.id in test_ids])\n",
    "print(\"train size = {}, test size = {}\".format(len(train_corpus.get_utterance_ids()),\n",
    "                                               len(test_corpus.get_utterance_ids())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train a classifier with a corpus to predict politeness labels for other Utterances. As an example, we will first train with a training corpus, and check predictions on some test utterances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../github/Cornell-Conversational-Analysis-Toolkit/convokit/classifier/util.py:58: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  X_y_df = pd.concat([X_df, y_df], axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<convokit.classifier.classifier.Classifier at 0x7fe476eb4b50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Classifier(obj_type=\"utterance\", \n",
    "                        pred_feats=[\"politeness_strategies\"], \n",
    "                        labeller=lambda utt: utt.meta['Binary'] == 1)\n",
    "clf.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predicing on the test corpus (you can also predict on a list of utterances by using `clf.transform_objs()` instead) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>486441</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626897</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626894</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626728</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620909</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60798</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156734</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147665</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234095</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.814905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        prediction     score\n",
       "id                          \n",
       "486441           1  0.729354\n",
       "626897           0  0.112681\n",
       "626894           0  0.244715\n",
       "626728           0  0.068628\n",
       "620909           1  0.826003\n",
       "...            ...       ...\n",
       "60798            0  0.113524\n",
       "156734           0  0.138481\n",
       "147665           0  0.469575\n",
       "234095           1  0.654587\n",
       "563032           1  0.814905\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.summarize(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at a few example predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test utterance:\n",
      "I understood just fine, but wasn't at my computer. Are you in a hurry?\n",
      "------------------------\n",
      "Result: polite, probability estimates = 0.7293542964575708\n",
      "\n",
      "1\n",
      "test utterance:\n",
      "I've always been intrigued by 'dark-complected man.' What's with the radio, and fist in the air?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = 0.112680884866657\n",
      "\n",
      "2\n",
      "test utterance:\n",
      "Your early edit's clearly indicate that you were not a newbie. How do explain this?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = 0.24471475719553584\n",
      "\n",
      "3\n",
      "test utterance:\n",
      "Instead of another 3O, why don't you put in a <url>. And no, it's not a threat - it's an observation - why don't you <url>?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = 0.0686276937064236\n",
      "\n",
      "4\n",
      "test utterance:\n",
      "Great Article RaveenS, Do u want me to add this to the template (Sri Lankan Conflict)? I think it should be included in the ''see also'' section what do you suggest?\n",
      "------------------------\n",
      "Result: polite, probability estimates = 0.8260027677817088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2label = {1: \"polite\", 0: \"impolite\"}\n",
    "\n",
    "for i, idx in enumerate(test_ids[0:5]):\n",
    "    print(i)\n",
    "    test_utt = test_corpus.get_utterance(idx)\n",
    "    ypred, yprob = test_utt.meta['prediction'], test_utt.meta['score']\n",
    "    print(\"test utterance:\\n{}\".format(test_utt.text))\n",
    "    print(\"------------------------\")\n",
    "    print(\"Result: {}, probability estimates = {}\\n\".format(pred2label[ypred], yprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check out the confusion matrix and classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 12],\n",
       "       [17, 37]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.confusion_matrix(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.74      0.70        46\n",
      "        True       0.76      0.69      0.72        54\n",
      "\n",
      "    accuracy                           0.71       100\n",
      "   macro avg       0.71      0.71      0.71       100\n",
      "weighted avg       0.71      0.71      0.71       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(clf.classification_report(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this is an implementation of a politeness classifier trained on a specific dataset (wikipedia) and on a specific binarization of politeness classes. Depending on your scenario, you might find it preferable to directly use the politeness strategies, as exemplified in the [conversations gone awry example](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb), rather than a politeness label/score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
