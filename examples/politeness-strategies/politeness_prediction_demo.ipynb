{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politeness prediction with ConvoKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a simple classifier to predict the politeness level of a request by considering the politeness strategies used, as seen in the paper [A computational approach to politeness with application to social factors](https://www.cs.cornell.edu/~cristian/Politeness.html), using ConvoKit. Note that this notebook is *not* intended to reproduce the paper results: legacy code for reproducibility is available at this [repository](https://github.com/sudhof/politeness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Cornell-Conversational-Analysis-Toolkit/convokit/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../Cornell-Conversational-Analysis-Toolkit/\")\n",
    "import convokit\n",
    "from convokit import Corpus, User, Utterance\n",
    "\n",
    "print(convokit.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from typing import List, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: load annotated dataset\n",
    "\n",
    "We will be using the wikipedia annotations from the [Stanford Politeness Corpus](https://www.cs.cornell.edu/~cristian/Politeness.html). \n",
    "\n",
    "Code below demonstrates how to convert the original CSV file into the corpus format expected by ConvoKit, but this resultant corpus can also be directly downloaded using the helper function `download(\"wiki-politeness-annotated\")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to modify the filepath depending on where your downloaded version is stored \n",
    "df = pd.read_csv(\"/kitchen/experimental_liye/perception/Stanford_politeness_corpus/wikipedia.annotated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Id</th>\n",
       "      <th>Request</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>Score3</th>\n",
       "      <th>Score4</th>\n",
       "      <th>Score5</th>\n",
       "      <th>TurkId1</th>\n",
       "      <th>TurkId2</th>\n",
       "      <th>TurkId3</th>\n",
       "      <th>TurkId4</th>\n",
       "      <th>TurkId5</th>\n",
       "      <th>Normalized Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>629705</td>\n",
       "      <td>Where did you learn English? How come you're t...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>A2UFD1I8ZO1V4G</td>\n",
       "      <td>A2YFPO0N4GIS25</td>\n",
       "      <td>AYG3MF094634L</td>\n",
       "      <td>A38WUWONC7EXTO</td>\n",
       "      <td>A15DM9BMKZZJQ6</td>\n",
       "      <td>-1.120049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>244336</td>\n",
       "      <td>Thanks very much for your edit to the &lt;url&gt; ar...</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>A2QN0EGBRGJU1M</td>\n",
       "      <td>A2GSW5RBAT5LQ5</td>\n",
       "      <td>AO5E3LWBYM72K</td>\n",
       "      <td>A2ULMYRKQMNNFG</td>\n",
       "      <td>A3TFQK7QK8X6LM</td>\n",
       "      <td>1.313955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Community      Id                                            Request  \\\n",
       "0  Wikipedia  629705  Where did you learn English? How come you're t...   \n",
       "1  Wikipedia  244336  Thanks very much for your edit to the <url> ar...   \n",
       "\n",
       "   Score1  Score2  Score3  Score4  Score5         TurkId1         TurkId2  \\\n",
       "0      13       9      11      11       5  A2UFD1I8ZO1V4G  A2YFPO0N4GIS25   \n",
       "1      23      16      24      21      25  A2QN0EGBRGJU1M  A2GSW5RBAT5LQ5   \n",
       "\n",
       "         TurkId3         TurkId4         TurkId5  Normalized Score  \n",
       "0  AYG3MF094634L  A38WUWONC7EXTO  A15DM9BMKZZJQ6         -1.120049  \n",
       "1  AO5E3LWBYM72K  A2ULMYRKQMNNFG  A3TFQK7QK8X6LM          1.313955  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to convert it to the format ConvoKit expects. Here is a simple helper function that does the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_corpus(df: DataFrame, id_col: str, text_col: str, meta_cols: List[str]) -> Corpus:\n",
    "    \n",
    "    \"\"\" Helper function to convert data to Corpus format\n",
    "     \n",
    "    Arguments:\n",
    "        df {DataFrame} -- Actual data, in a pandas Dataframe\n",
    "        id_col {str} -- name of the column that corresponds to utterances ids \n",
    "        text_col {str} -- name of the column that stores texts of the utterances  \n",
    "        meta_cols {List[str]} -- set of columns that stores relevant metadata \n",
    "    \n",
    "    Returns:\n",
    "        Corpus -- the converted corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # in this particular case, user, reply_to, and timestamp information are all not applicable \n",
    "    # and we will simply either create a placeholder entry, or leave it as None \n",
    "        \n",
    "    user = User(\"wiki_user\")\n",
    "    time = \"NOT_RECORDED\"\n",
    "\n",
    "    utterance_list = []    \n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        \n",
    "        # extracting meta data\n",
    "        metadata = {}\n",
    "        for meta_col in meta_cols:\n",
    "            metadata[meta_col] = row[meta_col]\n",
    "        \n",
    "        utterance_list.append(Utterance(row[id_col], user, row[id_col], None, time, \\\n",
    "                                        row[text_col], meta=metadata))\n",
    "    \n",
    "    return Corpus(utterances = utterance_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For meta data, we will include the normalized score, its corresponding binary label (based on a 75% vs. 25% percentile cutoff -- technically there are three classes, but we will only look at the two ends, thus \"binary\"), as well as all original annotations with turker information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- detailed annotations information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4353/4353 [00:10<00:00, 430.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# for simplicity, we will condense the turker information together\n",
    "df[\"Annotations\"] = [dict(zip([df.iloc[i][\"TurkId{}\".format(j)] for j in range(1,6)], \\\n",
    "                             [df.iloc[i][\"Score{}\".format(j)] for j in range(1,6)])) for i in tqdm(range(len(df)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- polite vs. impolite label (note that we are only interested in labels that are either +1 or -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the binary label based on Normalized score\n",
    "top = np.percentile(df['Normalized Score'], 75)\n",
    "bottom = np.percentile(df[\"Normalized Score\"], 25)\n",
    "df['Binary'] = [int(score >= top) - int(score <= bottom) for score in df['Normalized Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4353it [00:01, 3687.95it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus = convert_df_to_corpus(df, \"Id\", \"Request\", [\"Normalized Score\", \"Binary\", \"Annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you were to download the data directly, here is how: \n",
    "# wiki_corpus = Corpus(download(\"wiki-politeness-annotated\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus = Corpus(filename=\"../.convokit/saved-corpora/wiki-politeness-annotated/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: annotate the corpus with politeness strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get politeness strategies for each utterance, we will first obtain dependency parses for the utterances, and then check for strategy use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Parser, PolitenessStrategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding dependency parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = Parser()\n",
    "wiki_corpus = annotator.fit_transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding strategy information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PolitenessStrategies()\n",
    "wiki_corpus = ps.transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how a processed utterance now look. Dependency parses are stored in `parsed`, and politeness strategies are in `politeness_strategies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': 629705, 'user': User([('name', 'wiki_user')]), 'root': 629705, 'reply_to': None, 'timestamp': 'NOT_RECORDED', 'text': \"Where did you learn English? How come you're taking on a third language?\", 'meta': {'Normalized Score': -1.1200492637766977, 'Binary': -1, 'Annotations': {'A2UFD1I8ZO1V4G': 13, 'A2YFPO0N4GIS25': 9, 'AYG3MF094634L': 11, 'A38WUWONC7EXTO': 11, 'A15DM9BMKZZJQ6': 5}, 'parsed': Where did you learn English? How come you're taking on a third language?, 'politeness_strategies': {'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 0, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 0, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 1, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 0, 'feature_politeness_==HASHEDGE==': 0, 'feature_politeness_==HASPOSITIVE==': 0, 'feature_politeness_==HASNEGATIVE==': 0}}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_corpus.get_utterance(629705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to save the corpus by doing `wiki_corpus.dump(\"wiki-politeness-annotated\")` for further exploration. Note that if you do not specify a base path, data will be saved to `.convokit/saved-corpora` in your home directory by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. predict politeness \n",
    "\n",
    "We will see how a simple classifier considering the use of politeness strategies perform. Note that this is only for demonstration, and not geared towards achieving best performance. \n",
    "\n",
    "(Most of the code below are adapted from [here](https://github.com/sudhof/politeness/blob/master/scripts/train_model.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this prediction task, we will only consider the polite vs. impolite group (i.e., those with \"Binary\" field being either +1 or -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_corpus = Corpus(utterances=[utt for utt in wiki_corpus.iter_utterances() if utt.meta[\"Binary\"] != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from \"documents2feature_vectors\"\n",
    "def corpus2feature_vectors(corpus: Corpus, ids: List[int]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        corpus {Corpus} -- The corpus being converted. \n",
    "                        Requires pre-computed politeness_strategies in the utterance meta field. \n",
    "                        \n",
    "        ids {List[int]} -- ids being considered \n",
    "    \"\"\"\n",
    "    \n",
    "    fks = False\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for utt_id in ids:\n",
    "        \n",
    "        utt = corpus.get_utterance(utt_id)\n",
    "        fs = utt.meta[\"politeness_strategies\"]\n",
    "        if not fks:\n",
    "            fks = sorted(fs.keys())\n",
    "        fv = [fs[f] for f in fks]\n",
    "        \n",
    "        # the utterance is regarded as polite if its score is in the top 75% percentile (i.e., Binary = 1)\n",
    "        # and it is regarded as impolite is its score lies in the bottom 25% percentile (i.e., Binary = -1)\n",
    "        l = 1 if utt.meta[\"Binary\"] == 1 else 0\n",
    "        \n",
    "        X.append(fv)\n",
    "        y.append(l)\n",
    "        \n",
    "    X = csr_matrix(np.asarray(X))\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from \"train_svm\"\n",
    "def train_svm(corpus: Corpus, ntesting: int = 500, rseed:int = 123):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        corpus: annotated training corpus\n",
    "        ntesting:  number of docs to reserve for testing\n",
    "    \"\"\"\n",
    "\n",
    "    utt_ids = corpus.get_utterance_ids()\n",
    "    random.seed(rseed)\n",
    "    random.shuffle(utt_ids)\n",
    "\n",
    "    testing_ids = utt_ids[-ntesting:]\n",
    "    training_ids = utt_ids[:-ntesting]\n",
    "    \n",
    "    print(\"sample test_ids:\", testing_ids[-5:])\n",
    "\n",
    "    X, y = corpus2feature_vectors(corpus, training_ids)\n",
    "    Xtest, ytest = corpus2feature_vectors(corpus, testing_ids)\n",
    "\n",
    "    print(\"Fitting\")\n",
    "    clf = svm.SVC(C=0.02, kernel='linear', probability=True)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Test\n",
    "    y_pred = clf.predict(Xtest)\n",
    "    print(\"accuracy = {}\".format(np.mean(y_pred == ytest)))\n",
    "    print(classification_report(ytest, y_pred, labels = [1, 0], target_names=[\"polite\", \"impolite\"]))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = corpus2feature_vectors(binary_corpus, binary_corpus.get_utterance_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample test_ids: [485293, 252109, 623560, 328144, 627508]\n",
      "Fitting\n",
      "accuracy = 0.736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     polite       0.75      0.70      0.73       249\n",
      "   impolite       0.72      0.77      0.75       251\n",
      "\n",
      "avg / total       0.74      0.74      0.74       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = train_svm(binary_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this classifier to predict politeness labels for Utterances. As an example, we will use some test utterances, but you can also consider use this classifier to predict on new utterances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [485293, 252109, 623560, 328144, 627508]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unlabeled utterances, you will need to featurize slightly differently, as y wouldn't be available\n",
    "Xtest, y = corpus2feature_vectors(binary_corpus, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predicting for test utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)\n",
    "yprob = clf.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- to check predicted politeness label ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test utterance:\n",
      "Blocked, templated.  Next?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.81650082 0.18349918]\n",
      "\n",
      "1\n",
      "test utterance:\n",
      "Stephan, what did you mean by ''\"Is English your native language? You seem to fill in a lot of things not said with your assumptions.\"'' on my talk?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.7367566 0.2632434]\n",
      "\n",
      "2\n",
      "test utterance:\n",
      "I see you created a nonsense article yesterday because you were bored. If I unblock you will you disrupt more?\n",
      "------------------------\n",
      "Result: polite, probability estimates = [0.37764129 0.62235871]\n",
      "\n",
      "3\n",
      "test utterance:\n",
      "I have no need to search the interwebs, all that matters is it offends people and is a violation of NPOV and MoS. \"All Wikipedia articles and other encyclopedic content must be written from a neutral point of view (NPOV), representing fairly and without bias all significant views (that have been published by reliable sources)\" - ess-eff is a bias term for a minority group of fans, put it this way: is there an SF-channel, or how often is the clichxe9 ess-eff used outside fan-groups?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.51761742 0.48238258]\n",
      "\n",
      "4\n",
      "test utterance:\n",
      "I responded to your talk page comment, encouraging you to try your hand at reworking it.  Good analysis of a potential problem, so why not take a whack at it?\n",
      "------------------------\n",
      "Result: polite, probability estimates = [0.32961796 0.67038204]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2label = {1: \"polite\", 0: \"impolite\"}\n",
    "\n",
    "for i, idx in enumerate(test_ids):\n",
    "    print(i)\n",
    "    test_utt = binary_corpus.get_utterance(idx)\n",
    "    print(\"test utterance:\\n{}\".format(test_utt.text))\n",
    "    print(\"------------------------\")\n",
    "    print(\"Result: {}, probability estimates = {}\\n\".format(pred2label[ypred[i]], yprob[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this is an implementation of a politeness classifier is trained on a specific dataset (wikipedia) and on a specific binarization of politeness classes. Depending on your scenario, you might find it is preferable to directly use the politeness strategies, as exemplified in the [conversations gone awry example](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb), rather than a politeness label/score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
