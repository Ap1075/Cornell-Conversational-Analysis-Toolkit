{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politeness prediction with ConvoKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to train a simple classifier to predict the politeness level of a request by considering the politeness strategies used, as seen in the paper [A computational approach to politeness with application to social factors](https://www.cs.cornell.edu/~cristian/Politeness.html), using ConvoKit. Note that this notebook is *not* intended to reproduce the paper results: legacy code for reproducibility is available at this [repository](https://github.com/sudhof/politeness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"../github/Cornell-Conversational-Analysis-Toolkit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liye/miniconda3/work/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../github/Cornell-Conversational-Analysis-Toolkit/convokit/__init__.py\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, User, Utterance\n",
    "print(convokit.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from typing import List, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Loading (and converting) annotated dataset\n",
    "\n",
    "We will be using the wikipedia annotations from the [Stanford Politeness Corpus](https://www.cs.cornell.edu/~cristian/Politeness.html). \n",
    "\n",
    "Code below demonstrates how to convert the original CSV file into the corpus format expected by ConvoKit, but this resultant corpus can also be directly downloaded using the helper function `download(\"wiki-politeness-annotated\")`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to modify the filepath depending on where your downloaded version is stored \n",
    "df = pd.read_csv(\"/Stanford_politeness_corpus/wikipedia.annotated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Id</th>\n",
       "      <th>Request</th>\n",
       "      <th>Score1</th>\n",
       "      <th>Score2</th>\n",
       "      <th>Score3</th>\n",
       "      <th>Score4</th>\n",
       "      <th>Score5</th>\n",
       "      <th>TurkId1</th>\n",
       "      <th>TurkId2</th>\n",
       "      <th>TurkId3</th>\n",
       "      <th>TurkId4</th>\n",
       "      <th>TurkId5</th>\n",
       "      <th>Normalized Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>629705</td>\n",
       "      <td>Where did you learn English? How come you're t...</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>A2UFD1I8ZO1V4G</td>\n",
       "      <td>A2YFPO0N4GIS25</td>\n",
       "      <td>AYG3MF094634L</td>\n",
       "      <td>A38WUWONC7EXTO</td>\n",
       "      <td>A15DM9BMKZZJQ6</td>\n",
       "      <td>-1.120049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>244336</td>\n",
       "      <td>Thanks very much for your edit to the &lt;url&gt; ar...</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>A2QN0EGBRGJU1M</td>\n",
       "      <td>A2GSW5RBAT5LQ5</td>\n",
       "      <td>AO5E3LWBYM72K</td>\n",
       "      <td>A2ULMYRKQMNNFG</td>\n",
       "      <td>A3TFQK7QK8X6LM</td>\n",
       "      <td>1.313955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Community      Id                                            Request  \\\n",
       "0  Wikipedia  629705  Where did you learn English? How come you're t...   \n",
       "1  Wikipedia  244336  Thanks very much for your edit to the <url> ar...   \n",
       "\n",
       "   Score1  Score2  Score3  Score4  Score5         TurkId1         TurkId2  \\\n",
       "0      13       9      11      11       5  A2UFD1I8ZO1V4G  A2YFPO0N4GIS25   \n",
       "1      23      16      24      21      25  A2QN0EGBRGJU1M  A2GSW5RBAT5LQ5   \n",
       "\n",
       "         TurkId3         TurkId4         TurkId5  Normalized Score  \n",
       "0  AYG3MF094634L  A38WUWONC7EXTO  A15DM9BMKZZJQ6         -1.120049  \n",
       "1  AO5E3LWBYM72K  A2ULMYRKQMNNFG  A3TFQK7QK8X6LM          1.313955  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to convert it to the format ConvoKit expects. Here is a simple helper function that does the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_corpus(df: DataFrame, id_col: str, text_col: str, meta_cols: List[str]) -> Corpus:\n",
    "    \n",
    "    \"\"\" Helper function to convert data to Corpus format\n",
    "     \n",
    "    Arguments:\n",
    "        df {DataFrame} -- Actual data, in a pandas Dataframe\n",
    "        id_col {str} -- name of the column that corresponds to utterances ids \n",
    "        text_col {str} -- name of the column that stores texts of the utterances  \n",
    "        meta_cols {List[str]} -- set of columns that stores relevant metadata \n",
    "    \n",
    "    Returns:\n",
    "        Corpus -- the converted corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    # in this particular case, user, reply_to, and timestamp information are all not applicable \n",
    "    # and we will simply either create a placeholder entry, or leave it as None \n",
    "        \n",
    "    user = User(\"wiki_user\")\n",
    "    time = \"NOT_RECORDED\"\n",
    "\n",
    "    utterance_list = []    \n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        \n",
    "        # extracting meta data\n",
    "        metadata = {}\n",
    "        for meta_col in meta_cols:\n",
    "            metadata[meta_col] = row[meta_col]\n",
    "        \n",
    "        utterance_list.append(Utterance(str(row[id_col]), user, row[id_col], None, time, \\\n",
    "                                        row[text_col], meta=metadata))\n",
    "    \n",
    "    return Corpus(utterances = utterance_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For meta data, we will include the normalized score, its corresponding binary label (based on a 75% vs. 25% percentile cutoff -- technically there are three classes, but we will only look at the two ends, thus \"binary\"), as well as all original annotations with turker information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- detailed annotations information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4353/4353 [00:10<00:00, 416.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# for simplicity, we will condense the turker information together\n",
    "df[\"Annotations\"] = [dict(zip([df.iloc[i][\"TurkId{}\".format(j)] for j in range(1,6)], \\\n",
    "                             [df.iloc[i][\"Score{}\".format(j)] for j in range(1,6)])) for i in tqdm(range(len(df)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- polite vs. impolite label (note that we are only interested in labels that are either +1 or -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the binary label based on Normalized score\n",
    "top = np.percentile(df['Normalized Score'], 75)\n",
    "bottom = np.percentile(df[\"Normalized Score\"], 25)\n",
    "df['Binary'] = [int(score >= top) - int(score <= bottom) for score in df['Normalized Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4353it [00:00, 4582.64it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_corpus = convert_df_to_corpus(df, \"Id\", \"Request\", [\"Normalized Score\", \"Binary\", \"Annotations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you were to download the data directly, here is how: \n",
    "# from convokit import download\n",
    "# wiki_corpus = Corpus(download(\"wiki-politeness-annotated\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Annotate the corpus with politeness strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get politeness strategies for each utterance, we will first obtain dependency parses for the utterances, and then check for strategy use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import TextParser, PolitenessStrategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding dependency parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050/4353 utterances processed\n",
      "100/4353 utterances processed\n",
      "150/4353 utterances processed\n",
      "200/4353 utterances processed\n",
      "250/4353 utterances processed\n",
      "300/4353 utterances processed\n",
      "350/4353 utterances processed\n",
      "400/4353 utterances processed\n",
      "450/4353 utterances processed\n",
      "500/4353 utterances processed\n",
      "550/4353 utterances processed\n",
      "600/4353 utterances processed\n",
      "650/4353 utterances processed\n",
      "700/4353 utterances processed\n",
      "750/4353 utterances processed\n",
      "800/4353 utterances processed\n",
      "850/4353 utterances processed\n",
      "900/4353 utterances processed\n",
      "950/4353 utterances processed\n",
      "1000/4353 utterances processed\n",
      "1050/4353 utterances processed\n",
      "1100/4353 utterances processed\n",
      "1150/4353 utterances processed\n",
      "1200/4353 utterances processed\n",
      "1250/4353 utterances processed\n",
      "1300/4353 utterances processed\n",
      "1350/4353 utterances processed\n",
      "1400/4353 utterances processed\n",
      "1450/4353 utterances processed\n",
      "1500/4353 utterances processed\n",
      "1550/4353 utterances processed\n",
      "1600/4353 utterances processed\n",
      "1650/4353 utterances processed\n",
      "1700/4353 utterances processed\n",
      "1750/4353 utterances processed\n",
      "1800/4353 utterances processed\n",
      "1850/4353 utterances processed\n",
      "1900/4353 utterances processed\n",
      "1950/4353 utterances processed\n",
      "2000/4353 utterances processed\n",
      "2050/4353 utterances processed\n",
      "2100/4353 utterances processed\n",
      "2150/4353 utterances processed\n",
      "2200/4353 utterances processed\n",
      "2250/4353 utterances processed\n",
      "2300/4353 utterances processed\n",
      "2350/4353 utterances processed\n",
      "2400/4353 utterances processed\n",
      "2450/4353 utterances processed\n",
      "2500/4353 utterances processed\n",
      "2550/4353 utterances processed\n",
      "2600/4353 utterances processed\n",
      "2650/4353 utterances processed\n",
      "2700/4353 utterances processed\n",
      "2750/4353 utterances processed\n",
      "2800/4353 utterances processed\n",
      "2850/4353 utterances processed\n",
      "2900/4353 utterances processed\n",
      "2950/4353 utterances processed\n",
      "3000/4353 utterances processed\n",
      "3050/4353 utterances processed\n",
      "3100/4353 utterances processed\n",
      "3150/4353 utterances processed\n",
      "3200/4353 utterances processed\n",
      "3250/4353 utterances processed\n",
      "3300/4353 utterances processed\n",
      "3350/4353 utterances processed\n",
      "3400/4353 utterances processed\n",
      "3450/4353 utterances processed\n",
      "3500/4353 utterances processed\n",
      "3550/4353 utterances processed\n",
      "3600/4353 utterances processed\n",
      "3650/4353 utterances processed\n",
      "3700/4353 utterances processed\n",
      "3750/4353 utterances processed\n",
      "3800/4353 utterances processed\n",
      "3850/4353 utterances processed\n",
      "3900/4353 utterances processed\n",
      "3950/4353 utterances processed\n",
      "4000/4353 utterances processed\n",
      "4050/4353 utterances processed\n",
      "4100/4353 utterances processed\n",
      "4150/4353 utterances processed\n",
      "4200/4353 utterances processed\n",
      "4250/4353 utterances processed\n",
      "4300/4353 utterances processed\n",
      "4350/4353 utterances processed\n"
     ]
    }
   ],
   "source": [
    "# new parser\n",
    "parser = TextParser(verbosity=50)\n",
    "corpus = parser.transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adding strategy information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PolitenessStrategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_corpus = ps.transform(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how a processed utterance now look. Dependency parses are stored in `parsed`, and politeness strategies are in `politeness_strategies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': '629705', 'user': User([('name', 'wiki_user')]), 'root': 629705, 'reply_to': None, 'timestamp': 'NOT_RECORDED', 'text': \"Where did you learn English? How come you're taking on a third language?\", 'meta': {'Normalized Score': -1.1200492637766977, 'Binary': -1, 'Annotations': {'A2UFD1I8ZO1V4G': 13, 'A2YFPO0N4GIS25': 9, 'AYG3MF094634L': 11, 'A38WUWONC7EXTO': 11, 'A15DM9BMKZZJQ6': 5}, 'parsed': [{'rt': 3, 'toks': [{'tok': 'Where', 'tag': 'WRB', 'dep': 'advmod', 'up': 3, 'dn': []}, {'tok': 'did', 'tag': 'VBD', 'dep': 'aux', 'up': 3, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 3, 'dn': []}, {'tok': 'learn', 'tag': 'VB', 'dep': 'ROOT', 'dn': [0, 1, 2, 4, 5]}, {'tok': 'English', 'tag': 'NNP', 'dep': 'dobj', 'up': 3, 'dn': []}, {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 3, 'dn': []}]}, {'rt': 4, 'toks': [{'tok': 'How', 'tag': 'WRB', 'dep': 'advmod', 'up': 4, 'dn': []}, {'tok': 'come', 'tag': 'VB', 'dep': 'aux', 'up': 4, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 4, 'dn': []}, {'tok': \"'re\", 'tag': 'VBP', 'dep': 'aux', 'up': 4, 'dn': []}, {'tok': 'taking', 'tag': 'VBG', 'dep': 'ROOT', 'dn': [0, 1, 2, 3, 5, 8, 9]}, {'tok': 'on', 'tag': 'RP', 'dep': 'prt', 'up': 4, 'dn': []}, {'tok': 'a', 'tag': 'DT', 'dep': 'det', 'up': 8, 'dn': []}, {'tok': 'third', 'tag': 'JJ', 'dep': 'amod', 'up': 8, 'dn': []}, {'tok': 'language', 'tag': 'NN', 'dep': 'dobj', 'up': 4, 'dn': [6, 7]}, {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 4, 'dn': []}]}], 'politeness_strategies': {'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 0, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 0, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 1, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 0, 'feature_politeness_==HASHEDGE==': 0, 'feature_politeness_==HASPOSITIVE==': 0, 'feature_politeness_==HASNEGATIVE==': 0}}})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_corpus.get_utterance(629705)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to save the corpus by doing `wiki_corpus.dump(\"wiki-politeness-annotated\")` for further exploration. Note that if you do not specify a base path, data will be saved to `.convokit/saved-corpora` in your home directory by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] possibly some summarizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.summarize(wiki_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Predict Politeness \n",
    "\n",
    "We will see how a simple classifier considering the use of politeness strategies perform, using `Classifier` (note that this is only for demonstration, and not geared towards achieving best performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a prestep, we subset the corpus as we will only consider the polite vs. impolite class for prediction (i.e., those with \"Binary\" field being either +1 or -1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_corpus = Corpus(utterances=[utt for utt in wiki_corpus.iter_utterances() if utt.meta[\"Binary\"] != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Direct evaluation\n",
    "\n",
    "If you are interested in how effectiveness are these politeness strategies, `Classifier` provides evaluation with both train/test splits, as well as with cross validaton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cross validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus objects...\n",
      "Running a cross-validated evaluation...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.77981651, 0.75      , 0.74311927, 0.76551724, 0.71724138])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cv = Classifier(obj_type=\"utterance\", \n",
    "                        pred_feats=[\"politeness_strategies\"], \n",
    "                        labeller=lambda utt: utt.meta['Binary'] == 1)\n",
    "\n",
    "clf_cv.evaluate_with_cv(binary_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using corpus objects...\n",
      "Running a train-test-split evaluation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2178, 436]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-05c4f951e435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         labeller=lambda utt: utt.meta['Binary'] == 1)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_with_train_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/Cornell-Conversational-Analysis-Toolkit/convokit/classifier/classifier.py\u001b[0m in \u001b[0;36mevaluate_with_train_test_split\u001b[0;34m(self, corpus, objs, test_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     def evaluate_with_cv(self, corpus: Corpus = None,\n",
      "\u001b[0;32m~/miniconda3/work/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/work/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/work/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2178, 436]"
     ]
    }
   ],
   "source": [
    "clf_split = Classifier(obj_type=\"utterance\", \n",
    "                        pred_feats=[\"politeness_strategies\"], \n",
    "                        labeller=lambda utt: utt.meta['Binary'] == 1)\n",
    "\n",
    "clf_split.evaluate_with_train_test_split(binary_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Training a classifier to predict on other utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 2078, test size = 100\n"
     ]
    }
   ],
   "source": [
    "test_ids = binary_corpus.get_utterance_ids()[-100:]\n",
    "train_corpus = Corpus(utterances=[utt for utt in binary_corpus.iter_utterances() if utt.id not in test_ids])\n",
    "test_corpus = Corpus(utterances=[utt for utt in binary_corpus.iter_utterances() if utt.id in test_ids])\n",
    "print(\"train size = {}, test size = {}\".format(len(train_corpus.get_utterance_ids()),\n",
    "                                               len(test_corpus.get_utterance_ids())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train a classifier with a corpus to predict politeness labels for other Utterances. As an example, we will first train with a training corpus, and check predictions on some test utterances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.classifier.classifier.Classifier at 0x7f25a6eb8750>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Classifier(obj_type=\"utterance\", \n",
    "                        pred_feats=[\"politeness_strategies\"], \n",
    "                        labeller=lambda utt: utt.meta['Binary'] == 1)\n",
    "clf.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predicing on the test corpus (you can also predict on a list of utterances by using `clf.transform_objs()` instead) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>486441</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.27032699076635974, 0.7296730092336401]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626897</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8839569359863679, 0.11604306401363214]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626894</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7508903365652358, 0.24910966343476404]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626728</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9288241675632224, 0.07117583243677755]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620909</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.17450385896512047, 0.8254961410348793]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60798</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8830846074207092, 0.11691539257929102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156734</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8577681141817989, 0.14223188581820106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147665</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5270653974370094, 0.47293460256299075]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234095</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.34406990095603507, 0.6559300990439649]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563032</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.18548454831311967, 0.8145154516868803]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        prediction                                      score\n",
       "id                                                           \n",
       "486441           1  [0.27032699076635974, 0.7296730092336401]\n",
       "626897           0  [0.8839569359863679, 0.11604306401363214]\n",
       "626894           0  [0.7508903365652358, 0.24910966343476404]\n",
       "626728           0  [0.9288241675632224, 0.07117583243677755]\n",
       "620909           1  [0.17450385896512047, 0.8254961410348793]\n",
       "...            ...                                        ...\n",
       "60798            0  [0.8830846074207092, 0.11691539257929102]\n",
       "156734           0  [0.8577681141817989, 0.14223188581820106]\n",
       "147665           0  [0.5270653974370094, 0.47293460256299075]\n",
       "234095           1  [0.34406990095603507, 0.6559300990439649]\n",
       "563032           1  [0.18548454831311967, 0.8145154516868803]\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.summarize(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': 563032, 'user': User([('name', 'wiki_user')]), 'root': 563032, 'reply_to': None, 'timestamp': 'NOT_RECORDED', 'text': \"I don't think anybody wants the stale uninformative old Victorian results infoboxes. Can you gain consensus before deviating?\", 'meta': {'Normalized Score': -0.5716893911454594, 'Binary': -1, 'Annotations': {'A233ONYNWKDIYF': 9, 'A2UFD1I8ZO1V4G': 13, 'A194CUACV9SQ1K': 13, 'A29B522D0BX6HN': 11, 'AIPK94CUWL45W': 13}, 'parsed': [{'rt': 3, 'toks': [{'tok': 'I', 'tag': 'PRP', 'dep': 'nsubj', 'up': 3, 'dn': []}, {'tok': 'do', 'tag': 'VBP', 'dep': 'aux', 'up': 3, 'dn': []}, {'tok': \"n't\", 'tag': 'RB', 'dep': 'neg', 'up': 3, 'dn': []}, {'tok': 'think', 'tag': 'VB', 'dep': 'ROOT', 'dn': [0, 1, 2, 5, 13]}, {'tok': 'anybody', 'tag': 'NN', 'dep': 'nsubj', 'up': 5, 'dn': []}, {'tok': 'wants', 'tag': 'VBZ', 'dep': 'ccomp', 'up': 3, 'dn': [4, 12]}, {'tok': 'the', 'tag': 'DT', 'dep': 'det', 'up': 11, 'dn': []}, {'tok': 'stale', 'tag': 'JJ', 'dep': 'amod', 'up': 11, 'dn': []}, {'tok': 'uninformative', 'tag': 'JJ', 'dep': 'amod', 'up': 11, 'dn': []}, {'tok': 'old', 'tag': 'JJ', 'dep': 'amod', 'up': 11, 'dn': []}, {'tok': 'Victorian', 'tag': 'JJ', 'dep': 'amod', 'up': 11, 'dn': []}, {'tok': 'results', 'tag': 'NNS', 'dep': 'nsubj', 'up': 12, 'dn': [6, 7, 8, 9, 10]}, {'tok': 'infoboxes', 'tag': 'NNS', 'dep': 'ccomp', 'up': 5, 'dn': [11]}, {'tok': '.', 'tag': '.', 'dep': 'punct', 'up': 3, 'dn': []}]}, {'rt': 2, 'toks': [{'tok': 'Can', 'tag': 'MD', 'dep': 'aux', 'up': 2, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 2, 'dn': []}, {'tok': 'gain', 'tag': 'VB', 'dep': 'ROOT', 'dn': [0, 1, 3, 4, 6]}, {'tok': 'consensus', 'tag': 'NN', 'dep': 'dobj', 'up': 2, 'dn': []}, {'tok': 'before', 'tag': 'IN', 'dep': 'prep', 'up': 2, 'dn': [5]}, {'tok': 'deviating', 'tag': 'VBG', 'dep': 'pcomp', 'up': 4, 'dn': []}, {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 2, 'dn': []}]}], 'politeness_strategies': {'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 1, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 1, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 0, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 1, 'feature_politeness_==HASHEDGE==': 1, 'feature_politeness_==HASPOSITIVE==': 1, 'feature_politeness_==HASNEGATIVE==': 1}, 'prediction': 1, 'score': array([0.18548455, 0.81451545])}})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus.get_utterance(563032)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at a few example predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test utterance:\n",
      "I understood just fine, but wasn't at my computer. Are you in a hurry?\n",
      "------------------------\n",
      "Result: polite, probability estimates = [0.27032699 0.72967301]\n",
      "\n",
      "1\n",
      "test utterance:\n",
      "I've always been intrigued by 'dark-complected man.' What's with the radio, and fist in the air?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.88395694 0.11604306]\n",
      "\n",
      "2\n",
      "test utterance:\n",
      "Your early edit's clearly indicate that you were not a newbie. How do explain this?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.75089034 0.24910966]\n",
      "\n",
      "3\n",
      "test utterance:\n",
      "Instead of another 3O, why don't you put in a <url>. And no, it's not a threat - it's an observation - why don't you <url>?\n",
      "------------------------\n",
      "Result: impolite, probability estimates = [0.92882417 0.07117583]\n",
      "\n",
      "4\n",
      "test utterance:\n",
      "Great Article RaveenS, Do u want me to add this to the template (Sri Lankan Conflict)? I think it should be included in the ''see also'' section what do you suggest?\n",
      "------------------------\n",
      "Result: polite, probability estimates = [0.17450386 0.82549614]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2label = {1: \"polite\", 0: \"impolite\"}\n",
    "\n",
    "for i, idx in enumerate(test_ids[0:5]):\n",
    "    print(i)\n",
    "    test_utt = test_corpus.get_utterance(idx)\n",
    "    ypred, yprob = test_utt.meta['prediction'], test_utt.meta['score']\n",
    "    print(\"test utterance:\\n{}\".format(test_utt.text))\n",
    "    print(\"------------------------\")\n",
    "    print(\"Result: {}, probability estimates = {}\\n\".format(pred2label[ypred], yprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that this is an implementation of a politeness classifier trained on a specific dataset (wikipedia) and on a specific binarization of politeness classes. Depending on your scenario, you might find it preferable to directly use the politeness strategies, as exemplified in the [conversations gone awry example](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb), rather than a politeness label/score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
