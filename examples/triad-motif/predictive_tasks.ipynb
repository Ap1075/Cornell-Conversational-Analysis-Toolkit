{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/calebchiam/Documents/GitHub/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Hypergraph features for various predictive tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/reddit-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download(\"reddit-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x11f895a58>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)\n",
    "hc.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_cache = False\n",
    "if remake_cache:\n",
    "    with open(\"hyperconvo_feats.p\", \"wb\") as f:\n",
    "        hyperconvo_feats = {}\n",
    "        for convo in corpus.iter_conversations():\n",
    "            hyperconvo_feats.update(convo.meta[\"hyperconvo\"])\n",
    "        pickle.dump(hyperconvo_feats, f)\n",
    "else:\n",
    "    with open(\"hyperconvo_feats.p\", \"rb\") as f:\n",
    "        hyperconvo_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = 0\n",
    "present = 0\n",
    "for tlc_id, thread in threads.items():\n",
    "    if tlc_id not in thread:\n",
    "        missing += 1\n",
    "    else:\n",
    "        present += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the first 10 comments in each thread\n",
    "thread_pfxs = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "thread_roots_by_self_post = defaultdict(list)\n",
    "for top_level_comment, thread in threads.items():\n",
    "    rt = thread[next(iter(thread))].root\n",
    "    thread_roots_by_self_post[rt].append(top_level_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stats = hc.retrieve_motif_pathway_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_counts = hc.retrieve_motif_counts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motifs = hc.retrieve_motifs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NO_EDGE_TRIADS': [<convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34f28>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34898>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b347f0>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34630>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34128>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34198>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34048>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b34160>],\n",
       " 'SINGLE_EDGE_TRIADS': [<convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b343c8>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x1c2b340b8>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3feb8>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f3c8>],\n",
       " 'INCOMING_TRIADS': [],\n",
       " 'OUTGOING_TRIADS': [],\n",
       " 'DYADIC_TRIADS': [<convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3fe80>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f668>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f390>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f160>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3fb70>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f320>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f630>,\n",
       "  <convokit.hyperconvo.triadMotif.TriadMotif at 0x11fb3f128>],\n",
       " 'UNIDIRECTIONAL_TRIADS': [],\n",
       " 'INCOMING_2TO3_TRIADS': [],\n",
       " 'INCOMING_1TO3_TRIADS': [],\n",
       " 'DIRECTED_CYCLE_TRIADS': [],\n",
       " 'OUTGOING_3TO1_TRIADS': [],\n",
       " 'INCOMING_RECIPROCAL_TRIADS': [],\n",
       " 'OUTGOING_RECIPROCAL_TRIADS': [],\n",
       " 'DIRECTED_CYCLE_1TO3_TRIADS': [],\n",
       " 'DIRECIPROCAL_TRIADS': [],\n",
       " 'DIRECIPROCAL_2TO3_TRIADS': [],\n",
       " 'TRIRECIPROCAL_TRIADS': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads_motifs['dnppqdj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_motif_count = {thread_id: hc._latent_motif_count(motif_dict, trans=False)[0] for thread_id, motif_dict in threads_motifs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import normalize, StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate positive and negative examples based on task\n",
    "\n",
    "def generate_pos_neg(task: str, post_to_thread_obj, threads, thread_pfxs):\n",
    "    pos, neg = [], []\n",
    "    if task == \"comment-growth\":\n",
    "        for post_id, thread_roots in post_to_thread_obj.items():\n",
    "            has_pos = [root for root in thread_roots if len(threads[root]) >= 15]\n",
    "            has_neg = [root for root in thread_roots if len(threads[root]) == 10]\n",
    "            \n",
    "            if has_pos and has_neg:\n",
    "                pos.append(random.choice(has_pos))\n",
    "                neg.append(random.choice(has_neg))\n",
    "    elif task == \"commenter-growth\":\n",
    "        for post_id, thread_roots in post_to_thread_obj.items():\n",
    "            has_pos, has_neg = [], []\n",
    "            for root in thread_roots:\n",
    "                if len(threads[root]) >= 20:\n",
    "                    if len(set(c.user.name for c in threads[root].values())) >= \\\n",
    "                        len(set(c.user.name for c in thread_pfxs[root].values())) * 2:\n",
    "                            has_pos.append(root)\n",
    "                    else:\n",
    "                        has_neg.append(root)\n",
    "            if has_pos and has_neg:\n",
    "                pos.append(random.choice(has_pos))\n",
    "                neg.append(random.choice(has_neg))\n",
    "    print(\"- {} positive, {} negative pts for {} task\".format(len(pos), len(neg), task))\n",
    "    \n",
    "    return pos, neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1723 positive, 1723 negative pts for comment-growth task\n"
     ]
    }
   ],
   "source": [
    "pos_comment_growth, neg_comment_growth = generate_pos_neg(\"comment-growth\", \n",
    "                                                          thread_roots_by_self_post,\n",
    "                                                          threads,\n",
    "                                                          thread_pfxs\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1121 positive, 1121 negative pts for commenter-growth task\n"
     ]
    }
   ],
   "source": [
    "pos_commenter_growth, neg_commenter_growth = generate_pos_neg(\"commenter-growth\", \n",
    "                                                          thread_roots_by_self_post,\n",
    "                                                          threads,\n",
    "                                                          thread_pfxs\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract features for all these examples\n",
    "\n",
    "# construct the pair_df, flipping between positive and negative\n",
    "\n",
    "# StandardScaler + LogisticRegression to finish off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paired_features(feats, pos, neg):\n",
    "    X, y = [], []\n",
    "    flip = True\n",
    "    \n",
    "    for idx in range(len(pos)):\n",
    "        pos_feats = np.array(list(feats[pos[idx]].values()))\n",
    "        neg_feats = np.array(list(feats[neg[idx]].values()))\n",
    "        \n",
    "        if np.isnan(pos_feats).any() or np.isnan(neg_feats).any(): continue\n",
    "            \n",
    "        if flip:\n",
    "            y.append(1)\n",
    "            diff = pos_feats - neg_feats\n",
    "        else:\n",
    "            y.append(0)\n",
    "            diff = neg_feats - pos_feats\n",
    "        X.append(diff)\n",
    "        flip = not flip\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_motif = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_motif.items():\n",
    "    feats.update(motif_counts[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_paths = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_paths.items():\n",
    "    feats.update(path_stats[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_latent = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_latent.items():\n",
    "    feats.update(latent_motif_count[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperconv_motifall = deepcopy(hyperconvo_feats)\n",
    "for thread_id, feats in hyperconv_motifall.items():\n",
    "    feats.update(motif_counts[thread_id])\n",
    "    feats.update(path_stats[thread_id])\n",
    "    feats.update(latent_motif_count[thread_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feats):\n",
    "    feats_ordered = list(feats[next(iter(feats))])\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "    \n",
    "    assert len(feats_ordered) == len(coefs)\n",
    "    feats_coefs = sorted(list(zip(feats_ordered, coefs)), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"TOP 5 FEATURES\")\n",
    "    for ft, coef in feats_coefs[:5]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM 5 FEATURES\")\n",
    "    for ft, coef in feats_coefs[-5:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK: comment-growth\n",
      "\n",
      "- 1723 positive, 1723 negative pts for comment-growth task\n",
      "- hyperconv: 0.6379 train, 0.6291 test\n",
      "TOP 5 FEATURES\n",
      "norm.max[indegree over C->C mid-thread responses]: 0.352\n",
      "max[indegree over C->C responses]: 0.351\n",
      "entropy[indegree over C->C responses]: 0.332\n",
      "mean[outdegree over C->c responses]: 0.294\n",
      "mean[outdegree over C->C responses]: 0.294\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "mean-nonzero[outdegree over C->C responses]: -0.318\n",
      "norm.max[outdegree over C->c mid-thread responses]: -0.365\n",
      "norm.max[outdegree over C->C mid-thread responses]: -0.365\n",
      "entropy[outdegree over C->c mid-thread responses]: -0.875\n",
      "entropy[outdegree over C->C mid-thread responses]: -0.875\n",
      "\n",
      "- motifcount: 0.6176 train, 0.6029 test\n",
      "TOP 5 FEATURES\n",
      "NO_EDGE_TRIADS: 0.623\n",
      "DIRECIPROCAL_TRIADS: 0.245\n",
      "OUTGOING_TRIADS: 0.205\n",
      "DIRECTED_CYCLE_TRIADS: 0.162\n",
      "SINGLE_EDGE_TRIADS: 0.142\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "UNIDIRECTIONAL_TRIADS: 0.030\n",
      "INCOMING_1TO3_TRIADS: 0.000\n",
      "OUTGOING_RECIPROCAL_TRIADS: -0.028\n",
      "DYADIC_TRIADS: -0.031\n",
      "INCOMING_TRIADS: -0.155\n",
      "\n",
      "- latentmotif: 0.6226 train, 0.6116 test\n",
      "TOP 5 FEATURES\n",
      "NO_EDGE_TRIADS: 0.910\n",
      "INCOMING_1TO3_TRIADS: 0.300\n",
      "OUTGOING_TRIADS: 0.197\n",
      "OUTGOING_3TO1_TRIADS: 0.129\n",
      "DIRECIPROCAL_2TO3_TRIADS: 0.109\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "DIRECTED_CYCLE_1TO3_TRIADS: -0.052\n",
      "OUTGOING_RECIPROCAL_TRIADS: -0.062\n",
      "DYADIC_TRIADS: -0.099\n",
      "SINGLE_EDGE_TRIADS: -0.162\n",
      "INCOMING_TRIADS: -0.194\n",
      "\n",
      "- motifpaths: 0.6263 train, 0.6058 test\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS',): 0.610\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'INCOMING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.268\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_2TO3_TRIADS', 'INCOMING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.223\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECIPROCAL_TRIADS'): 0.214\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'OUTGOING_3TO1_TRIADS', 'INCOMING_RECIPROCAL_TRIADS'): 0.210\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS'): -0.142\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): -0.152\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'OUTGOING_3TO1_TRIADS', 'DIRECIPROCAL_TRIADS'): -0.199\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'OUTGOING_3TO1_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): -0.236\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_2TO3_TRIADS', 'INCOMING_RECIPROCAL_TRIADS'): -0.249\n",
      "\n",
      "- hyperconv-motif: 0.6587 train, 0.6647 test\n",
      "TOP 5 FEATURES\n",
      "NO_EDGE_TRIADS: 0.746\n",
      "entropy[indegree over C->C responses]: 0.616\n",
      "max[outdegree over C->c responses]: 0.528\n",
      "max[outdegree over C->C responses]: 0.528\n",
      "norm.max[indegree over C->C mid-thread responses]: 0.489\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "norm.max[outdegree over C->C responses]: -0.349\n",
      "norm.max[outdegree over C->c mid-thread responses]: -0.381\n",
      "norm.max[outdegree over C->C mid-thread responses]: -0.381\n",
      "entropy[outdegree over C->c mid-thread responses]: -0.723\n",
      "entropy[outdegree over C->C mid-thread responses]: -0.723\n",
      "\n",
      "- hyperconv-paths: 0.6773 train, 0.6261 test\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS',): 0.762\n",
      "entropy[indegree over C->C responses]: 0.589\n",
      "norm.max[indegree over C->C mid-thread responses]: 0.520\n",
      "max[outdegree over C->c responses]: 0.428\n",
      "max[outdegree over C->C responses]: 0.428\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "norm.max[outdegree over C->C mid-thread responses]: -0.421\n",
      "mean-nonzero[outdegree over C->c responses]: -0.488\n",
      "mean-nonzero[outdegree over C->C responses]: -0.488\n",
      "entropy[outdegree over C->c mid-thread responses]: -0.677\n",
      "entropy[outdegree over C->C mid-thread responses]: -0.677\n",
      "\n",
      "- hyperconv-latent: 0.6572 train, 0.6617 test\n",
      "TOP 5 FEATURES\n",
      "NO_EDGE_TRIADS: 1.017\n",
      "entropy[indegree over C->C responses]: 0.613\n",
      "max[outdegree over C->c responses]: 0.529\n",
      "max[outdegree over C->C responses]: 0.529\n",
      "norm.max[indegree over C->C mid-thread responses]: 0.513\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "norm.max[outdegree over C->c mid-thread responses]: -0.375\n",
      "norm.max[outdegree over C->C mid-thread responses]: -0.375\n",
      "SINGLE_EDGE_TRIADS: -0.450\n",
      "entropy[outdegree over C->c mid-thread responses]: -0.683\n",
      "entropy[outdegree over C->C mid-thread responses]: -0.683\n",
      "\n",
      "- hyperconvo-motifall: 0.6773 train, 0.6231 test\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS',): 0.589\n",
      "entropy[indegree over C->C responses]: 0.584\n",
      "norm.max[indegree over C->C mid-thread responses]: 0.521\n",
      "max[outdegree over C->c responses]: 0.430\n",
      "max[outdegree over C->C responses]: 0.430\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "norm.max[outdegree over C->C mid-thread responses]: -0.421\n",
      "mean-nonzero[outdegree over C->c responses]: -0.487\n",
      "mean-nonzero[outdegree over C->C responses]: -0.487\n",
      "entropy[outdegree over C->c mid-thread responses]: -0.677\n",
      "entropy[outdegree over C->C mid-thread responses]: -0.677\n",
      "\n",
      "TASK: commenter-growth\n",
      "\n",
      "- 1121 positive, 1121 negative pts for commenter-growth task\n",
      "- hyperconv: 0.6433 train, 0.5204 test\n",
      "TOP 5 FEATURES\n",
      "norm.max[indegree over C->C responses]: 0.799\n",
      "prop-nonzero[outdegree over C->c mid-thread responses]: 0.692\n",
      "prop-nonzero[outdegree over C->C mid-thread responses]: 0.692\n",
      "norm.2nd-largest[indegree over C->C responses]: 0.474\n",
      "max[indegree over c->c mid-thread responses]: 0.450\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest[outdegree over C->c responses]: -0.425\n",
      "2nd-largest[outdegree over C->C responses]: -0.425\n",
      "max[indegree over C->C responses]: -0.425\n",
      "mean-nonzero[indegree over C->C responses]: -0.525\n",
      "max[indegree over C->C mid-thread responses]: -0.562\n",
      "\n",
      "- motifcount: 0.5424 train, 0.5244 test\n",
      "TOP 5 FEATURES\n",
      "INCOMING_2TO3_TRIADS: 0.181\n",
      "INCOMING_TRIADS: 0.131\n",
      "DIRECTED_CYCLE_1TO3_TRIADS: 0.120\n",
      "OUTGOING_RECIPROCAL_TRIADS: 0.052\n",
      "TRIRECIPROCAL_TRIADS: 0.052\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "DIRECIPROCAL_TRIADS: -0.036\n",
      "INCOMING_RECIPROCAL_TRIADS: -0.055\n",
      "SINGLE_EDGE_TRIADS: -0.080\n",
      "DYADIC_TRIADS: -0.108\n",
      "OUTGOING_TRIADS: -0.172\n",
      "\n",
      "- latentmotif: 0.5491 train, 0.4933 test\n",
      "TOP 5 FEATURES\n",
      "INCOMING_TRIADS: 0.169\n",
      "INCOMING_2TO3_TRIADS: 0.154\n",
      "DIRECTED_CYCLE_TRIADS: 0.134\n",
      "INCOMING_1TO3_TRIADS: 0.100\n",
      "UNIDIRECTIONAL_TRIADS: 0.081\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "DYADIC_TRIADS: -0.101\n",
      "DIRECIPROCAL_TRIADS: -0.115\n",
      "INCOMING_RECIPROCAL_TRIADS: -0.129\n",
      "OUTGOING_TRIADS: -0.153\n",
      "SINGLE_EDGE_TRIADS: -0.163\n",
      "\n",
      "- motifpaths: 0.5580 train, 0.4800 test\n",
      "TOP 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): 0.285\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'OUTGOING_3TO1_TRIADS', 'DIRECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): 0.224\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_2TO3_TRIADS'): 0.223\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_2TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): 0.201\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): 0.175\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'UNIDIRECTIONAL_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS'): -0.172\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'OUTGOING_TRIADS', 'OUTGOING_3TO1_TRIADS'): -0.190\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS', 'DIRECIPROCAL_2TO3_TRIADS', 'TRIRECIPROCAL_TRIADS'): -0.246\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS', 'OUTGOING_RECIPROCAL_TRIADS'): -0.283\n",
      "('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_1TO3_TRIADS', 'DIRECTED_CYCLE_1TO3_TRIADS'): -0.295\n",
      "\n",
      "- hyperconv-motif: 0.6501 train, 0.5339 test\n",
      "TOP 5 FEATURES\n",
      "norm.max[indegree over C->C responses]: 0.718\n",
      "prop-nonzero[outdegree over C->c mid-thread responses]: 0.704\n",
      "prop-nonzero[outdegree over C->C mid-thread responses]: 0.704\n",
      "norm.2nd-largest[indegree over C->C responses]: 0.469\n",
      "max[indegree over c->c mid-thread responses]: 0.464\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest[outdegree over C->c responses]: -0.459\n",
      "2nd-largest[outdegree over C->C responses]: -0.459\n",
      "max[indegree over C->C mid-thread responses]: -0.486\n",
      "mean-nonzero[indegree over C->C responses]: -0.491\n",
      "entropy[indegree over C->C responses]: -0.662\n",
      "\n",
      "- hyperconv-paths: 0.6591 train, 0.5294 test\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[outdegree over C->c mid-thread responses]: 0.754\n",
      "prop-nonzero[outdegree over C->C mid-thread responses]: 0.754\n",
      "norm.max[indegree over C->C responses]: 0.715\n",
      "prop-multiple[indegree over C->C responses]: 0.548\n",
      "norm.2nd-largest[indegree over C->C responses]: 0.532\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest[outdegree over C->c responses]: -0.375\n",
      "2nd-largest[outdegree over C->C responses]: -0.375\n",
      "mean-nonzero[indegree over C->C responses]: -0.474\n",
      "max[indegree over C->C mid-thread responses]: -0.518\n",
      "entropy[indegree over C->C responses]: -0.562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- hyperconv-latent: 0.6444 train, 0.5385 test\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[outdegree over C->c mid-thread responses]: 0.699\n",
      "prop-nonzero[outdegree over C->C mid-thread responses]: 0.699\n",
      "norm.max[indegree over C->C responses]: 0.697\n",
      "norm.2nd-largest[indegree over C->C responses]: 0.489\n",
      "max[indegree over c->c mid-thread responses]: 0.478\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest[outdegree over C->c responses]: -0.432\n",
      "2nd-largest[outdegree over C->C responses]: -0.432\n",
      "mean-nonzero[indegree over C->C responses]: -0.432\n",
      "max[indegree over C->C mid-thread responses]: -0.494\n",
      "entropy[indegree over C->C responses]: -0.602\n",
      "\n",
      "- hyperconvo-motifall: 0.6602 train, 0.5249 test\n",
      "TOP 5 FEATURES\n",
      "prop-nonzero[outdegree over C->c mid-thread responses]: 0.754\n",
      "prop-nonzero[outdegree over C->C mid-thread responses]: 0.754\n",
      "norm.max[indegree over C->C responses]: 0.709\n",
      "prop-multiple[indegree over C->C responses]: 0.552\n",
      "norm.2nd-largest[indegree over C->C responses]: 0.532\n",
      "\n",
      "BOTTOM 5 FEATURES\n",
      "2nd-largest[outdegree over C->c responses]: -0.370\n",
      "2nd-largest[outdegree over C->C responses]: -0.370\n",
      "mean-nonzero[indegree over C->C responses]: -0.471\n",
      "max[indegree over C->C mid-thread responses]: -0.518\n",
      "entropy[indegree over C->C responses]: -0.581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(2019)\n",
    "\n",
    "for task in [\"comment-growth\", \"commenter-growth\"]: #, \"post-deleted\", \"user-deleted\"\n",
    "    print(\"TASK: {}\\n\".format(task))\n",
    "    \n",
    "    pos, neg = generate_pos_neg(task, thread_roots_by_self_post, threads, thread_pfxs)\n",
    "\n",
    "    X, y = generate_paired_features(hyperconvo_feats, pos, neg)\n",
    "    X_motifcnt, y_motifcnt = generate_paired_features(motif_counts, pos, neg)\n",
    "    X_latent, y_latent = generate_paired_features(latent_motif_count, pos, neg)\n",
    "    X_path, y_path = generate_paired_features(path_stats, pos, neg)\n",
    "    X_hcmotif, y_hcmotif = generate_paired_features(hyperconv_motif, pos, neg)\n",
    "    X_hcpath, y_hcpath = generate_paired_features(hyperconv_paths, pos, neg)\n",
    "    X_hclatent, y_hclatent = generate_paired_features(hyperconv_latent, pos, neg)\n",
    "    X_all, y_all = generate_paired_features(hyperconv_motifall, pos, neg)\n",
    "    for X, y, feats, name in [(X, y, hyperconvo_feats, \"hyperconv\"),\n",
    "                       (X_motifcnt, y_motifcnt, motif_counts, \"motifcount\"),\n",
    "                       (X_latent, y_latent, latent_motif_count, \"latentmotif\"),\n",
    "                       (X_path, y_path, path_stats, \"motifpaths\"),\n",
    "                       (X_hcmotif, y_hcmotif, hyperconv_motif, \"hyperconv-motif\"),\n",
    "                       (X_hcpath, y_hcpath, hyperconv_paths, \"hyperconv-paths\"),\n",
    "                       (X_hclatent, y_hclatent, hyperconv_latent, \"hyperconv-latent\"),\n",
    "                       (X_all, y_all, hyperconv_motifall, \"hyperconvo-motifall\")\n",
    "                      ]:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_acc = clf.score(X_train, y_train)\n",
    "        test_acc = clf.score(X_test, y_test)\n",
    "        print(\"- {}: {:.4f} train, {:.4f} test\".format(name, train_acc, test_acc))\n",
    "        print_extreme_coefs(clf, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task comment-growth\n",
      "- 1723 positive, 1723 negative pts for comment-growth task\n",
      "- hyperconvo: 0.5917 train, 0.5362 test\n",
      "- reply tree: 0.5688 train, 0.5362 test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-e6848aa056eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mtext_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreads_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(2019)\n",
    "\n",
    "for task in [\"comment-growth\", \"commenter-growth\"]: #, \"post-deleted\", \"user-deleted\"\n",
    "    print(\"task {}\".format(task))\n",
    "    \n",
    "    pos, neg = generate_pos_neg(task, thread_roots_by_self_post, threads, thread_pfxs)\n",
    "\n",
    "    X, y = generate_paired_hyperconv_features(hyperconvo_feats, pos, neg)\n",
    "    # make data from pos and neg\n",
    "    X = []\n",
    "    X_volume, X_reply, X_bow = [], [], []\n",
    "    X_motifpath = []\n",
    "    X_motifcount = []\n",
    "    X_latentmotif = []\n",
    "    threads_text = []\n",
    "    for root in pos + neg:\n",
    "        # get ordered set of feature values\n",
    "        v = [hyperconvo_feats[root][k] for k in sorted(hyperconvo_feats[root].keys())]\n",
    "        # data cleaning\n",
    "        v = [t if (not np.isnan(t) and np.isfinite(t)) else 0 for t in v]\n",
    "        X.append(v)\n",
    "        \n",
    "        X_motifpath.append(list(path_stats[root].values()))\n",
    "        X_motifcount.append(list(motif_counts[root].values()))\n",
    "        X_latentmotif.append(list(latent_motif_count[root].values()))\n",
    "        # volume baseline - get num participants in thread with at least length of 10\n",
    "        X_volume.append([len(set(c.user.name for c in thread_pfxs[root].values()))])   \n",
    "        # reply tree baseline\n",
    "        X_reply.append([hyperconvo_feats[root][k] if (not np.isnan(hyperconvo_feats[root][k]) and np.isfinite(hyperconvo_feats[root][k])) else 0 for k in sorted(hyperconvo_feats[root].keys()) \n",
    "                        if \"c->c\" in k])\n",
    "        # BOW baseline text\n",
    "        thread_text = \" \".join([u.text for u in thread_pfxs[root].values()\n",
    "                                if not (task == \"post-deleted\" and u.id == root)])  \n",
    "        # don't consider root post for post-deleted task, since we could just look for the string \"[deleted]\"\n",
    "        threads_text.append(thread_text)\n",
    "        \n",
    "    ys = [1]*len(pos) + [0]*len(neg)\n",
    "\n",
    "    X, ys = np.array(X), np.array(ys)\n",
    "    X_volume = np.array(X_volume)\n",
    "    X_reply = np.array(X_reply)\n",
    "    X_motifpath = np.array(X_motifpath)\n",
    "    X_motifcount = np.array(X_motifcount)\n",
    "    X_latentmotif = np.array(X_latentmotif)\n",
    "    X_hc_path = np.concatenate([X_motifpath, X], axis=1)\n",
    "    X_hc_count = np.concatenate([X_motifcount, X], axis=1)\n",
    "    X_hc_latent = np.concatenate([X_latentmotif, X], axis=1)\n",
    "    X_all = np.concatenate([X_motifpath, X_motifcount, X_latentmotif, X], axis=1)\n",
    "    for X_tmp, name in [(X, \"hyperconvo\"), \n",
    "#                         (X_volume, \"volume\"), \n",
    "                        (X_reply, \"reply tree\"), \n",
    "                        (None, \"BOW\"), \n",
    "                        (X_motifpath, \"motifpaths\"), \n",
    "                        (X_motifcount, \"motifcounts\"),\n",
    "                        (X_latentmotif, \"latentmotifcounts\"),\n",
    "                        (X_hc_path, \"hyperconv-motifpaths\"),\n",
    "                        (X_hc_count, \"hyperconvo-motifcounts\"),\n",
    "                        (X_hc_latent, \"hyperconvo-latentmotifs\"),\n",
    "                        (X_all, \"hyperconvo-motifall\")\n",
    "                       ]:\n",
    "        if name == \"BOW\":\n",
    "            text_train, text_test, y_train, y_test = train_test_split(threads_text, ys, test_size=0.1, random_state=42)\n",
    "            cv = CountVectorizer(min_df=0.05, max_df=0.8)\n",
    "            X_train = cv.fit_transform(text_train)\n",
    "            X_test = cv.transform(text_test)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_tmp, ys, test_size=0.1, random_state=42)\n",
    "        \n",
    "        clf = Pipeline([(\"normalizer\", Normalizer()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "#         clf = Pipeline([(\"normalizer\", Normalizer()), (\"featselect\", SelectPercentile(f_classif, 10)), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "#         clf = GridSearchCV(base_clf, {\"logreg__C\": [10**i for i in range(-4,4)], \"featselect__percentile\": list(range(10, 110, 10))}, cv=3)\n",
    "\n",
    "#         print(X_train.shape)\n",
    "#         print(y_train.shape)\n",
    "\n",
    "#         clf = LogisticRegression(solver=\"liblinear\")\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_acc = clf.score(X_train, y_train)\n",
    "        test_acc = clf.score(X_test, y_test)\n",
    "        print(\"- {}: {:.4f} train, {:.4f} test\".format(name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
