{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/calebchiam/Documents/GitHub/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'e58slx0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motifs = hc.retrieve_motifs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motif_path_stats = hc.retrieve_motif_pathway_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_paths = hc.retrieve_motif_paths(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import TriadMotif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_motif(motif: TriadMotif):\n",
    "    utts_replied_to = [edge_set[0]['reply_to'] for edge_set in motif.edges]\n",
    "    return max(Counter(utts_replied_to).values()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_id = ('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS')\n",
    "incoming_2to3_id = ('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "pos = []\n",
    "for thread_id, motif_paths in threads_paths.items():\n",
    "    valid_incoming = [motif for motif in motif_paths[incoming_id] if validate_motif(motif)]\n",
    "    valid_2to3 = [motif for motif in motif_paths[incoming_2to3_id] if validate_motif(motif)]\n",
    "\n",
    "    if valid_incoming and valid_2to3:\n",
    "        neg.append(choice(valid_incoming))\n",
    "        pos.append(choice(valid_2to3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "378 / 408 of the pairs satisfy the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bow_feats = dict()\n",
    "neg_bow_feats = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tlc(motif: TriadMotif):\n",
    "    return motif.edges[0][0]['top_level_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos_text = []\n",
    "neg_text = []\n",
    "for motif in pos:\n",
    "    # BOW baseline text\n",
    "    # motif_text taken from first two edges\n",
    "    time_sorted_edges = sorted([e[0] for e in motif.edges], key=lambda x: x['timestamp'])\n",
    "    text1 = \" \".join([\"1_\"+w.strip() for w in time_sorted_edges[0]['text'].split(\" \")])\n",
    "    text2 = \" \".join([\"2_\"+w.strip() for w in time_sorted_edges[1]['text'].split(\" \")])\n",
    "    pos_text.append(text1 + \" \" + text2)\n",
    "    \n",
    "for motif in neg:\n",
    "    # BOW baseline text\n",
    "    # motif_text taken from first two edges\n",
    "    time_sorted_edges = sorted([e[0] for e in motif.edges], key=lambda x: x['timestamp'])\n",
    "    text1 = \" \".join([\"1_\"+w.strip() for w in time_sorted_edges[0]['text'].split(\" \")])\n",
    "    text2 = \" \".join([\"2_\"+w.strip() for w in time_sorted_edges[1]['text'].split(\" \")])\n",
    "    neg_text.append(text1 + \" \" + text2)\n",
    "\n",
    "# pos_train, pos_test, neg_train, neg_test = train_test_split(pos, neg, test_size=0.2, random_state=42)\n",
    "pos_ids, neg_ids = [get_tlc(motif) for motif in pos], [get_tlc(motif) for motif in neg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_id_to_text = {pos_ids[i]: pos_text[i] for i in range(len(pos_ids))}\n",
    "neg_id_to_text = {neg_ids[i]: neg_text[i] for i in range(len(neg_ids))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(list(pos_id_to_text), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = [pos_id_to_text[id] for id in train_ids]\n",
    "neg_train = [neg_id_to_text[id] for id in train_ids]\n",
    "pos_test = [pos_id_to_text[id] for id in test_ids]\n",
    "neg_test = [neg_id_to_text[id] for id in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.8, max_features=None, min_df=0.05,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0.05, max_df=0.8, ngram_range=(1, 3)) # excluding stop_words field improves performance\n",
    "cv.fit(pos_train + neg_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = cv.transform(pos_train + pos_test).toarray()\n",
    "neg_data = cv.transform(neg_train + neg_test).toarray()\n",
    "cols = cv.get_feature_names()\n",
    "pos_df = pd.DataFrame(pos_data, index=train_ids + test_ids, columns=cols)\n",
    "neg_df = pd.DataFrame(neg_data, index=train_ids + test_ids, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 159)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paired_X_y(pos_df, neg_df):\n",
    "    df = pd.DataFrame(columns=pos_df.columns)\n",
    "    y = []\n",
    "    for idx in range(pos_df.shape[0]):\n",
    "        if idx % 2 == 0:\n",
    "            df = df.append(pos_df.iloc[idx] - neg_df.iloc[idx])\n",
    "            y.append(1)\n",
    "        else:\n",
    "            df = df.append(neg_df.iloc[idx] - pos_df.iloc[idx])\n",
    "            y.append(0)\n",
    "    y = pd.DataFrame(y, index=train_ids+test_ids)\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_paired_X_y(pos_df, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import normalize, StandardScaler, Normalizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "X_train = X.loc[train_ids]\n",
    "y_train = y.loc[train_ids]\n",
    "X_test = X.loc[test_ids]\n",
    "y_test = y.loc[test_ids]\n",
    "clf.fit(X.loc[train_ids], y.loc[train_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BOW: 0.8696 train, 0.5333 test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"- BOW: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feats, k):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "    \n",
    "    assert len(feats) == len(coefs)\n",
    "    feats_coefs = sorted(list(zip(feats, coefs)), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"TOP {} FEATURES\".format(k))\n",
    "    for ft, coef in feats_coefs[:k]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(k))\n",
    "    for ft, coef in feats_coefs[-k:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 20 FEATURES\n",
      "1_don: 1.090\n",
      "1_at: 0.920\n",
      "1_of 1_the: 0.904\n",
      "1_he: 0.856\n",
      "1_i: 0.840\n",
      "1_because: 0.742\n",
      "1_the: 0.740\n",
      "2_ gt: 0.714\n",
      "2_with: 0.699\n",
      "2_there: 0.677\n",
      "2_a: 0.667\n",
      "1_your: 0.658\n",
      "1_how: 0.639\n",
      "2_not: 0.609\n",
      "2_to: 0.602\n",
      "2_people: 0.594\n",
      "2_in 2_the: 0.568\n",
      "1_like: 0.564\n",
      "com: 0.559\n",
      "1_than: 0.551\n",
      "\n",
      "BOTTOM 20 FEATURES\n",
      "1_are: -0.527\n",
      "1_people: -0.536\n",
      "1_they: -0.538\n",
      "1_as: -0.568\n",
      "2_about: -0.573\n",
      "1_has: -0.578\n",
      "2_know: -0.582\n",
      "2_the: -0.596\n",
      "2_even: -0.616\n",
      "2_it: -0.694\n",
      "2_in: -0.712\n",
      "2_would: -0.740\n",
      "the: -0.784\n",
      "1_more: -0.847\n",
      "1_of: -0.878\n",
      "ve: -0.881\n",
      "1_in 1_the: -0.987\n",
      "re: -1.122\n",
      "2_first: -1.151\n",
      "1_was: -1.164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, list(cv.get_feature_names()), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time diff between first/second edge, length of first edge text, length of second edge text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_motif(motif_inst):\n",
    "    time_sorted_edges = sorted([e[0] for e in motif_inst.edges], key=lambda x: x['timestamp'])\n",
    "    time_diff = time_sorted_edges[1]['timestamp'] - time_sorted_edges[0]['timestamp']\n",
    "    num_words_1 = len(list(time_sorted_edges[0]['text'].split(\" \")))\n",
    "    num_words_2 = len(list(time_sorted_edges[1]['text'].split(\" \")))\n",
    "    return [time_diff, num_words_1, num_words_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats = {get_tlc(motif): get_features_from_motif(motif) for motif in pos}\n",
    "neg_feats = {get_tlc(motif): get_features_from_motif(motif) for motif in neg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats_df = pd.DataFrame.from_dict(pos_feats).T\n",
    "neg_feats_df = pd.DataFrame.from_dict(neg_feats).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats_df.columns = ['time_diff', 'first_utt_len', 'second_utt_len']\n",
    "neg_feats_df.columns = ['time_diff', 'first_utt_len', 'second_utt_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_paired_X_y(pos_feats_df, neg_feats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = X.loc[train_ids]\n",
    "y_train2 = y.loc[train_ids]\n",
    "X_test2 = X.loc[test_ids]\n",
    "y_test2 = y.loc[test_ids]\n",
    "\n",
    "clf2 = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "clf2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 1)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 159)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Basic features: 0.8696 train, 0.5333 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf2.score(X_train2, y_train2)\n",
    "test_acc = clf2.score(X_test2, y_test2)\n",
    "print(\"- Basic features: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW + Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = pd.concat([X_train, X_train2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "clf3.fit(X_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_combined = pd.concat([X_test, X_test2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Basic features: 0.8696 train, 0.5067 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf3.score(X_train_combined, y_train)\n",
    "test_acc = clf3.score(X_test_combined, y_test)\n",
    "print(\"- Basic features: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
