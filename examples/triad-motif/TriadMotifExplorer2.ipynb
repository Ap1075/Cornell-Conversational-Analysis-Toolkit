{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/caleb/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "changemyview_dir = '/sauna/reddit_201810_raw/corpus/cats3~-~changemyview/changemyview'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../../sauna/reddit_201810_raw/reddit-corpus-2/reddit-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus(filename=changemyview_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'e58slx0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motifs = hc.retrieve_motifs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motif_path_stats = hc.retrieve_motif_pathway_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_paths = hc.retrieve_motif_paths(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import TriadMotif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_motif(motif: TriadMotif):\n",
    "    utts_replied_to = [edge_set[0]['reply_to'] for edge_set in motif.edges]\n",
    "    return max(Counter(utts_replied_to).values()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_id = ('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS')\n",
    "incoming_2to3_id = ('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "pos = []\n",
    "for thread_id, motif_paths in threads_paths.items():\n",
    "    valid_incoming = [motif for motif in motif_paths[incoming_id] if validate_motif(motif)]\n",
    "    valid_2to3 = [motif for motif in motif_paths[incoming_2to3_id] if validate_motif(motif)]\n",
    "\n",
    "    if valid_incoming and valid_2to3:\n",
    "        neg.append(choice(valid_incoming))\n",
    "        pos.append(choice(valid_2to3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "378 / 408 of the pairs satisfy the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bow_feats = dict()\n",
    "neg_bow_feats = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tlc(motif: TriadMotif):\n",
    "    return motif.edges[0][0]['top_level_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos_text = []\n",
    "neg_text = []\n",
    "for motif in pos:\n",
    "    # BOW baseline text\n",
    "    # motif_text taken from first two edges\n",
    "    time_sorted_edges = sorted([e[0] for e in motif.edges], key=lambda x: x['timestamp'])\n",
    "    text1 = \" \".join([\"1_\"+w.strip() for w in time_sorted_edges[0]['text'].split(\" \")])\n",
    "    text2 = \" \".join([\"2_\"+w.strip() for w in time_sorted_edges[1]['text'].split(\" \")])\n",
    "    pos_text.append(text1 + \" \" + text2)\n",
    "    \n",
    "for motif in neg:\n",
    "    # BOW baseline text\n",
    "    # motif_text taken from first two edges\n",
    "    time_sorted_edges = sorted([e[0] for e in motif.edges], key=lambda x: x['timestamp'])\n",
    "    text1 = \" \".join([\"1_\"+w.strip() for w in time_sorted_edges[0]['text'].split(\" \")])\n",
    "    text2 = \" \".join([\"2_\"+w.strip() for w in time_sorted_edges[1]['text'].split(\" \")])\n",
    "    neg_text.append(text1 + \" \" + text2)\n",
    "\n",
    "# pos_train, pos_test, neg_train, neg_test = train_test_split(pos, neg, test_size=0.2, random_state=42)\n",
    "pos_ids, neg_ids = [get_tlc(motif) for motif in pos], [get_tlc(motif) for motif in neg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_id_to_text = {pos_ids[i]: pos_text[i] for i in range(len(pos_ids))}\n",
    "neg_id_to_text = {neg_ids[i]: neg_text[i] for i in range(len(neg_ids))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(list(pos_id_to_text), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = [pos_id_to_text[id] for id in train_ids]\n",
    "neg_train = [neg_id_to_text[id] for id in train_ids]\n",
    "pos_test = [pos_id_to_text[id] for id in test_ids]\n",
    "neg_test = [neg_id_to_text[id] for id in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.8, max_features=None, min_df=0.05,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0.05, max_df=0.8, ngram_range=(1, 3)) # excluding stop_words field improves performance\n",
    "cv.fit(pos_train + neg_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = cv.transform(pos_train + pos_test).toarray()\n",
    "neg_data = cv.transform(neg_train + neg_test).toarray()\n",
    "cols = cv.get_feature_names()\n",
    "pos_df = pd.DataFrame(pos_data, index=train_ids + test_ids, columns=cols)\n",
    "neg_df = pd.DataFrame(neg_data, index=train_ids + test_ids, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1594, 376)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paired_X_y(pos_df, neg_df):\n",
    "    df = pd.DataFrame(columns=pos_df.columns)\n",
    "    y = []\n",
    "    for idx in range(pos_df.shape[0]):\n",
    "        if idx % 2 == 0:\n",
    "            df = df.append(pos_df.iloc[idx] - neg_df.iloc[idx])\n",
    "            y.append(1)\n",
    "        else:\n",
    "            df = df.append(neg_df.iloc[idx] - pos_df.iloc[idx])\n",
    "            y.append(0)\n",
    "    y = pd.DataFrame(y, index=train_ids+test_ids)\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_paired_X_y(pos_df, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import normalize, StandardScaler, Normalizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "X_train = X.loc[train_ids]\n",
    "y_train = y.loc[train_ids]\n",
    "X_test = X.loc[test_ids]\n",
    "y_test = y.loc[test_ids]\n",
    "clf.fit(X.loc[train_ids], y.loc[train_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BOW: 0.7765 train, 0.5423 test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"- BOW: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feats, k):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "    \n",
    "    assert len(feats) == len(coefs)\n",
    "    feats_coefs = sorted(list(zip(feats, coefs)), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"TOP {} FEATURES\".format(k))\n",
    "    for ft, coef in feats_coefs[:k]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(k))\n",
    "    for ft, coef in feats_coefs[-k:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 30 FEATURES\n",
      "http: 1.362\n",
      "1_or: 1.267\n",
      "2_still: 1.168\n",
      "2_have 2_to: 1.042\n",
      "2_be: 1.019\n",
      "2_the 2_same: 0.972\n",
      "2_other: 0.915\n",
      "2_the 2_: 0.914\n",
      "2_that: 0.908\n",
      "re: 0.880\n",
      "1_very: 0.838\n",
      "1_still: 0.833\n",
      "1_ 2_ gt: 0.815\n",
      "2_with: 0.756\n",
      "2_those: 0.747\n",
      "1_i 1_think: 0.744\n",
      "ll: 0.743\n",
      "1_go: 0.740\n",
      "1_at: 0.704\n",
      "2_as 2_a: 0.701\n",
      "1_is 1_that: 0.697\n",
      "1_if: 0.692\n",
      "1_i 1_don: 0.674\n",
      "deleted: 0.665\n",
      "2_so: 0.646\n",
      "2_never: 0.645\n",
      "1_most: 0.627\n",
      "1_many: 0.616\n",
      "2_ deleted: 0.612\n",
      "2_own: 0.598\n",
      "\n",
      "BOTTOM 30 FEATURES\n",
      "2_they: -0.595\n",
      "1_not: -0.612\n",
      "2_you: -0.630\n",
      "1_ 1_: -0.636\n",
      "1_some: -0.645\n",
      "2_get: -0.657\n",
      "2_in 2_the: -0.662\n",
      "2_would: -0.671\n",
      "2_you 2_can: -0.692\n",
      "2_take: -0.697\n",
      "2_it 2_is: -0.705\n",
      "2_you re: -0.745\n",
      "1_more: -0.760\n",
      "2_didn: -0.818\n",
      "1_ 2_: -0.822\n",
      "2_see: -0.850\n",
      "1_been: -0.903\n",
      "and: -0.915\n",
      "2_people: -0.917\n",
      "1_of: -0.939\n",
      "1_think: -0.940\n",
      "2_same: -0.949\n",
      "1_how: -1.011\n",
      "1_then: -1.056\n",
      "1_you: -1.066\n",
      "2_while: -1.104\n",
      "1_to: -1.138\n",
      "com: -1.146\n",
      "1_don: -1.257\n",
      "2_fact: -1.668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, list(cv.get_feature_names()), k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time diff between first/second edge, length of first edge text, length of second edge text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_motif(motif_inst):\n",
    "    time_sorted_edges = sorted([e[0] for e in motif_inst.edges], key=lambda x: x['timestamp'])\n",
    "    time_diff = time_sorted_edges[1]['timestamp'] - time_sorted_edges[0]['timestamp']\n",
    "    num_words_1 = len(list(time_sorted_edges[0]['text'].split(\" \")))\n",
    "    num_words_2 = len(list(time_sorted_edges[1]['text'].split(\" \")))\n",
    "    return [time_diff, num_words_1, num_words_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats = {get_tlc(motif): get_features_from_motif(motif) for motif in pos}\n",
    "neg_feats = {get_tlc(motif): get_features_from_motif(motif) for motif in neg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats_df = pd.DataFrame.from_dict(pos_feats).T\n",
    "neg_feats_df = pd.DataFrame.from_dict(neg_feats).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats_df.columns = ['time_diff', 'first_utt_len', 'second_utt_len']\n",
    "neg_feats_df.columns = ['time_diff', 'first_utt_len', 'second_utt_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_paired_X_y(pos_feats_df, neg_feats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = X.loc[train_ids]\n",
    "y_train2 = y.loc[train_ids]\n",
    "X_test2 = X.loc[test_ids]\n",
    "y_test2 = y.loc[test_ids]\n",
    "\n",
    "clf2 = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "clf2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1275, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1275, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Basic features: 0.4980 train, 0.4953 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf2.score(X_train2, y_train2)\n",
    "test_acc = clf2.score(X_test2, y_test2)\n",
    "print(\"- Basic features: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW + Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = pd.concat([X_train, X_train2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "clf3.fit(X_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_combined = pd.concat([X_test, X_test2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Basic features: 0.7765 train, 0.5423 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf3.score(X_train_combined, y_train)\n",
    "test_acc = clf3.score(X_test_combined, y_test)\n",
    "print(\"- Basic features: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 30 FEATURES\n",
      "http: 1.374\n",
      "1_or: 1.282\n",
      "2_still: 1.206\n",
      "2_have 2_to: 1.060\n",
      "2_other: 0.957\n",
      "2_be: 0.947\n",
      "2_the 2_: 0.909\n",
      "2_the 2_same: 0.891\n",
      "2_that: 0.871\n",
      "1_very: 0.863\n",
      "1_still: 0.848\n",
      "1_ 2_ gt: 0.809\n",
      "re: 0.806\n",
      "2_those: 0.794\n",
      "1_go: 0.785\n",
      "1_i 1_think: 0.741\n",
      "1_at: 0.730\n",
      "1_if: 0.724\n",
      "1_is 1_that: 0.714\n",
      "2_with: 0.714\n",
      "ll: 0.711\n",
      "2_never: 0.670\n",
      "1_i 1_don: 0.669\n",
      "2_as 2_a: 0.662\n",
      "deleted: 0.661\n",
      "1_many: 0.622\n",
      "2_ deleted: 0.621\n",
      "1_most: 0.618\n",
      "2_no: 0.607\n",
      "2_of 2_the: 0.596\n",
      "\n",
      "BOTTOM 30 FEATURES\n",
      "1_where: -0.586\n",
      "2_you: -0.621\n",
      "1_not: -0.637\n",
      "1_some: -0.648\n",
      "2_get: -0.655\n",
      "2_you 2_can: -0.664\n",
      "1_ 1_: -0.682\n",
      "2_would: -0.682\n",
      "2_in 2_the: -0.690\n",
      "2_it 2_is: -0.691\n",
      "2_take: -0.712\n",
      "1_more: -0.724\n",
      "2_you re: -0.741\n",
      "2_didn: -0.833\n",
      "1_ 2_: -0.833\n",
      "2_same: -0.870\n",
      "2_see: -0.879\n",
      "and: -0.890\n",
      "1_been: -0.891\n",
      "1_think: -0.942\n",
      "1_of: -0.959\n",
      "2_people: -0.975\n",
      "com: -0.994\n",
      "1_how: -1.009\n",
      "1_you: -1.037\n",
      "1_then: -1.058\n",
      "2_while: -1.120\n",
      "1_to: -1.159\n",
      "1_don: -1.265\n",
      "2_fact: -1.737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf3, list(X_train_combined.columns), k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
