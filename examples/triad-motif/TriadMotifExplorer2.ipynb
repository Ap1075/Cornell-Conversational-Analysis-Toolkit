{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/caleb/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../../../sauna/reddit_201810_raw/reddit-corpus-2/reddit-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = convokit.Corpus(filename='reddit-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'e58slx0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motifs = hc.retrieve_motifs(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_motif_path_stats = hc.retrieve_motif_pathway_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_paths = hc.retrieve_motif_paths(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import TriadMotif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_motif(motif: TriadMotif):\n",
    "    utts_replied_to = [edge_set[0]['reply_to'] for edge_set in motif.edges]\n",
    "    return max(Counter(utts_replied_to).values()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_id = ('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS')\n",
    "incoming_2to3_id = ('NO_EDGE_TRIADS', 'SINGLE_EDGE_TRIADS', 'INCOMING_TRIADS', 'INCOMING_2TO3_TRIADS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "pos = []\n",
    "for thread_id, motif_paths in threads_paths.items():\n",
    "    valid_incoming = [motif for motif in motif_paths[incoming_id] if validate_motif(motif)]\n",
    "    valid_2to3 = [motif for motif in motif_paths[incoming_2to3_id] if validate_motif(motif)]\n",
    "\n",
    "    if valid_incoming and valid_2to3:\n",
    "        neg.append(choice(valid_incoming))\n",
    "        pos.append(choice(valid_2to3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2692"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2692"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "378 / 408 of the pairs satisfy the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_bow_feats = dict()\n",
    "neg_bow_feats = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tlc(motif: TriadMotif):\n",
    "    return motif.edges[0][0]['top_level_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pos_text = []\n",
    "neg_text = []\n",
    "for motif in pos:\n",
    "    # BOW baseline text\n",
    "    # motif_text taken from first two edges\n",
    "    time_sorted_edges = sorted([e[0] for e in motif.edges], key=lambda x: x['timestamp'])\n",
    "    text1 = \" \".join([\"1_\"+w.strip() for w in time_sorted_edges[0]['text'].split(\" \")])\n",
    "    text2 = \" \".join([\"2_\"+w.strip() for w in time_sorted_edges[1]['text'].split(\" \")])\n",
    "    pos_text.append(text1 + \" \" + text2)\n",
    "    \n",
    "for motif in neg:\n",
    "    # BOW baseline text\n",
    "    # motif_text taken from first two edges\n",
    "    time_sorted_edges = sorted([e[0] for e in motif.edges], key=lambda x: x['timestamp'])\n",
    "    text1 = \" \".join([\"1_\"+w.strip() for w in time_sorted_edges[0]['text'].split(\" \")])\n",
    "    text2 = \" \".join([\"2_\"+w.strip() for w in time_sorted_edges[1]['text'].split(\" \")])\n",
    "    neg_text.append(text1 + \" \" + text2)\n",
    "\n",
    "# pos_train, pos_test, neg_train, neg_test = train_test_split(pos, neg, test_size=0.2, random_state=42)\n",
    "pos_ids, neg_ids = [get_tlc(motif) for motif in pos], [get_tlc(motif) for motif in neg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_id_to_text = {pos_ids[i]: pos_text[i] for i in range(len(pos_ids))}\n",
    "neg_id_to_text = {neg_ids[i]: neg_text[i] for i in range(len(neg_ids))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(list(pos_id_to_text), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = [pos_id_to_text[id] for id in train_ids]\n",
    "neg_train = [neg_id_to_text[id] for id in train_ids]\n",
    "pos_test = [pos_id_to_text[id] for id in test_ids]\n",
    "neg_test = [neg_id_to_text[id] for id in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.8, max_features=None, min_df=0.05,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0.05, max_df=0.8, ngram_range=(1, 3)) # excluding stop_words field improves performance\n",
    "cv.fit(pos_train + neg_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = cv.transform(pos_train + pos_test).toarray()\n",
    "neg_data = cv.transform(neg_train + neg_test).toarray()\n",
    "cols = cv.get_feature_names()\n",
    "pos_df = pd.DataFrame(pos_data, index=train_ids + test_ids, columns=cols)\n",
    "neg_df = pd.DataFrame(neg_data, index=train_ids + test_ids, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2652, 192)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paired_X_y(pos_df, neg_df):\n",
    "    df = pd.DataFrame(columns=pos_df.columns)\n",
    "    y = []\n",
    "    for idx in range(pos_df.shape[0]):\n",
    "        if idx % 2 == 0:\n",
    "            df = df.append(pos_df.iloc[idx] - neg_df.iloc[idx])\n",
    "            y.append(1)\n",
    "        else:\n",
    "            df = df.append(neg_df.iloc[idx] - pos_df.iloc[idx])\n",
    "            y.append(0)\n",
    "    y = pd.DataFrame(y, index=train_ids+test_ids)\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_paired_X_y(pos_df, neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import normalize, StandardScaler, Normalizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "X_train = X.loc[train_ids]\n",
    "y_train = y.loc[train_ids]\n",
    "X_test = X.loc[test_ids]\n",
    "y_test = y.loc[test_ids]\n",
    "clf.fit(X.loc[train_ids], y.loc[train_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BOW: 0.6068 train, 0.5612 test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"- BOW: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extreme_coefs(clf, feats, k):\n",
    "    coefs = clf.named_steps['logreg'].coef_[0].tolist()\n",
    "    \n",
    "    assert len(feats) == len(coefs)\n",
    "    feats_coefs = sorted(list(zip(feats, coefs)), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"TOP {} FEATURES\".format(k))\n",
    "    for ft, coef in feats_coefs[:k]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()\n",
    "    print(\"BOTTOM {} FEATURES\".format(k))\n",
    "    for ft, coef in feats_coefs[-k:]:\n",
    "        print(\"{}: {:.3f}\".format(ft, coef))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 20 FEATURES\n",
      "1_the: 0.337\n",
      "1_ 1_: 0.246\n",
      "2_: 0.194\n",
      "2_why: 0.182\n",
      "1_i: 0.173\n",
      "1_have: 0.166\n",
      "2_do: 0.165\n",
      "2_make: 0.150\n",
      "1_way: 0.144\n",
      "2_all: 0.143\n",
      "1_who: 0.138\n",
      "2_one: 0.120\n",
      "1_really: 0.119\n",
      "1_has: 0.116\n",
      "2_ gt: 0.115\n",
      "2_you: 0.103\n",
      "2_at: 0.102\n",
      "2_being: 0.099\n",
      "2_because: 0.099\n",
      "1_people: 0.097\n",
      "\n",
      "BOTTOM 20 FEATURES\n",
      "1_my: -0.072\n",
      "1_than: -0.072\n",
      "2_not: -0.078\n",
      "com: -0.088\n",
      "2_most: -0.089\n",
      "1_on 1_the: -0.089\n",
      "1_they: -0.092\n",
      "1_most: -0.094\n",
      "2_if 2_you: -0.095\n",
      "2_or: -0.097\n",
      "2_of: -0.100\n",
      "2_to: -0.104\n",
      "2_he: -0.112\n",
      "2_really: -0.117\n",
      "1_if: -0.122\n",
      "2_like: -0.128\n",
      "1_and: -0.135\n",
      "1_he: -0.137\n",
      "1_to: -0.142\n",
      "1_of: -0.164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_extreme_coefs(clf, list(cv.get_feature_names()), k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time diff between first/second edge, length of first edge text, length of second edge text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_motif(motif_inst):\n",
    "    time_sorted_edges = sorted([e[0] for e in motif_inst.edges], key=lambda x: x['timestamp'])\n",
    "    time_diff = time_sorted_edges[1]['timestamp'] - time_sorted_edges[0]['timestamp']\n",
    "    num_words_1 = len(list(time_sorted_edges[0]['text'].split(\" \")))\n",
    "    num_words_2 = len(list(time_sorted_edges[1]['text'].split(\" \")))\n",
    "    return [time_diff, num_words_1, num_words_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats = {get_tlc(motif): get_features_from_motif(motif) for motif in pos}\n",
    "neg_feats = {get_tlc(motif): get_features_from_motif(motif) for motif in neg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats_df = pd.DataFrame.from_dict(pos_feats).T\n",
    "neg_feats_df = pd.DataFrame.from_dict(neg_feats).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_feats_df.columns = ['time_diff', 'first_utt_len', 'second_utt_len']\n",
    "neg_feats_df.columns = ['time_diff', 'first_utt_len', 'second_utt_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_paired_X_y(pos_feats_df, neg_feats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = X.loc[train_ids]\n",
    "y_train2 = y.loc[train_ids]\n",
    "X_test2 = X.loc[test_ids]\n",
    "y_test2 = y.loc[test_ids]\n",
    "\n",
    "clf2 = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "clf2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2121, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2121, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Basic features: 0.5158 train, 0.4896 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf2.score(X_train2, y_train2)\n",
    "test_acc = clf2.score(X_test2, y_test2)\n",
    "print(\"- Basic features: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW + Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = pd.concat([X_train, X_train2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardScaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = Pipeline([(\"standardScaler\", StandardScaler()), (\"logreg\", LogisticRegression(solver='liblinear'))])\n",
    "clf3.fit(X_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_combined = pd.concat([X_test, X_test2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Basic features: 0.6096 train, 0.5612 test\n"
     ]
    }
   ],
   "source": [
    "train_acc = clf3.score(X_train_combined, y_train)\n",
    "test_acc = clf3.score(X_test_combined, y_test)\n",
    "print(\"- Basic features: {:.4f} train, {:.4f} test\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
