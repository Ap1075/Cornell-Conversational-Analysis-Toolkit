{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples of how to call the hyperconvo functions from Convokit and perform analyses similar to the ones presented in the corresponding paper (http://www.cs.cornell.edu/~cristian/Patterns_of_participant_interactions.html), which describes the hypergraph methodology for modeling and analyzing online public discussions.\n",
    "\n",
    "Note that due to limitations in what data we can access and distribute, the beta version of the hypergraph methodology presented here currently supports less functionality:\n",
    "\n",
    "* Instead of Facebook data, we release a dataset of discussions from Reddit (the 1000 most recent threads, with at least 10 comments, from 100 highly active subreddits -- more details in the corresponding readme here: https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/tree/master/datasets/reddit-corpus). For demonstration purposes, we here use a small subset of this dataset (\"reddit-corpus-small\") containing 100 threads per subreddit.\n",
    "* Since Reddit does not provide information on who reacted to (i.e., upvoted) each comment, we are presently unable to support reaction-edges in the hypergraph. As such, we can only compute features over in/outdegree distributions of _reply_-edges, and the feature set derived from motifs in the hypergraph is limited to counts and binary indicators or whether or not a motif is present. We will work  towards supporting reactions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/calebchiam/Documents/GitHub/Cornell-Conversational-Analysis-Toolkit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we download the reddit corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/calebchiam/.convokit/downloads/reddit-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = convokit.Corpus(filename=convokit.download(\"reddit-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = convokit.HyperConvo(prefix_len=10, min_thread_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_path_stats = hc.retrieve_motif_pathway_stats(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.DataFrame(data=0, index=motif_paths.keys(), columns=convokit.TriadMotif.paths_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thread_id, motif_path_stats_dict in motif_path_stats.items():\n",
    "    for motif_path, val in motif_path_stats_dict.items():\n",
    "        feat_df.loc[thread_id][motif_path] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, Normalizer, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_mtx = Imputer(axis=1, missing_values=-1).fit_transform(feat_df.values)\n",
    "feat_mtx = StandardScaler().fit_transform(feat_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=7, algorithm='arpack') # deals with an issue where the randomized alg hangs\n",
    "svd.fit(feat_mtx)\n",
    "U, s, V = svd.transform(feat_mtx) / svd.singular_values_, \\\n",
    "        svd.singular_values_, \\\n",
    "        svd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_norm = Normalizer().fit_transform(U)\n",
    "V_norm = Normalizer().fit_transform(V)\n",
    "U_df = pd.DataFrame(data=U_norm, index=feat_df.index)\n",
    "V_df = pd.DataFrame(data=V_norm, index=convokit.TriadMotif.paths_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the paper, for further interpretability we can consider embeddings of _communities_ (subreddits, standing for Facebook pages) in terms of the discussions they foster, by averaging the embeddings of all threads in a particular subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = corpus.utterance_threads(prefix_len=10, include_root=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(threads, thread_id):\n",
    "    return threads[thread_id][thread_id].meta[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = [get_subreddit(threads, thread_id) for thread_id in threads]\n",
    "U_df['subreddit'] = subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_means = U_df.groupby('subreddit').mean()\n",
    "subreddit_df = pd.DataFrame(\n",
    "        data=Normalizer().fit_transform(subreddit_means.values),\n",
    "        index = subreddit_means.index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's give a rough overview of the space we've sketched out through this procedure, by visualizing the subreddit embeddings using the TSNE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=2018)\n",
    "tsne_df = pd.DataFrame(data=tsne.fit_transform(subreddit_df.values),\n",
    "                      index=subreddit_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(tsne_df[0].values, tsne_df[1].values)\n",
    "for i, txt in enumerate(tsne_df.index):\n",
    "    plt.annotate(txt, (tsne_df.values[i,0], tsne_df.values[i,1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking across this plot, we can spot a few interpretable-looking groupings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = [\"battlefield3\", \"Diablo\", \"DotA2\", \"Guildwars2\", \"leagueoflegends\", \"Minecraft\", \"pokemon\", \"skyrim\", \"starcraft\", \"tf2\", \"wow\"]\n",
    "sports = [\"baseball\", \"CFB\", \"hockey\", \"MMA\", \"nba\", \"nfl\", \"soccer\"]\n",
    "tech = [\"Android\", \"apple\", \"technology\", \"techsupport\", \"programming\", \"buildapc\"]\n",
    "reaction_bait = [\"WTF\", \"pics\", \"gifs\", \"aww\", \"funny\", \"todayilearned\",\n",
    "                   \"AdviceAnimals\"]\n",
    "relationships = [\"AskMen\", \"AskWomen\", \"relationships\", \"relationship_advice\", \"OkCupid\"]\n",
    "countries = [\"singapore\", \"canada\", \"unitedkingdom\", \"australia\"]\n",
    "plt.scatter(tsne_df[0].values, tsne_df[1].values, color=\"#dddddd\")\n",
    "plt.scatter(tsne_df[0].values, tsne_df[1].values, color=[\n",
    "    \"green\" if l in games else\n",
    "    \"gold\" if l in tech else\n",
    "    \"purple\" if l in relationships else\n",
    "    \"red\" if l in reaction_bait else\n",
    "    \"blue\" if l in sports else\n",
    "    \"pink\" if l in countries else\n",
    "    \"#00000000\"\n",
    "    for l in tsne_df.index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, video games (e.g., tf2, DotA2; red) tend to group together, along with buy-sell subreddits (e.g., tf2trade, Dota2Trade, yellow), subreddits related to relationships (e.g., AskWomen, relationship_advice; green) and large default-subreddit hubs for sharing random links (e.g., pics, AskReddit; purple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be other interesting groupings that you may spot as well -- for instance, a vaguely right-wing MensRights cluster (with Libertarian, guns); a rather intriguing cluster consisting of politics, sex, business, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some topical groups are more diffuse -- for instance, sports-based subreddits (blue). Perhaps this is a limitation of our representation, or that these subreddits actually foster very different interactional dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to delve into these groupings is to look at nearest neighbors of subreddits, in terms of the embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dists = pairwise_distances(subreddit_df.values, metric='cosine')\n",
    "flat_dists = np.ravel(dists)\n",
    "idx1, idx2 = np.unravel_index(np.arange(len(flat_dists)), dists.shape)\n",
    "pairwise_dist_df = pd.DataFrame.from_dict({'p1': subreddit_df.index[idx1],\n",
    "                                           'p2': subreddit_df.index[idx2],\n",
    "                                           'dist': flat_dists},\n",
    "                                           orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nearest_neighbors(pairwise_dist_df, \n",
    "                           test_subreddits=[],\n",
    "                           top_N=10):\n",
    "    for subreddit in test_subreddits:\n",
    "        subset_df = pairwise_dist_df[(pairwise_dist_df.p1 == subreddit)\n",
    "                                 & (pairwise_dist_df.p2 != subreddit)]\n",
    "        print(subreddit)\n",
    "        print(subset_df.sort_values('dist')[['p2', 'dist']].head(top_N))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_nearest_neighbors(pairwise_dist_df, ['apple', 'politics', 'leagueoflegends',\n",
    "                                          'AskWomen', 'Music', 'pics',\n",
    "                                          'australia', 'Random_Acts_Of_Amazon',\n",
    "                                          'Bitcoin', 'MensRights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to interpret each dimension of the embedding -- roughly speaking the threads, features and subreddits with extremal values along one dimension could be seen as characterizing a particular \"type\" of discussion, in terms of the discussion structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dimension(dim, ascending=True, n=5):\n",
    "    top_threads = U_df.sort_values(dim, ascending=ascending).head(n)\n",
    "    display(top_threads)\n",
    "    display(V_df.sort_values(dim, ascending=ascending).head(n))\n",
    "    display(subreddit_df.sort_values(dim, ascending=ascending).head(n))\n",
    "    return top_threads.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For instance, the first latent dimension divides the space of Reddit discussions between focused dialogues involving 2 people who repeatedly interact, and \"expansionary\" threads involving multiple people who generally only engage once (as with the corresponding dimension discussed in the paper, this echoes the contrast explored in papers such as Backstrom et. al, 2013). At the subreddit level, we see a divide between subreddits that are selling things (perhaps the dialogues consist of a buyer and a seller) and large default link-sharing subreddits like AskReddit and pics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_threads = display_dimension(0, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
