{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring prompt types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demos two transformers, which broadly aim at producing abstract representations of an utterance in terms of its phrasing and its rhetorical intent: \n",
    "\n",
    "* The `PhrasingMotifs` transformer extracts representations of utterances in terms of how they are phrased;\n",
    "* The `PromptTypes` transformer computes latent representations of utterances in terms of their rhetorical intention -- the _responses_ they aim at prompting -- and assigns utterances to different (automatically-inferred) types of intentions.\n",
    "\n",
    "It also demos some additional transformers used in preprocessing steps.\n",
    "\n",
    "\n",
    "\n",
    "Together, these transformers implement the methodology detailed in the [paper](http://www.cs.cornell.edu/~cristian/Asking_too_much.html), \n",
    "\n",
    "```\n",
    "Asking Too Much? The Rhetorical Role of Questions in Political Discourse \n",
    "Justine Zhang, Arthur Spirling, Cristian Danescu-Niculescu-Mizil\n",
    "Proceedings of EMNLP 2017\n",
    "```\n",
    "\n",
    "ConvoKit also includes an end-to-end implementation, `PromptTypesWrapper`, that runs the transformers one after another, and handles the particular pre-processing steps found in the paper. See [this notebook](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/prompt-types/prompt-type-wrapper-demo.ipynb) for a demonstration of this end-to-end transformer.\n",
    "\n",
    "This is a really clear example of a method which reflects both good (we think) ideas and somewhat ad-hoc implementation decisions. As such, there are lots of options and potential variations to consider (beyond the deeper question of what phrasings and intentions even are) -- I'll detail these as I go along.\n",
    "\n",
    "Note that due to small methodological tweaks and changes in the random seed, the particular output of the transformers as presently implemented may not totally match the output from the paper, but the broad types of questions returned are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the corpus. We will examine a dataset of questions from question periods that take place in the British House of Commons (also detailed in the paper). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit import download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the corpus, plus some pre-computed dependency parses (see [this notebook](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/text-processing/text_preprocessing_demo.ipynb) for a demonstration of how to get these parses on your own; for this dataset they should be included with our release)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTION 1: DOWNLOAD CORPUS \n",
    "# UNCOMMENT THESE LINES TO DOWNLOAD CORPUS\n",
    "# DATA_DIR = '<YOUR DIRECTORY>'\n",
    "# ROOT_DIR = download('parliament-corpus', data_dir=DATA_DIR)\n",
    "\n",
    "# OPTION 2: READ PREVIOUSLY-DOWNLOADED CORPUS FROM DISK\n",
    "# UNCOMMENT THIS LINE AND REPLACE WITH THE DIRECTORY WHERE THE PARLIAMENT-CORPUS IS LOCATED\n",
    "# ROOT_DIR = '<YOUR DIRECTORY>'\n",
    "\n",
    "corpus = convokit.Corpus(ROOT_DIR)\n",
    "corpus.load_info('utterance',['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kitchen/convokit_corpora/parliament-corpus/'\n",
    "corpus = convokit.Corpus(ROOT_DIR)\n",
    "corpus.load_info('utterance',['parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VERBOSITY = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our specific goal, which we'll use ConvoKit to accomplish, is to produce an abstract representation of questions asked by members of parliament, in terms of:\n",
    "\n",
    "* how they are phrased: what phrasing, or lexico-syntatic \"motif\", does a question have? \n",
    "* their rhetorical intention: what's the intent of the asker -- which we take to mean the response the asker aims to prompt? \n",
    "\n",
    "In other words, what are the different types of questions people ask in parliament?\n",
    "\n",
    "Here's an example of an utterance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_utt_id = '1997-01-27a.4.0'\n",
    "utt = corpus.get_utterance(test_utt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To state our goals more precisely:\n",
    "\n",
    "* For each _sentence_ that has a question (all but the last), we want to come up with a representation of the sentence's phrasing. Intuitively, for instance, the first two sentences sound like they could both be thought of as a \"Does X agree that Y?\" -- whether Y is asking about a yacht or a harbour. \n",
    "* For each utterance, we want to come up with a representation of the utterance's rhetorical intent. Intuitively, all the questions could be construed as asking if the answerer is in agreement with the asker -- whether they \"agree\" with the opinion or \"share\" the opinion. We might think of this as being an example of an \"agreeing\" type of question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, if we want to get at this higher level of abstraction, we have to look beyond the particular n-grams: it doesn't seem plausible that there is a meaningful type of question about yachts (unless our specific context is the parliamentary subcommittee on yachts). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing step: Arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One place to start is to look at the structural \"skeleton\" of sentences -- i.e., its dependency parse. Thus, we are first going to provide a representation of questions in terms of their dependency parse by extracting all the parent-to-child token edges, or \"arcs\". We will use the `TextToArcs` class to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.text_processing import TextToArcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_arcs` is a transformer (actually a `TextProcessor`) that will read the dependency parse of an utterance and write the resultant arcs to a field called `'arcs'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/433787 utterances processed\n",
      "20000/433787 utterances processed\n",
      "30000/433787 utterances processed\n",
      "40000/433787 utterances processed\n",
      "50000/433787 utterances processed\n",
      "60000/433787 utterances processed\n",
      "70000/433787 utterances processed\n",
      "80000/433787 utterances processed\n",
      "90000/433787 utterances processed\n",
      "100000/433787 utterances processed\n",
      "110000/433787 utterances processed\n",
      "120000/433787 utterances processed\n",
      "130000/433787 utterances processed\n",
      "140000/433787 utterances processed\n",
      "150000/433787 utterances processed\n",
      "160000/433787 utterances processed\n",
      "170000/433787 utterances processed\n",
      "180000/433787 utterances processed\n",
      "190000/433787 utterances processed\n",
      "200000/433787 utterances processed\n",
      "210000/433787 utterances processed\n",
      "220000/433787 utterances processed\n",
      "230000/433787 utterances processed\n",
      "240000/433787 utterances processed\n",
      "250000/433787 utterances processed\n",
      "260000/433787 utterances processed\n",
      "270000/433787 utterances processed\n",
      "280000/433787 utterances processed\n",
      "290000/433787 utterances processed\n",
      "300000/433787 utterances processed\n",
      "310000/433787 utterances processed\n",
      "320000/433787 utterances processed\n",
      "330000/433787 utterances processed\n",
      "340000/433787 utterances processed\n",
      "350000/433787 utterances processed\n",
      "360000/433787 utterances processed\n",
      "370000/433787 utterances processed\n",
      "380000/433787 utterances processed\n",
      "390000/433787 utterances processed\n",
      "400000/433787 utterances processed\n",
      "410000/433787 utterances processed\n",
      "420000/433787 utterances processed\n",
      "430000/433787 utterances processed\n",
      "433787/433787 utterances processed\n"
     ]
    }
   ],
   "source": [
    "get_arcs = TextToArcs('arcs', verbosity=VERBOSITY)\n",
    "corpus = get_arcs.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'arcs'` is a list where each element corresponds to a sentence in the utterance. Each sentence is represented in terms of its arcs, in a space-separated string. \n",
    "\n",
    "Each arc, in turn, can be read as follows:\n",
    "\n",
    "* `x_y` means that `x` is the parent and `y` is the child token (e.g., `agree_does` = `agree --> does`)\n",
    "* `x_*` means that `x` is a token with at least one descendant, which we do not resolve (this is roughly like bigrams backing off to unigrams)\n",
    "* `x>y` means that `x` and `y` are the first two tokens in the sentence (the decision here was that how the sentence starts is a signal of \"phrasing structure\" on par with the dependency tree structure)\n",
    "* `x>*` means that `x` is the first token in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s_* a_* about_* about_yacht agree_* agree_does agree_hon agree_welcomed been_* does>* does>my does_* friend_* has_* hon_* hon_friend hon_my hon_right last_* my_* replacement_* right_* royal_* statement_* statement_about statement_week that_* week_'s week_* week_last welcomed_* welcomed_been welcomed_has welcomed_statement welcomed_that welcomed_widely widely_* yacht_* yacht_a yacht_replacement yacht_royal\",\n",
       " 'agree_* agree_also agree_become agree_does agree_he also_* become_* become_britannia become_centrepiece become_ideally become_should become_spanning become_that britannia_* centrepiece_* centrepiece_of centrepiece_the does>* does>he does_* gosport_* harbour_* harbour_portsmouth he_* ideally_* in_* in_harbour millennium_* of_* of_project portsmouth_* project_* project_in project_millennium project_the should_* spanning_* spanning_gosport that_* the_*',\n",
       " 'am_* am_i am_sure i>* i_* idea_* idea_that popular_* popular_very prove_* prove_idea prove_popular prove_that prove_would sure_* sure_prove that_* very_* would_*',\n",
       " \"'s_* a_* as>* as>to as_* distaste_* distaste_for distaste_my does_* does_plans for_* for_tactics for_yacht friend_* hon_* hon_friend hon_my hon_right my_* new_* opposition_'s opposition_* opposition_the plans_* plans_as plans_for plans_to right_* share_* share_distaste share_does share_hon tactics_* tactics_opposition the_* to_* yacht_* yacht_a yacht_new\",\n",
       " 'attitude_* attitude_grudging attitude_their discussion_* during_* during_years every_* express_* express_attitude express_during express_to grudging_* had_* had_opportunity had_they negative_* opportunity_* opportunity_every opportunity_express past_* project_* project_the the_* their_* they>* they_* to_* two_* under_* under_discussion was_* was_project was_under was_when when_* years_* years_past years_the years_two years_was']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('arcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further preprocessing: cleaned-up arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, while we've got the methodology to start making sense of the dependency tree, we arguably haven't progressed beyond producing fancy bigram representations of sentences. One problem is perhaps that the default arc extraction is a bit too permissive -- it gives us _all_ of the arcs. We might not want this for a few reasons:\n",
    "\n",
    "* We only want to learn about question phrasings; we don't actually care about non-question sentences.\n",
    "* The structure of a question might be best encapsulated by the arcs that go out of the _root_ of the tree; as you get further down we might end up with less structural and more content-specific representations.\n",
    "* Likewise, the particular _nouns_ used (e.g., `yacht`) might not be good descriptions of the more abstract phrasing pattern.\n",
    "\n",
    "All of these points are debatable, and the resultant modules I'll show below hopefully allow you to play around with them. Taking these point as is for now, though, we'll do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.phrasing_motifs import CensorNouns, QuestionSentences\n",
    "from convokit.convokitPipeline import ConvokitPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will actually create a pipeline to extract the arcs we want. This pipeline has the following components, in order:\n",
    "\n",
    "* `CensorNouns`: a transformer that removes all the nouns and pronouns from a dependency parse. This transformer also collapses constructions like `What time [is it]` into `What [is it]`.\n",
    "* `TextToArcs`: calling the arc extractor from above with an extra parameter: `root_only=True` which will only extract arcs attached to the root (in addition to the first two tokens, though this is also tunable by passing in parameter `use_start=True`).\n",
    "* `QuestionSentences`: a transformer that, given utterance fields consisting of a list of sentences, removes all the sentences which contain question marks. Here, we pass an extra parameter `input_filter=question_filter`, telling it to ignore utterances which aren't listed in the Corpus as questions (i.e., if a player asks a question, we'll discount this, since it's not labeled in the Corpus as a reporter question). \n",
    "    * (you may wonder how this transformer can tell whether a sentence has a question mark in it, given that the output of `TextToArcs` doesn't have any punctuation. Under the hood, `QuestionSentences` looks at the dependency parse of the sentence and checks whether the last token is a question.)\n",
    "    * `QuestionSentences` also omits any sentences which don't begin in capital letters. To turn this off, pass parameter `use_caps=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_filter(utt, aux_input={}):\n",
    "    return utt.meta['is_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_arc_pipe = ConvokitPipeline([\n",
    "    ('censor_nouns', CensorNouns('parsed_censored', verbosity=VERBOSITY)),\n",
    "    ('shallow_arcs', TextToArcs('arcs_censored', input_field='parsed_censored', \n",
    "                               root_only=True, verbosity=VERBOSITY)),\n",
    "    ('question_sentence_filter', QuestionSentences('question_arcs', input_field='arcs_censored',\n",
    "                                         input_filter=question_filter, verbosity=VERBOSITY))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = q_arc_pipe.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline results in a more minimalistic representation of utterances, in terms of just the arcs at the root of dependency trees, just the questions, and no nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('question_arcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_utt_id_1 = '2015-06-09c.1041.5'\n",
    "utt1 = corpus.get_utterance(test_utt_id_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1.get_info('question_arcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrasing motifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to arrive at our representation of phrasings, we can go one further level of abstraction. In short, some of these arcs feel less fully-specified than others. While `agree_does` sounds like it hints at a coherent question, `doing_is` seems like it's not meaningful until you consider that it occurs in the same sentence as `doing_ensure` (i.e., \"_what is the Government doing to ensure...?_\")\n",
    "\n",
    "Our intuition is to think of phrasings as frequently-cooccurring sets of multiple arcs. To extract these frequent arc-sets (which may remind you of the data mining idea of extracting frequent itemsets) we will use the `PhrasingMotifs` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.phrasing_motifs import PhrasingMotifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm_model = PhrasingMotifs('motifs','question_arcs',min_support=100,fit_filter=question_filter,\n",
    "                          verbosity=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `pm_model` will:\n",
    "\n",
    "* extract all sets of arcs, as read from the `question_arcs` field, which occur at least 50 times in a corpus. These frequently-occurring arc sets will constitute the set, or \"vocabulary\", of phrasings.\n",
    "* write the resultant output -- the phrasings that an utterance contains -- to a field called `question_motifs`. \n",
    "\n",
    "On the latter point, `pm_model` will only transform (i.e., label phrasings for) utterances which are questions, i.e., `question_filter(utterance) = True`. That is, in both the train and transform steps, we totally ignore non-questions.\n",
    "\n",
    "Note that the phrasings learned by `pm_model` are therefore _corpus-specific_ -- different corpora may have different frequently-occurring sets, resulting in different vocabularies of phrasings. For instance, you wouldn't expect people in the British House of Commons to ask questions that sound like questions asked to tennis players. In this respect, think of `PhrasingMotifs` like models from scikit learn (e.g., `LogisticRegression`) -- it is fit to a particular dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_model.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most common phrasings and how often they occur in the data (in # of sentences). Note that `('*',)` denotes the null phrasing -- i.e., it encapsulates sentences with _any_ root word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_model.print_top_phrasings(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having \"trained\", or fitted our model, we can then use it to annotate each (question) utterance in the corpus with the phrasings this utterance contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pm_model.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that each sentence can and probably will have multiple phrasings it embodies. For instance, two sentences with phrasing `agree_do` and `agree_will` will also have phrasing `agree_*`. Intuitively, more finely-specified phrasings (i.e., `agree_does`) more closely specify the phrasing embodied by a sentence (we could imagine \"Do you agree...\" and \"Will you agree...\" being very different, but perhaps also more similar to each other than \"Can you explain..\"). \n",
    "\n",
    "We want to keep track of both the complete set of phrasings and the most finely-specified phrasing you can have for each utterance. Therefore, `PhrasingMotifs` actually annotates utterances with _two_ fields.\n",
    "\n",
    "`motifs` lists all the phrasings (arcs in a phrasing motif are separated by two underscores, `'__'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('motifs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `motifs__sink` lists the most finely specified _sink phrasings_ (they are \"sinks\" in the sense that if you think of phrasings as a directed graph where A-->B when B is a more finely-specified version of A, these sinks have no child phrasings which are contained in the utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('motifs__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save a subset of our output to disk -- the filtered arcs, and the motifs, potentially for use in a later transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.dump_info('utterance', ['motifs', 'motifs__sink', 'arcs_censored'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save `pm_model` to disk and later reload it, thus caching the trained model (i.e., the motifs in a corpus and the internal representation of these motifs). Here, we save the model to a `pm_model` subfolder in the corpus directory via `dump_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing itemset counts\n",
      "writing downlinks\n",
      "writing itemset to ids\n",
      "writing meta information\n"
     ]
    }
   ],
   "source": [
    "pm_model.dump_model(os.path.join(ROOT_DIR, 'pm_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subfolder then stores the motifs, as well as relations between the motifs that facilitate transforming new utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downlinks.json\titemset_counts.json  itemset_to_ids.json  meta.json\r\n"
     ]
    }
   ],
   "source": [
    "pm_model_dir = os.path.join(ROOT_DIR, 'pm_model')\n",
    "!ls $pm_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we later initialize a new `PhrasingMotifs` model, `new_pm_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pm_model = PhrasingMotifs('motifs_new','question_arcs',min_support=100,fit_filter=question_filter,\n",
    "                          verbosity=VERBOSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `load_model()` then reloads the stored model from our earlier run into this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading itemset counts\n",
      "reading downlinks\n",
      "reading itemset to ids\n",
      "reading meta information\n"
     ]
    }
   ],
   "source": [
    "new_pm_model.load_model(os.path.join(ROOT_DIR, 'pm_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check that we've loaded the same thing that we previously saved, we'll get the motifs in our test utterance using `new_pm_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt = new_pm_model.transform_utterance(utt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output from the original run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agree_*__does>*', 'agree_*__agree_also', 'as>* share_*__share_does']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('motifs__sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see the new output matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agree_*__does>*', 'agree_*__agree_also', 'as>* share_*__share_does']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('motifs_new__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example variation: not removing the nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note** this takes a while to run, and is somewhat of an extension -- you can safely skip these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to use `PhrasingMotifs` that might be more or less suited to your own application. For instance, you may wonder what happens if we do not remove the nouns (as we did with `CensorNouns` above). To try this out, we can create an alternate pipeline that uses `TextToArcs` to generate root arcs (setting argument `root_only=True`) on the original parses, not the noun-censored ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/433787 utterances processed\n",
      "20000/433787 utterances processed\n",
      "30000/433787 utterances processed\n",
      "40000/433787 utterances processed\n",
      "50000/433787 utterances processed\n",
      "60000/433787 utterances processed\n",
      "70000/433787 utterances processed\n",
      "80000/433787 utterances processed\n",
      "90000/433787 utterances processed\n",
      "100000/433787 utterances processed\n",
      "110000/433787 utterances processed\n",
      "120000/433787 utterances processed\n",
      "130000/433787 utterances processed\n",
      "140000/433787 utterances processed\n",
      "150000/433787 utterances processed\n",
      "160000/433787 utterances processed\n",
      "170000/433787 utterances processed\n",
      "180000/433787 utterances processed\n",
      "190000/433787 utterances processed\n",
      "200000/433787 utterances processed\n",
      "210000/433787 utterances processed\n",
      "220000/433787 utterances processed\n",
      "230000/433787 utterances processed\n",
      "240000/433787 utterances processed\n",
      "250000/433787 utterances processed\n",
      "260000/433787 utterances processed\n",
      "270000/433787 utterances processed\n",
      "280000/433787 utterances processed\n",
      "290000/433787 utterances processed\n",
      "300000/433787 utterances processed\n",
      "310000/433787 utterances processed\n",
      "320000/433787 utterances processed\n",
      "330000/433787 utterances processed\n",
      "340000/433787 utterances processed\n",
      "350000/433787 utterances processed\n",
      "360000/433787 utterances processed\n",
      "370000/433787 utterances processed\n",
      "380000/433787 utterances processed\n",
      "390000/433787 utterances processed\n",
      "400000/433787 utterances processed\n",
      "410000/433787 utterances processed\n",
      "420000/433787 utterances processed\n",
      "430000/433787 utterances processed\n",
      "433787/433787 utterances processed\n",
      "10000/433787 utterances processed\n",
      "20000/433787 utterances processed\n",
      "30000/433787 utterances processed\n",
      "40000/433787 utterances processed\n",
      "50000/433787 utterances processed\n",
      "60000/433787 utterances processed\n",
      "70000/433787 utterances processed\n",
      "80000/433787 utterances processed\n",
      "90000/433787 utterances processed\n",
      "100000/433787 utterances processed\n",
      "110000/433787 utterances processed\n",
      "120000/433787 utterances processed\n",
      "130000/433787 utterances processed\n",
      "140000/433787 utterances processed\n",
      "150000/433787 utterances processed\n",
      "160000/433787 utterances processed\n",
      "170000/433787 utterances processed\n",
      "180000/433787 utterances processed\n",
      "190000/433787 utterances processed\n",
      "200000/433787 utterances processed\n",
      "210000/433787 utterances processed\n",
      "220000/433787 utterances processed\n",
      "230000/433787 utterances processed\n",
      "240000/433787 utterances processed\n",
      "250000/433787 utterances processed\n",
      "260000/433787 utterances processed\n",
      "270000/433787 utterances processed\n",
      "280000/433787 utterances processed\n",
      "290000/433787 utterances processed\n",
      "300000/433787 utterances processed\n",
      "310000/433787 utterances processed\n",
      "320000/433787 utterances processed\n",
      "330000/433787 utterances processed\n",
      "340000/433787 utterances processed\n",
      "350000/433787 utterances processed\n",
      "360000/433787 utterances processed\n",
      "370000/433787 utterances processed\n",
      "380000/433787 utterances processed\n",
      "390000/433787 utterances processed\n",
      "400000/433787 utterances processed\n",
      "410000/433787 utterances processed\n",
      "420000/433787 utterances processed\n",
      "430000/433787 utterances processed\n",
      "433787/433787 utterances processed\n"
     ]
    }
   ],
   "source": [
    "q_arc_pipe_full = ConvokitPipeline([\n",
    "    ('shallow_arcs_full', TextToArcs('root_arcs', input_field='parsed', \n",
    "                               root_only=True, verbosity=VERBOSITY)),\n",
    "    ('question_sentence_filter', QuestionSentences('question_arcs_full', input_field='root_arcs',\n",
    "                                         input_filter=question_filter, verbosity=VERBOSITY)),\n",
    "\n",
    "])\n",
    "corpus = q_arc_pipe_full.transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a new `PhrasingMotifs` model that finds phrasings with the nouns still included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting frequent itemsets for 325339 sets\n",
      "\tfirst pass: counting itemsets up to and including 5 items large\n",
      "\tfirst pass: 10000/325339 sets processed\n",
      "\tfirst pass: 20000/325339 sets processed\n",
      "\tfirst pass: 30000/325339 sets processed\n",
      "\tfirst pass: 40000/325339 sets processed\n",
      "\tfirst pass: 50000/325339 sets processed\n",
      "\tfirst pass: 60000/325339 sets processed\n",
      "\tfirst pass: 70000/325339 sets processed\n",
      "\tfirst pass: 80000/325339 sets processed\n",
      "\tfirst pass: 90000/325339 sets processed\n",
      "\tfirst pass: 100000/325339 sets processed\n",
      "\tfirst pass: 110000/325339 sets processed\n",
      "\tfirst pass: 120000/325339 sets processed\n",
      "\tfirst pass: 130000/325339 sets processed\n",
      "\tfirst pass: 140000/325339 sets processed\n",
      "\tfirst pass: 150000/325339 sets processed\n",
      "\tfirst pass: 160000/325339 sets processed\n",
      "\tfirst pass: 170000/325339 sets processed\n",
      "\tfirst pass: 180000/325339 sets processed\n",
      "\tfirst pass: 190000/325339 sets processed\n",
      "\tfirst pass: 200000/325339 sets processed\n",
      "\tfirst pass: 210000/325339 sets processed\n",
      "\tfirst pass: 220000/325339 sets processed\n",
      "\tfirst pass: 230000/325339 sets processed\n",
      "\tfirst pass: 240000/325339 sets processed\n",
      "\tfirst pass: 250000/325339 sets processed\n",
      "\tfirst pass: 260000/325339 sets processed\n",
      "\tfirst pass: 270000/325339 sets processed\n",
      "\tfirst pass: 280000/325339 sets processed\n",
      "\tfirst pass: 290000/325339 sets processed\n",
      "\tfirst pass: 300000/325339 sets processed\n",
      "\tfirst pass: 310000/325339 sets processed\n",
      "\tfirst pass: 320000/325339 sets processed\n",
      "\tsecond pass: counting itemsets more than 5 items large\n",
      "\tsecond pass: checking 143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 10000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 20000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 30000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 40000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 50000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 60000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 70000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 80000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 90000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 100000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 110000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 120000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 130000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checked 140000/143847 sets for itemsets of length 6\n",
      "\tsecond pass: checking 48212 sets for itemsets of length 7\n",
      "\tsecond pass: checked 10000/48212 sets for itemsets of length 7\n",
      "\tsecond pass: checked 20000/48212 sets for itemsets of length 7\n",
      "\tsecond pass: checked 30000/48212 sets for itemsets of length 7\n",
      "\tsecond pass: checked 40000/48212 sets for itemsets of length 7\n",
      "\tsecond pass: checking 4931 sets for itemsets of length 8\n",
      "making itemset tree for 19915 itemsets\n",
      "deduplicating itemsets\n",
      "\tcounting itemset cooccurrences for 10000/325250 collections\n",
      "\tcounting itemset cooccurrences for 20000/325250 collections\n",
      "\tcounting itemset cooccurrences for 30000/325250 collections\n",
      "\tcounting itemset cooccurrences for 40000/325250 collections\n",
      "\tcounting itemset cooccurrences for 50000/325250 collections\n",
      "\tcounting itemset cooccurrences for 60000/325250 collections\n",
      "\tcounting itemset cooccurrences for 70000/325250 collections\n",
      "\tcounting itemset cooccurrences for 80000/325250 collections\n",
      "\tcounting itemset cooccurrences for 90000/325250 collections\n",
      "\tcounting itemset cooccurrences for 100000/325250 collections\n",
      "\tcounting itemset cooccurrences for 110000/325250 collections\n",
      "\tcounting itemset cooccurrences for 120000/325250 collections\n",
      "\tcounting itemset cooccurrences for 130000/325250 collections\n",
      "\tcounting itemset cooccurrences for 140000/325250 collections\n",
      "\tcounting itemset cooccurrences for 150000/325250 collections\n",
      "\tcounting itemset cooccurrences for 160000/325250 collections\n",
      "\tcounting itemset cooccurrences for 170000/325250 collections\n",
      "\tcounting itemset cooccurrences for 180000/325250 collections\n",
      "\tcounting itemset cooccurrences for 190000/325250 collections\n",
      "\tcounting itemset cooccurrences for 200000/325250 collections\n",
      "\tcounting itemset cooccurrences for 210000/325250 collections\n",
      "\tcounting itemset cooccurrences for 220000/325250 collections\n",
      "\tcounting itemset cooccurrences for 230000/325250 collections\n",
      "\tcounting itemset cooccurrences for 240000/325250 collections\n",
      "\tcounting itemset cooccurrences for 250000/325250 collections\n",
      "\tcounting itemset cooccurrences for 260000/325250 collections\n",
      "\tcounting itemset cooccurrences for 270000/325250 collections\n",
      "\tcounting itemset cooccurrences for 280000/325250 collections\n",
      "\tcounting itemset cooccurrences for 290000/325250 collections\n",
      "\tcounting itemset cooccurrences for 300000/325250 collections\n",
      "\tcounting itemset cooccurrences for 310000/325250 collections\n",
      "\tcounting itemset cooccurrences for 320000/325250 collections\n",
      "\tfinding supersets\n",
      "\tgetting supersets for 10000/19915 itemsets\n"
     ]
    }
   ],
   "source": [
    "noun_pm_model = PhrasingMotifs('motifs_full','question_arcs_full',min_support=100,\n",
    "                               fit_filter=question_filter, \n",
    "                          verbosity=VERBOSITY)\n",
    "noun_pm_model.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common phrasings, of course, won't be very topic-specific (unless people talk about yachts very very frequently in parliament). However, we do see that phrasings now reflect the pronoun used (which may be troublesome if we believe that \"Does _he_ agree\" and \"Does _she_ agree\" are getting at similar things)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('*',) 325339\n",
      "('will>*',) 70226\n",
      "('does>*',) 61032\n",
      "('is_*',) 57964\n",
      "('is>*',) 45268\n",
      "('is>*', 'is_*') 42850\n",
      "('agree_*',) 36109\n",
      "('agree_does',) 33705\n",
      "('agree_*', 'agree_does') 33705\n",
      "('agree_*', 'does>*') 30013\n",
      "('agree_does', 'does>*') 29988\n",
      "('agree_*', 'agree_does', 'does>*') 29988\n",
      "('will>the',) 26218\n",
      "('will>*', 'will>the') 26218\n",
      "('will>he',) 23049\n",
      "('will>*', 'will>he') 23049\n",
      "('is_aware',) 22063\n",
      "('is_*', 'is_aware') 22063\n",
      "('does>the',) 20932\n",
      "('does>*', 'does>the') 20932\n",
      "('what>*',) 20791\n",
      "('is>*', 'is_aware') 20707\n",
      "('is>*', 'is_*', 'is_aware') 20707\n",
      "('does>he',) 16417\n",
      "('does>*', 'does>he') 16417\n"
     ]
    }
   ],
   "source": [
    "noun_pm_model.print_top_phrasings(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the sink phrasings for our example utterance from earlier, comparing against the noun-less run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt = noun_pm_model.transform_utterance(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agree_*__does>*', 'agree_*__agree_also', 'as>* share_*__share_does']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('motifs__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agree_*__agree_hon',\n",
       " 'agree_*__agree_also__agree_he',\n",
       " 'as>* share_*__share_hon']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('motifs_full__sink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get this extra \"hon\" -- which actually stands for \"honourable [member]\" -- an artefact of parliamentary etiquette. \n",
    "\n",
    "For our particular dataset, removing nouns has the benefit of removing most of these etiquette-related words. However, you may also imagine cases where nouns actually carry a lot of useful information about rhetorical intent (including in this domain -- one could argue that asking about a person versus asking about a department is a strong signal of trying to get at different things, for instance). As such, noun-removal is something that you may want to play around with, and/or try to improve upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we intuited above, \"do you agree\" and \"do you share my opinion\" are both getting at similar intentions. However, extracting these phrasings alone won't allow us to make this association. Rather, our strategy will be to produce vector representations of them which encode this similarity. Clustering these representations then gives us different \"types of question\".\n",
    "\n",
    "Our key intuition here is that questions with similar intentions will tend to be answered in similiar ways. Thus, \"do you agree\" and \"do you share\" may both often be answered with \"yes, I agree\"; if tomorrow I asked a new question of this ilk (\"do you agree that we should invest in planes, instead of yachts\"), I might be expecting a similar sort of answer. \n",
    "\n",
    "For a full explanation of this idea, and how we operationalized it, you can read our paper. In ConvoKit, we implement this methodology of producing vector representations and clusterings via the `PromptTypes` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convokit.prompt_types import PromptTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PromptTypes` will train a model -- a low-dimensional embedding, along with a k-means clustering -- by using question-answer pairs as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def question_filter(utt, aux_input={}):\n",
    "    return utt.meta['is_question']\n",
    "def response_filter(utt, aux_input={}):\n",
    "    return (not utt.meta['is_question']) and (utt.reply_to is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize `pt` with the following arguments:\n",
    "\n",
    "* `n_types=8`: we want to infer 8 types of questions.\n",
    "* `prompt_field='motifs'`: we want to encode questions in terms of the phrasing motifs we extracted above. thus, `pt` will produce representations of these motifs (rather than, e.g., the raw tokens in a question)\n",
    "* `ref_field='arcs_censored'`: we will encode responses in terms of the noun-less arcs we extracted above (in practice, this appears to work better than using phrasings of responses as well, perhaps because responses are noisier)\n",
    "* `prompt_filter=question_filter` and `ref_filter=response_filter`: To tell the transformer what counts as a question and an answer, we will pass the constructor the above filters (i.e., boolean functions). Note that in a less questions-heavy dataset, we could omit these filters and hence infer types of \"prompts\" beyond questions.\n",
    "* `prompt_transform_field='motifs__sink'`: while we want to come up with a representation of _all_ phrasing motifs, when we produce a vector representation of a _particular_ utterance we want to use the most finely-specified phrasing.\n",
    "\n",
    "There are some other arguments you can set, which are listed in the docstring. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PromptTypes(n_types=8, prompt_field='motifs', ref_field='arcs_censored', \n",
    "                 prompt_transform_field='motifs__sink',\n",
    "                 output_field='prompt_types',\n",
    "    random_state=1000, verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit `pt` to the corpus -- that is, learn the associations between question phrasings and response dependency arcs that allow us to produce our vector representations, as well as a clsutering of these representations that gives us our different question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pt.fit(corpus, prompt_filter=question_filter, ref_filter=response_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calling `display_type()` as below will print the question phrasings, response arcs, and prototypical questions and responses that are associated with each inferred type of question. We will examine some of these types more closely by way of examples, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.summarize(corpus=corpus, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(i)\n",
    "    pt.display_type(i, corpus=corpus, k=15)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this trained model is used to transform a corpus, it will output several representations or features associated with each utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt = pt.transform_utterance(utt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector representation encapsulating the utterance's rhetorical intent (in short, an embedding of the utterance based on the responses associated with questions containing its constituent phrasings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('prompt_types__prompt_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between the vector of that utterance and the centroid of each cluster, i.e., type of question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('prompt_types__prompt_dists.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular type of question this utterance is, as well as how close it is to the centroid of that particular cluster (roughly, how well it fits that question type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('prompt_types__prompt_type.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt.get_info('prompt_types__prompt_type_dist.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that our running example is of a question type exemplified by phrasings like `does [the Minister] agree...` -- we may characterize the entire cluster as encapsulating \"agreeing\" questions which are perhaps asked helpfully to bolster the answerer's reputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top prompt:\n",
      "                                                0         1         2  \\\n",
      "agree_*__agree_is                        1.178031  0.392994  1.060446   \n",
      "agree_*__agree_be__does>*                1.135692  0.400345  1.017279   \n",
      "agree_*__agree_is__does>*                1.175818  0.400844  1.064866   \n",
      "agree_*__agree_be                        1.130732  0.401616  1.019973   \n",
      "agree_*__agree_have                      1.150848  0.443389  1.058488   \n",
      "agree_*__agree_are                       1.190365  0.449033  1.090853   \n",
      "agree_*__agree_does__agree_have__does>*  1.135804  0.457290  1.089983   \n",
      "agree_*__agree_are__agree_does__does>*   1.191144  0.460577  1.096827   \n",
      "agree_*__agree_also                      1.176007  0.469945  1.133404   \n",
      "continue_*__will>*                       1.144928  0.474175  1.038749   \n",
      "agree_*__as>*                            1.124752  0.501247  0.949284   \n",
      "agree_*__agree_does__as>*                1.147453  0.507481  0.971820   \n",
      "agree_*__agree_welcome                   1.203914  0.508741  1.044801   \n",
      "is_*__is_agree                           1.205125  0.510086  1.110060   \n",
      "learned_*__learned_agree                 1.150620  0.510949  1.114529   \n",
      "\n",
      "                                                3         4         5  \\\n",
      "agree_*__agree_is                        1.118347  0.855209  1.245478   \n",
      "agree_*__agree_be__does>*                1.131241  0.792990  1.213395   \n",
      "agree_*__agree_is__does>*                1.112551  0.863416  1.251924   \n",
      "agree_*__agree_be                        1.136800  0.783320  1.204353   \n",
      "agree_*__agree_have                      1.128842  0.857956  1.246168   \n",
      "agree_*__agree_are                       1.160460  0.853937  1.257627   \n",
      "agree_*__agree_does__agree_have__does>*  1.115280  0.841571  1.235343   \n",
      "agree_*__agree_are__agree_does__does>*   1.157171  0.862814  1.263466   \n",
      "agree_*__agree_also                      1.174203  0.886887  1.278550   \n",
      "continue_*__will>*                       0.984086  0.959959  1.204275   \n",
      "agree_*__as>*                            1.138565  0.797229  1.184821   \n",
      "agree_*__agree_does__as>*                1.164712  0.828512  1.214503   \n",
      "agree_*__agree_welcome                   1.115074  0.861948  1.146991   \n",
      "is_*__is_agree                           1.200722  0.844801  1.242438   \n",
      "learned_*__learned_agree                 1.205899  0.846344  1.288598   \n",
      "\n",
      "                                                6         7  type_id  \n",
      "agree_*__agree_is                        0.957048  1.149282      1.0  \n",
      "agree_*__agree_be__does>*                0.888559  1.157708      1.0  \n",
      "agree_*__agree_is__does>*                0.958897  1.152021      1.0  \n",
      "agree_*__agree_be                        0.881920  1.156517      1.0  \n",
      "agree_*__agree_have                      0.958250  1.169463      1.0  \n",
      "agree_*__agree_are                       0.944175  1.150599      1.0  \n",
      "agree_*__agree_does__agree_have__does>*  0.959817  1.151314      1.0  \n",
      "agree_*__agree_are__agree_does__does>*   0.948823  1.148104      1.0  \n",
      "agree_*__agree_also                      1.029875  1.136467      1.0  \n",
      "continue_*__will>*                       0.972601  1.214005      1.0  \n",
      "agree_*__as>*                            0.851265  1.206571      1.0  \n",
      "agree_*__agree_does__as>*                0.869202  1.206230      1.0  \n",
      "agree_*__agree_welcome                   0.981365  1.091862      1.0  \n",
      "is_*__is_agree                           0.927002  1.124411      1.0  \n",
      "learned_*__learned_agree                 0.893007  1.134564      1.0  \n",
      "top response:\n",
      "                             0         1         2         3         4  \\\n",
      "agree_certainly       1.172600  0.465506  1.069451  1.099853  0.948805   \n",
      "agree_is              1.166624  0.472614  1.069428  1.093611  0.947803   \n",
      "agree_however         1.164143  0.473389  1.074121  1.115208  0.929423   \n",
      "agree_will            1.173610  0.479482  1.040547  1.119927  0.944613   \n",
      "agree_also            1.186571  0.479957  1.076302  1.094734  0.942869   \n",
      "agree_absolutely      1.186106  0.480638  1.060317  1.066145  0.983891   \n",
      "agree_wholeheartedly  1.176771  0.481028  1.090431  1.087757  0.959604   \n",
      "is_also               1.187847  0.481143  1.050677  0.999050  0.867820   \n",
      "agree_strongly        1.174496  0.485545  1.086276  1.061606  0.976277   \n",
      "agree_completely      1.178798  0.485695  1.086973  1.074517  0.973782   \n",
      "agree_be              1.157274  0.486000  1.077425  1.102518  0.940736   \n",
      "is_vital              1.145542  0.486760  1.006241  0.948051  0.924983   \n",
      "is_reduce             1.104239  0.487099  1.052885  1.132830  0.747489   \n",
      "agree_are             1.170594  0.487802  1.078409  1.071283  0.960251   \n",
      "agree_totally         1.174853  0.488112  1.101322  1.113340  0.921742   \n",
      "\n",
      "                             5         6         7  type_id  \n",
      "agree_certainly       1.309079  0.975690  1.206101      1.0  \n",
      "agree_is              1.307342  0.989236  1.209806      1.0  \n",
      "agree_however         1.302398  0.999549  1.198431      1.0  \n",
      "agree_will            1.307097  1.005520  1.214075      1.0  \n",
      "agree_also            1.310332  0.994407  1.199438      1.0  \n",
      "agree_absolutely      1.310408  0.987268  1.224410      1.0  \n",
      "agree_wholeheartedly  1.295036  1.021919  1.197020      1.0  \n",
      "is_also               1.105690  0.991532  1.118194      1.0  \n",
      "agree_strongly        1.311594  1.025479  1.223978      1.0  \n",
      "agree_completely      1.314119  1.020939  1.213598      1.0  \n",
      "agree_be              1.304915  0.997933  1.207461      1.0  \n",
      "is_vital              1.149789  0.974998  1.207240      1.0  \n",
      "is_reduce             1.167290  0.895301  1.104156      1.0  \n",
      "agree_are             1.297376  1.011452  1.203730      1.0  \n",
      "agree_totally         1.290903  1.027282  1.161894      1.0  \n"
     ]
    }
   ],
   "source": [
    "pt.display_type(utt.get_info('prompt_types__prompt_type.8'), k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform the other utterances in the corpus as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pt.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utterance is of a type that's perhaps more information-seeking and querying for an update (\"what steps is the Government taking, what are they doing to ensure\", etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1.get_info('motifs__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt1.get_info('prompt_types__prompt_type.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt.display_type(utt1.get_info('prompt_types__prompt_type.8'), k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This utterance, on the other hand is a lot more aggressive -- perhaps _accusatory_ to the ends of putting the answerer on the spot (\"will the secretary admit that the policy is a failure?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt2 = corpus.get_utterance('1987-03-04a.857.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt2.get_info('motifs__sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt2.get_info('prompt_types__prompt_type.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt2.get_info('prompt_types__prompt_type.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top prompt:\n",
      "                                                     0         1         2  \\\n",
      "why>*                                         1.104096  1.277495  1.306421   \n",
      "explain_*                                     1.074950  1.264843  1.250738   \n",
      "admit_*                                       1.193359  1.260325  1.288746   \n",
      "admit_*__will>*                               1.222762  1.234878  1.309631   \n",
      "justify_*                                     1.194117  1.271824  1.319768   \n",
      "explain_*__explain_will                       1.080877  1.182869  1.291523   \n",
      "is>*__is_*__is_true                           1.162309  1.139200  1.311678   \n",
      "is_*__why>*                                   1.175892  1.168984  1.278892   \n",
      "does>*__realise_*__realise_does__realise_not  1.152017  1.244742  1.338369   \n",
      "admit_*__admit_will__will>*                   1.231131  1.266068  1.307017   \n",
      "explain_*__will>*                             1.092443  1.146115  1.287732   \n",
      "is_*__is_true                                 1.165552  1.179165  1.333115   \n",
      "admit_*__admit_will                           1.215929  1.298490  1.283715   \n",
      "how>*__how>does                               1.167098  1.238600  1.305153   \n",
      "is_*__is_why                                  1.197365  1.231318  1.274278   \n",
      "\n",
      "                                                     3         4         5  \\\n",
      "why>*                                         1.387563  0.890040  0.865998   \n",
      "explain_*                                     1.364432  0.897947  0.852793   \n",
      "admit_*                                       1.396757  0.925808  0.952902   \n",
      "admit_*__will>*                               1.359393  0.996535  1.007489   \n",
      "justify_*                                     1.358508  1.028394  0.993562   \n",
      "explain_*__explain_will                       1.372512  0.854090  0.924254   \n",
      "is>*__is_*__is_true                           1.481953  0.831132  1.024624   \n",
      "is_*__why>*                                   1.362826  0.828258  0.909416   \n",
      "does>*__realise_*__realise_does__realise_not  1.389140  0.905501  0.973377   \n",
      "admit_*__admit_will__will>*                   1.349086  1.047198  0.989124   \n",
      "explain_*__will>*                             1.335354  0.856301  0.917518   \n",
      "is_*__is_true                                 1.489960  0.884423  1.037931   \n",
      "admit_*__admit_will                           1.370550  1.019011  0.965366   \n",
      "how>*__how>does                               1.345073  0.981865  0.915864   \n",
      "is_*__is_why                                  1.422375  0.881879  0.949384   \n",
      "\n",
      "                                                     6         7  type_id  \n",
      "why>*                                         1.218400  0.576553      7.0  \n",
      "explain_*                                     1.200059  0.581301      7.0  \n",
      "admit_*                                       1.208874  0.586009      7.0  \n",
      "admit_*__will>*                               1.252459  0.596350      7.0  \n",
      "justify_*                                     1.244813  0.604280      7.0  \n",
      "explain_*__explain_will                       1.177795  0.606052      7.0  \n",
      "is>*__is_*__is_true                           1.101389  0.612163      7.0  \n",
      "is_*__why>*                                   1.209574  0.620568      7.0  \n",
      "does>*__realise_*__realise_does__realise_not  1.137492  0.624678      7.0  \n",
      "admit_*__admit_will__will>*                   1.283928  0.629214      7.0  \n",
      "explain_*__will>*                             1.133664  0.629701      7.0  \n",
      "is_*__is_true                                 1.156545  0.632958      7.0  \n",
      "admit_*__admit_will                           1.262404  0.633411      7.0  \n",
      "how>*__how>does                               1.225334  0.634334      7.0  \n",
      "is_*__is_why                                  1.230425  0.638297      7.0  \n",
      "top response:\n",
      "                     0         1         2         3         4         5  \\\n",
      "wonder_*      1.164943  1.106582  1.276903  1.314197  0.848936  0.880912   \n",
      "failed_*      1.204485  1.124149  1.325940  1.336414  0.916511  0.960918   \n",
      "were_*        1.199059  1.066386  1.362643  1.349828  0.904496  1.076865   \n",
      "is_wrong      1.162171  1.148290  1.384513  1.314271  0.934694  0.974407   \n",
      "instead>*     1.170362  1.231754  1.257461  1.263558  0.987653  0.854655   \n",
      "am_surprised  1.159865  1.219641  1.229011  1.333929  0.948790  1.030188   \n",
      "talks_*       1.231180  1.256361  1.227169  1.337933  1.019082  0.901977   \n",
      "talks_about   1.223082  1.293428  1.222819  1.339095  1.035168  0.889763   \n",
      "were_there    1.201340  1.148576  1.388233  1.355567  0.983789  1.096035   \n",
      "astonished_*  1.202651  1.196230  1.305231  1.333094  1.018210  1.151290   \n",
      "surely>*      1.139462  1.009522  1.179284  1.314364  0.728407  0.957278   \n",
      "suggest_to    1.139849  1.207259  1.228474  1.315161  0.945578  0.860964   \n",
      "hoped_*       1.089816  1.259839  1.293617  1.320115  0.966500  0.982924   \n",
      "was_for       1.193836  1.287366  1.424377  1.406558  1.131729  1.125883   \n",
      "suggest_*     1.146471  1.128591  1.096500  1.303793  0.783320  0.763265   \n",
      "\n",
      "                     6         7  type_id  \n",
      "wonder_*      1.180662  0.606207      7.0  \n",
      "failed_*      1.252548  0.654931      7.0  \n",
      "were_*        1.195550  0.672637      7.0  \n",
      "is_wrong      1.248959  0.681267      7.0  \n",
      "instead>*     1.205783  0.681974      7.0  \n",
      "am_surprised  1.206338  0.696276      7.0  \n",
      "talks_*       1.232762  0.699943      7.0  \n",
      "talks_about   1.235879  0.707574      7.0  \n",
      "were_there    1.244528  0.714718      7.0  \n",
      "astonished_*  1.234952  0.721673      7.0  \n",
      "surely>*      1.086058  0.721925      7.0  \n",
      "suggest_to    1.164441  0.729415      7.0  \n",
      "hoped_*       1.239720  0.729955      7.0  \n",
      "was_for       1.342660  0.732654      7.0  \n",
      "suggest_*     1.026639  0.737730      7.0  \n"
     ]
    }
   ],
   "source": [
    "pt.display_type(utt2.get_info('prompt_types__prompt_type.8'), k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the other types should hopefully give you an intuition for the range of questions that tend to be asked in parliament, as well as the coherence of these types (which align fairly well with the output of the paper, even under different random seeds and small implementation tweaks). \n",
    "\n",
    "### a few caveats and potential modifications\n",
    "\n",
    "One thorn in our sides might be that the model occasionally gets caught up on very generic motifs e.g., `'is>*'`, and as such, will fit many questions to the type containing `'is>*'` instead of going with a better signal; various optional parameters detailed in the documentation may provide incomplete solutions to this. Another caveat is that while this model allows us to associate together lexically-diverging phrasings (e.g., \"will the Minister admit\" and \"does the Minister not realise\" both serve to be accusatory towards the Minister), we are ultimately relying on the fact that our domain has a sufficient amount of lexical regularity (e.g., the institutional norms of how people talk in parliament) -- we might need to be cleverer when dealing with noisier settings where this regularity isn't guaranteed (like social media data). \n",
    "\n",
    "Finally, as a data-specific note, cluster 7 may be a result of the parser assuming that \"Will the learned Gentleman please answer my question?\" has \"learned\" as the root verb -- an artefact of parliamentary discourse we haven't handled. You may wish to play around with this by modifying how the data is preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save our trained `pt_model` to disk for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping embedding model\n",
      "dumping training embeddings\n",
      "dumping type model 8\n"
     ]
    }
   ],
   "source": [
    "pt.dump_model(os.path.join(ROOT_DIR, 'pt_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In broad strokes, what's loaded to disk is:\n",
    "\n",
    "* TfIdf models that store the distribution of phrasings and arcs in the training data;\n",
    "* SVD models that allow us to map raw phrasing/arc counts to vector representations;\n",
    "* a KMeans model to cluster vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "km_model.8.joblib\t   svd_model.joblib\t   train_ref_ids.npy\r\n",
      "prompt_df.8.tsv\t\t   train_prompt_df.8.tsv   train_ref_vects.npy\r\n",
      "prompt_tfidf_model.joblib  train_prompt_ids.npy    U_prompt.npy\r\n",
      "ref_df.8.tsv\t\t   train_prompt_vects.npy  U_ref.npy\r\n",
      "ref_tfidf_model.joblib\t   train_ref_df.8.tsv\r\n"
     ]
    }
   ],
   "source": [
    "pt_model_dir = os.path.join(ROOT_DIR, 'pt_model')\n",
    "!ls $pt_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a new `PromptTypes` model and loading our saved model then allows us to use it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pt = PromptTypes(prompt_field='motifs', ref_field='arcs_censored', \n",
    "                 prompt_transform_field='motifs__sink',\n",
    "                 output_field='prompt_types_new', prompt__tfidf_min_df=100,\n",
    "                 ref__tfidf_min_df=100, \n",
    "    random_state=1000, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding model\n",
      "loading training embeddings\n",
      "loading type model 8\n"
     ]
    }
   ],
   "source": [
    "new_pt.load_model(pt_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt = new_pt.transform_utterance(utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt.get_info('prompt_types_new__prompt_type.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples of potential variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying other numbers of prompt types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `refit_types(n)` will retrain the clustering component of the `PromptType` model to infer a different number of types. Suppose we only wanted 4 types of questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.refit_types(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt.summarize(corpus, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n",
    "    pt.display_type(i, type_key=4, k=15)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "top prompt:\n",
      "                                   0         1         2         3  type_id\n",
      "give_*__will>*              0.655711  1.044415  1.071511  1.029346      0.0\n",
      "give_*__give_will           0.658192  1.014454  1.078425  1.061438      0.0\n",
      "give_*                      0.673571  0.990091  1.022852  1.040259      0.0\n",
      "make_*                      0.684921  1.048797  1.156398  0.804960      0.0\n",
      "in>*                        0.692132  0.950216  1.197650  0.760962      0.0\n",
      "ask_*                       0.696715  1.045955  1.109069  0.983805      0.0\n",
      "raise_*                     0.705024  1.153468  1.039631  0.994258      0.0\n",
      "press_*                     0.705496  1.134460  1.047380  0.893885      0.0\n",
      "have_*                      0.706000  0.876789  1.027710  0.973250      0.0\n",
      "give_*__give_to__give_will  0.707316  1.122148  1.194305  0.997719      0.0\n",
      "may>*__press_*              0.708647  1.115044  1.082358  1.012375      0.0\n",
      "be_*                        0.714519  0.913248  1.190963  0.722537      0.0\n",
      "make_*__make_will           0.714843  1.123190  1.122112  0.841403      0.0\n",
      "consider_*                  0.716905  1.159955  1.160823  0.940488      0.0\n",
      "may>*                       0.720993  1.070436  0.980767  0.869330      0.0\n",
      "top response:\n",
      "                         0         1         2         3  type_id\n",
      "understand_*      0.660758  0.967388  1.101134  0.882260      0.0\n",
      "understand_will   0.686179  0.992421  1.084193  1.000622      0.0\n",
      "appreciate_will   0.692760  1.042685  1.126532  0.999959      0.0\n",
      "appreciate_*      0.697310  1.031315  1.156866  0.915099      0.0\n",
      "consider_be       0.698675  1.131167  1.132862  0.935512      0.0\n",
      "be_shall          0.708620  1.008680  1.049406  0.755290      0.0\n",
      "however>*         0.713552  0.956109  1.142501  0.767833      0.0\n",
      "am_aware          0.713780  1.168424  1.066329  1.024992      0.0\n",
      "be_happy          0.718780  1.060285  1.018509  0.767131      0.0\n",
      "consider_is       0.723478  1.107553  1.157946  0.853528      0.0\n",
      "understand_be     0.728148  0.997533  1.096165  0.942369      0.0\n",
      "be_obviously      0.729153  1.085082  1.092575  0.790463      0.0\n",
      "consider_will     0.729230  1.135878  1.184590  0.993722      0.0\n",
      "be_inappropriate  0.729266  0.985065  1.075740  0.932980      0.0\n",
      "have_then         0.734835  1.016300  1.054238  0.988990      0.0\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "top prompt:\n",
      "                                                     0         1         2  \\\n",
      "why>*                                         1.084803  0.599018  1.307095   \n",
      "explain_*                                     1.057081  0.603269  1.273608   \n",
      "why>*__why>does                               1.015838  0.616319  1.302578   \n",
      "explain_*__explain_will                       1.079769  0.630488  1.291137   \n",
      "is_*__why>*                                   1.106676  0.631934  1.299950   \n",
      "admit_*                                       1.122925  0.645433  1.329517   \n",
      "explain_*__explain_is                         1.024231  0.648327  1.226263   \n",
      "explain_*__will>*                             1.077421  0.648583  1.256203   \n",
      "has>*__has>not                                0.986941  0.648587  1.308542   \n",
      "is>*__is_*__is_true                           1.113250  0.658116  1.401291   \n",
      "what>*__what>does                             1.107043  0.667725  1.257771   \n",
      "does>*__realise_*__realise_does__realise_not  1.116090  0.669450  1.328021   \n",
      "is_*__is_why                                  1.116622  0.670102  1.350714   \n",
      "think_*                                       0.967423  0.674502  1.226076   \n",
      "is_*__is_true                                 1.139474  0.678269  1.407445   \n",
      "\n",
      "                                                     3  type_id  \n",
      "why>*                                         1.177747      1.0  \n",
      "explain_*                                     1.164838      1.0  \n",
      "why>*__why>does                               1.036392      1.0  \n",
      "explain_*__explain_will                       1.095729      1.0  \n",
      "is_*__why>*                                   1.087064      1.0  \n",
      "admit_*                                       1.169694      1.0  \n",
      "explain_*__explain_is                         1.100287      1.0  \n",
      "explain_*__will>*                             1.063427      1.0  \n",
      "has>*__has>not                                0.932444      1.0  \n",
      "is>*__is_*__is_true                           1.039251      1.0  \n",
      "what>*__what>does                             1.139703      1.0  \n",
      "does>*__realise_*__realise_does__realise_not  1.135078      1.0  \n",
      "is_*__is_why                                  1.143616      1.0  \n",
      "think_*                                       0.869750      1.0  \n",
      "is_*__is_true                                 1.089395      1.0  \n",
      "top response:\n",
      "                  0         1         2         3  type_id\n",
      "wonder_*   1.102550  0.630263  1.247757  1.043702      1.0\n",
      "is>*       0.987182  0.675863  1.167329  1.008560      1.0\n",
      "failed_*   1.168335  0.680799  1.279090  1.086981      1.0\n",
      "suggest_*  0.963510  0.682211  1.229504  1.013517      1.0\n",
      "says_*     0.999478  0.693099  1.168053  1.066380      1.0\n",
      "instead>*  1.096070  0.696463  1.204946  1.173042      1.0\n",
      "remind_*   0.947590  0.700704  1.293826  0.930490      1.0\n",
      "was_*      1.053760  0.705950  1.230897  0.829287      1.0\n",
      "is_what    1.072299  0.706022  1.159011  1.041435      1.0\n",
      "is_wrong   1.172510  0.706187  1.268029  1.104523      1.0\n",
      "seems_*    0.953523  0.711764  1.258475  0.985633      1.0\n",
      "said_been  0.997174  0.714389  1.184890  1.100818      1.0\n",
      "why>*      1.079495  0.715840  1.197961  1.113588      1.0\n",
      "tell_will  1.002473  0.717460  1.100252  1.151327      1.0\n",
      "was_said   0.975116  0.719189  1.234908  1.062216      1.0\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "top prompt:\n",
      "                                     0         1         2         3  type_id\n",
      "taking_*__taking_is__what>*   1.186642  1.298968  0.610644  1.230078      2.0\n",
      "will>*__work_*__work_with     1.038083  1.313052  0.616865  1.071419      2.0\n",
      "taking_*__taking_are          1.180766  1.316495  0.619078  1.242732      2.0\n",
      "doing_*__what>*               1.207099  1.265001  0.620269  1.230093      2.0\n",
      "taking_*__what>*              1.186769  1.319948  0.621183  1.254192      2.0\n",
      "do_*__do_help                 1.107590  1.261799  0.625326  1.196763      2.0\n",
      "done_*__done_being            1.111542  1.260478  0.628891  1.144888      2.0\n",
      "doing_*                       1.200332  1.241426  0.633485  1.205967      2.0\n",
      "taking_*                      1.210905  1.310812  0.634384  1.249518      2.0\n",
      "do_*__do_can__do_what         1.125488  1.273349  0.634535  1.193493      2.0\n",
      "work_*__work_will__work_with  1.054103  1.325888  0.635598  1.090177      2.0\n",
      "will>*__work_*                1.021889  1.275080  0.635610  0.996767      2.0\n",
      "do_*__do_what                 1.122094  1.202994  0.636780  1.163209      2.0\n",
      "work_*__work_with             1.057552  1.326270  0.641098  1.047891      2.0\n",
      "work_*__work_will             1.003861  1.284119  0.643713  1.018207      2.0\n",
      "top response:\n",
      "                       0         1         2         3  type_id\n",
      "is_working      1.149431  1.225225  0.697749  1.049094      2.0\n",
      "ensure_to       1.046961  1.219194  0.698242  0.942119      2.0\n",
      "raises_*        0.893111  1.292872  0.701282  1.075911      2.0\n",
      "through>*       1.234752  1.307775  0.728002  1.163529      2.0\n",
      "taking_in       1.102440  1.189482  0.733048  1.017897      2.0\n",
      "ensuring_*      1.122741  1.186004  0.735710  1.004110      2.0\n",
      "met_discuss     0.965170  1.333671  0.736543  1.129836      2.0\n",
      "ensuring_is     1.124037  1.213249  0.737293  1.024210      2.0\n",
      "supporting_are  1.234522  1.221080  0.740494  1.236928      2.0\n",
      "met_recently    1.004378  1.280549  0.740640  1.116357      2.0\n",
      "working_on      1.197636  1.268093  0.745331  1.314857      2.0\n",
      "working_are     1.198862  1.261939  0.750917  1.316890      2.0\n",
      "working_make    1.199160  1.266292  0.751455  1.321831      2.0\n",
      "working_with    1.198137  1.263348  0.752408  1.318785      2.0\n",
      "need_have       1.113944  1.167931  0.753314  0.987292      2.0\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "top prompt:\n",
      "                                                0         1         2  \\\n",
      "agree_*__agree_be                        1.007495  1.112158  1.107192   \n",
      "agree_*__agree_be__does>*                1.013111  1.115667  1.105861   \n",
      "agree_*__as>*                            0.966445  1.145842  1.105667   \n",
      "does>*__does_*                           1.053365  1.078229  1.216725   \n",
      "agree_*__agree_is                        1.073712  1.119725  1.101231   \n",
      "does_*                                   1.050546  1.060695  1.193542   \n",
      "agree_*__agree_is__does>*                1.077258  1.123916  1.096377   \n",
      "learned_*                                0.905275  1.077099  1.222742   \n",
      "agree_*__agree_does__as>*                0.995842  1.152260  1.131194   \n",
      "agree_*__agree_have                      1.061776  1.135221  1.103817   \n",
      "agree_*__agree_does__agree_have__does>*  1.064259  1.117984  1.092341   \n",
      "agree_*__agree_are                       1.086571  1.120816  1.137058   \n",
      "resist_*                                 0.883995  1.045427  1.145605   \n",
      "does_*__learned_*                        1.066366  1.099184  1.214238   \n",
      "learned_*__learned_agree                 1.072431  1.118935  1.177705   \n",
      "\n",
      "                                                3  type_id  \n",
      "agree_*__agree_be                        0.483249      3.0  \n",
      "agree_*__agree_be__does>*                0.486911      3.0  \n",
      "agree_*__as>*                            0.528051      3.0  \n",
      "does>*__does_*                           0.529711      3.0  \n",
      "agree_*__agree_is                        0.536007      3.0  \n",
      "does_*                                   0.541052      3.0  \n",
      "agree_*__agree_is__does>*                0.543204      3.0  \n",
      "learned_*                                0.548396      3.0  \n",
      "agree_*__agree_does__as>*                0.552076      3.0  \n",
      "agree_*__agree_have                      0.555461      3.0  \n",
      "agree_*__agree_does__agree_have__does>*  0.556890      3.0  \n",
      "agree_*__agree_are                       0.557414      3.0  \n",
      "resist_*                                 0.559807      3.0  \n",
      "does_*__learned_*                        0.559963      3.0  \n",
      "learned_*__learned_agree                 0.560518      3.0  \n",
      "top response:\n",
      "                      0         1         2         3  type_id\n",
      "is_be          0.888178  0.985381  1.068741  0.519246      3.0\n",
      "is_reduce      1.004212  1.057046  1.104272  0.521249      3.0\n",
      "be_interested  0.917253  1.017003  1.030152  0.548118      3.0\n",
      "be_of          0.822688  1.003579  1.093583  0.553710      3.0\n",
      "be_for         0.887950  0.989333  1.102005  0.558904      3.0\n",
      "be_indeed      0.848207  0.973938  1.091555  0.562321      3.0\n",
      "be_have        0.841648  1.050206  1.006258  0.563110      3.0\n",
      "be_better      0.904047  0.904909  1.112916  0.566116      3.0\n",
      "is_necessary   0.976586  1.034933  1.074532  0.568289      3.0\n",
      "be_also        0.860851  1.052394  1.001948  0.570016      3.0\n",
      "hope_*         0.911286  1.062635  1.011890  0.575861      3.0\n",
      "be_should      0.828261  0.936631  1.091719  0.575970      3.0\n",
      "be_can         0.904084  0.978975  1.054818  0.576731      3.0\n",
      "be_were        0.838111  0.906726  1.137315  0.577619      3.0\n",
      "is_maintain    0.970071  1.092092  1.074144  0.578385      3.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n",
    "    pt.display_type(i, type_key=4, k=15)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying other input formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also experiment with different representations of the input text -- for instance, in lieu of using phrasing motifs we may instead pass questions into the model as just the raw arcs, similar to the responses. This can be modified by changing the `prompt_field` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt_arcs = PromptTypes(prompt_field='arcs_censored', ref_field='arcs_censored', \n",
    "                 prompt_transform_field='arcs_censored',\n",
    "                 output_field='prompt_types_arcs', prompt__tfidf_min_df=100,\n",
    "                 ref__tfidf_min_df=100, n_types=8,\n",
    "    random_state=1000, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_arcs.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt_arcs.summarize(corpus, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "top prompt:\n",
      "                  0         1         2         3         4         5  \\\n",
      "be_would   0.530219  0.963077  1.226676  0.938349  0.917846  0.772022   \n",
      "be_not     0.531131  0.893339  1.250256  0.938005  1.015710  0.823600   \n",
      "asked_for  0.567070  1.004391  1.152797  1.014580  1.030091  0.796486   \n",
      "would>*    0.568361  0.898698  1.225740  0.971188  0.936750  0.737356   \n",
      "hope_*     0.568801  1.141948  1.105715  1.029404  0.762265  0.735186   \n",
      "be_*       0.572172  1.049550  1.080383  0.870872  0.767274  0.797001   \n",
      "will_*     0.574922  1.111423  1.269132  0.945342  0.945557  0.869572   \n",
      "bearing>*  0.581783  0.884632  1.250685  0.913127  0.963242  0.881540   \n",
      "take_will  0.589269  1.074388  1.162336  1.000842  0.827029  0.747008   \n",
      "in>*       0.590970  0.874668  1.180957  0.886673  0.916052  0.806717   \n",
      "\n",
      "                  6         7  type_id  \n",
      "be_would   0.884726  1.143401      0.0  \n",
      "be_not     0.879968  1.132878      0.0  \n",
      "asked_for  0.907388  1.025336      0.0  \n",
      "would>*    0.912118  1.045723      0.0  \n",
      "hope_*     0.935751  1.198803      0.0  \n",
      "be_*       0.771297  1.205190      0.0  \n",
      "will_*     1.058129  1.166344      0.0  \n",
      "bearing>*  0.835680  1.083997      0.0  \n",
      "take_will  0.967198  1.206135      0.0  \n",
      "in>*       0.761756  1.020693      0.0  \n",
      "top response:\n",
      "                       0         1         2         3         4         5  \\\n",
      "is_aware        0.568116  0.895001  1.162683  0.934695  0.943507  0.789280   \n",
      "be_however      0.568120  1.010091  1.123350  0.917706  0.872316  0.739191   \n",
      "be_may          0.578769  1.022139  1.083776  0.954101  0.879822  0.714070   \n",
      "be_possible     0.587528  0.962238  1.131092  0.987893  0.934240  0.788857   \n",
      "be_appropriate  0.596141  0.952655  1.123636  0.917295  0.931862  0.827861   \n",
      "note_*          0.603404  1.026542  1.289014  0.990909  1.000269  0.971476   \n",
      "however>*       0.606537  0.989153  1.167820  0.841870  0.870794  0.879480   \n",
      "be_indeed       0.611487  0.974421  1.095308  0.975625  0.989713  0.702492   \n",
      "be_aware        0.612491  1.105551  1.039430  0.991399  0.844193  0.672648   \n",
      "be_would        0.612558  0.959945  1.122026  0.935793  0.900816  0.706686   \n",
      "\n",
      "                       6         7  type_id  \n",
      "is_aware        0.777339  1.059246      0.0  \n",
      "be_however      0.764785  1.135390      0.0  \n",
      "be_may          0.788972  1.154416      0.0  \n",
      "be_possible     0.821850  1.179834      0.0  \n",
      "be_appropriate  0.745562  1.156389      0.0  \n",
      "note_*          1.066207  1.146018      0.0  \n",
      "however>*       0.777635  1.153071      0.0  \n",
      "be_indeed       0.786581  1.132878      0.0  \n",
      "be_aware        0.887282  1.158042      0.0  \n",
      "be_would        0.732260  1.139043      0.0  \n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "top prompt:\n",
      "                     0         1         2         3         4         5  \\\n",
      "why>*         1.083092  0.494855  1.314762  0.970446  1.263502  1.299022   \n",
      "admit_*       1.086327  0.535432  1.326593  1.067386  1.254513  1.245217   \n",
      "explain_*     1.070715  0.547005  1.277226  0.904530  1.211841  1.263510   \n",
      "why>does      0.954571  0.571955  1.299353  1.010342  1.192762  1.188125   \n",
      "explain_is    1.016540  0.580782  1.259094  0.903271  1.162577  1.199012   \n",
      "admit_will    1.158039  0.583282  1.294502  1.087815  1.235305  1.282503   \n",
      "notice_*      0.973677  0.593886  1.236638  1.052253  1.154008  1.089837   \n",
      "admit_is      1.109063  0.600237  1.261125  1.056433  1.200033  1.197337   \n",
      "explain_will  1.053733  0.603583  1.294314  0.945592  1.262100  1.225337   \n",
      "think_does    0.893349  0.604310  1.248755  1.064268  1.087111  0.959226   \n",
      "\n",
      "                     6         7  type_id  \n",
      "why>*         0.869292  0.861947      1.0  \n",
      "admit_*       0.936406  0.778540      1.0  \n",
      "explain_*     0.866455  0.844459      1.0  \n",
      "why>does      0.805719  0.890694      1.0  \n",
      "explain_is    0.855227  0.947750      1.0  \n",
      "admit_will    0.946976  0.751676      1.0  \n",
      "notice_*      0.824442  0.985727      1.0  \n",
      "admit_is      0.932139  0.777615      1.0  \n",
      "explain_will  0.907880  0.807651      1.0  \n",
      "think_does    0.814114  0.871048      1.0  \n",
      "top response:\n",
      "                        0         1         2         3         4         5  \\\n",
      "understand_does  0.913923  0.530206  1.244604  0.926023  1.105960  1.117995   \n",
      "wonder_*         0.985948  0.602874  1.268265  1.053565  1.218689  1.157646   \n",
      "was_said         1.035679  0.604168  1.233082  0.995426  1.152640  1.237069   \n",
      "yet>*            1.039249  0.620947  1.254817  1.016462  1.172871  1.168158   \n",
      "understand_not   0.869241  0.621943  1.248844  0.923098  1.088132  1.084707   \n",
      "notice_*         1.044004  0.623325  1.259992  1.052741  1.189384  1.076443   \n",
      "am_surprised     1.091704  0.635292  1.281284  1.091244  1.208398  1.206535   \n",
      "seems_*          0.941793  0.639770  1.233280  0.944273  1.075262  1.101017   \n",
      "is_perhaps       1.045189  0.647580  1.237778  1.055043  1.064447  1.122077   \n",
      "suggest_*        0.934541  0.648035  1.205366  0.990180  1.074675  1.128712   \n",
      "\n",
      "                        6         7  type_id  \n",
      "understand_does  0.762377  1.007756      1.0  \n",
      "wonder_*         0.857353  0.861379      1.0  \n",
      "was_said         0.703595  0.985555      1.0  \n",
      "yet>*            0.852109  0.883626      1.0  \n",
      "understand_not   0.791077  1.022939      1.0  \n",
      "notice_*         0.942317  0.938974      1.0  \n",
      "am_surprised     1.040858  0.970584      1.0  \n",
      "seems_*          0.739431  0.931863      1.0  \n",
      "is_perhaps       0.899821  0.968100      1.0  \n",
      "suggest_*        0.722302  0.908740      1.0  \n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "top prompt:\n",
      "                     0         1         2         3         4         5  \\\n",
      "what>*        1.271193  1.242829  0.533470  1.004097  1.067294  1.145698   \n",
      "doing_*       1.300250  1.303236  0.557337  1.210892  1.097350  1.055878   \n",
      "work_will     1.157736  1.375612  0.574530  1.136019  0.927075  0.908271   \n",
      "taking_*      1.290690  1.391301  0.581209  1.196096  1.116523  1.101188   \n",
      "work_with     1.158728  1.381960  0.586168  1.175593  0.901680  0.910564   \n",
      "take_what     1.201615  1.342370  0.590636  1.204965  0.953228  1.003013   \n",
      "taking_are    1.311241  1.396211  0.591723  1.189067  1.147045  1.127837   \n",
      "doing_ensure  1.293618  1.393545  0.596289  1.225890  1.068850  1.074776   \n",
      "doing_is      1.345393  1.310997  0.597982  1.225611  1.133954  1.116887   \n",
      "doing_are     1.241835  1.270208  0.610991  1.178636  1.067941  1.014717   \n",
      "\n",
      "                     6         7  type_id  \n",
      "what>*        1.016859  1.172818      2.0  \n",
      "doing_*       1.199609  1.136187      2.0  \n",
      "work_will     1.143408  1.246337      2.0  \n",
      "taking_*      1.227775  1.231948      2.0  \n",
      "work_with     1.156354  1.254893      2.0  \n",
      "take_what     1.167415  1.226327      2.0  \n",
      "taking_are    1.250719  1.235651      2.0  \n",
      "doing_ensure  1.262923  1.225764      2.0  \n",
      "doing_is      1.225269  1.141828      2.0  \n",
      "doing_are     1.153684  1.113814      2.0  \n",
      "top response:\n",
      "                       0         1         2         3         4         5  \\\n",
      "ensure_to       1.108131  1.306448  0.640705  1.039010  0.951349  0.822272   \n",
      "is_working      1.208815  1.287216  0.648815  1.124987  1.090635  0.951845   \n",
      "supporting_are  1.331074  1.300439  0.662957  1.249540  1.173767  1.134189   \n",
      "leading_*       1.316077  1.350710  0.673457  1.236021  1.196604  1.025748   \n",
      "supporting_*    1.356352  1.312667  0.673724  1.261750  1.183934  1.153322   \n",
      "working_on      1.322885  1.293547  0.677542  1.181899  1.160779  1.235486   \n",
      "working_be      1.328065  1.294312  0.678657  1.186892  1.150785  1.246086   \n",
      "working_are     1.324010  1.287685  0.680483  1.180907  1.163999  1.239402   \n",
      "working_with    1.323986  1.289027  0.682443  1.180717  1.162597  1.241024   \n",
      "working_make    1.326829  1.289550  0.682877  1.181881  1.157818  1.241981   \n",
      "\n",
      "                       6         7  type_id  \n",
      "ensure_to       1.062415  1.123305      2.0  \n",
      "is_working      1.120426  1.184870      2.0  \n",
      "supporting_are  1.188918  1.012198      2.0  \n",
      "leading_*       1.298313  1.102672      2.0  \n",
      "supporting_*    1.205841  1.015859      2.0  \n",
      "working_on      1.278479  1.212714      2.0  \n",
      "working_be      1.275349  1.210267      2.0  \n",
      "working_are     1.274515  1.208007      2.0  \n",
      "working_with    1.275863  1.210122      2.0  \n",
      "working_make    1.281238  1.209500      2.0  \n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "top prompt:\n",
      "                  0         1         2         3         4         5  \\\n",
      "can>*      1.018824  1.021625  1.041883  0.622441  0.995691  1.169494   \n",
      "tell_*     1.126328  0.910502  1.127649  0.661463  1.133067  1.305920   \n",
      "tell_will  1.121723  0.891714  1.120412  0.662334  1.141615  1.328441   \n",
      "say_can    1.057926  1.049132  1.124311  0.662739  1.082772  1.277675   \n",
      "give_*     0.819632  1.024809  1.112161  0.673394  0.935250  1.146022   \n",
      "made_for   0.883479  0.999214  1.164987  0.679807  0.995934  1.059952   \n",
      "give_can   0.978375  1.074045  0.946909  0.682588  0.972306  1.103979   \n",
      "give_on    1.067692  1.074810  0.987935  0.696376  0.995702  1.177160   \n",
      "tell_can   1.197841  1.021841  1.149910  0.697700  1.137681  1.322311   \n",
      "give_is    1.061616  1.043630  1.003967  0.699779  0.936881  1.178604   \n",
      "\n",
      "                  6         7  type_id  \n",
      "can>*      0.816677  1.051386      3.0  \n",
      "tell_*     0.879944  1.003947      3.0  \n",
      "tell_will  0.876438  1.034463      3.0  \n",
      "say_can    0.857190  1.090051      3.0  \n",
      "give_*     0.748347  1.184066      3.0  \n",
      "made_for   0.895450  1.115143      3.0  \n",
      "give_can   0.713509  1.128161      3.0  \n",
      "give_on    0.836152  1.177430      3.0  \n",
      "tell_can   0.997913  1.010481      3.0  \n",
      "give_is    0.907763  1.175974      3.0  \n",
      "top response:\n",
      "                    0         1         2         3         4         5  \\\n",
      "am_able      0.918482  0.963612  1.098547  0.647609  0.976600  1.152163   \n",
      "answer_*     1.125471  0.983866  1.115481  0.672861  1.076114  1.296806   \n",
      "answer_not   1.136628  1.053689  1.110742  0.674258  1.055459  1.308879   \n",
      "answer_can   1.139320  1.060703  1.112515  0.678601  1.067978  1.303003   \n",
      "give_not     1.076256  1.088616  1.061624  0.679629  0.979549  1.206291   \n",
      "have_not     1.026753  1.030671  1.080826  0.693814  1.015578  1.079229   \n",
      "tell_not     1.106022  1.007095  1.026094  0.694261  1.052736  1.217657   \n",
      "undertake_*  0.971457  1.156951  1.057059  0.699248  0.867250  1.125230   \n",
      "answer_will  1.063819  0.900397  1.131275  0.702337  1.066300  1.245694   \n",
      "give_can     1.058614  1.138621  1.026300  0.702890  0.994973  1.166854   \n",
      "\n",
      "                    6         7  type_id  \n",
      "am_able      0.858954  1.134012      3.0  \n",
      "answer_*     0.892951  1.063856      3.0  \n",
      "answer_not   0.965980  1.103090      3.0  \n",
      "answer_can   0.985615  1.096762      3.0  \n",
      "give_not     0.924362  1.080406      3.0  \n",
      "have_not     0.957841  1.074684      3.0  \n",
      "tell_not     0.915570  1.038272      3.0  \n",
      "undertake_*  0.953822  1.174749      3.0  \n",
      "answer_will  0.725859  1.020938      3.0  \n",
      "give_can     0.894773  1.092891      3.0  \n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "top prompt:\n",
      "                   0         1         2         3         4         5  \\\n",
      "be_may      0.853949  1.189232  1.054926  0.985699  0.577264  0.900928   \n",
      "meet_*      0.840623  1.167096  0.993284  1.021191  0.595776  0.886182   \n",
      "agree_meet  0.972518  1.231165  1.047166  1.048583  0.596678  1.014961   \n",
      "meet_will   0.857370  1.161467  0.993744  1.035558  0.601141  0.915025   \n",
      "agree_will  0.811923  1.195992  1.055278  1.027548  0.605951  0.801249   \n",
      "may>*       0.768533  1.105226  0.973381  0.977166  0.608913  0.842178   \n",
      "bring_will  0.867076  1.194298  0.983246  1.034591  0.617695  0.868240   \n",
      "know_*      0.978795  1.192066  0.812671  1.005119  0.622053  0.925272   \n",
      "support_*   0.901840  1.139869  1.019983  0.996784  0.627555  0.944263   \n",
      "press_may   0.927806  1.161533  0.974656  1.044184  0.634418  1.064336   \n",
      "\n",
      "                   6         7  type_id  \n",
      "be_may      0.997012  1.192397      4.0  \n",
      "meet_*      0.869690  1.196073      4.0  \n",
      "agree_meet  1.026935  1.269125      4.0  \n",
      "meet_will   0.857744  1.214656      4.0  \n",
      "agree_will  0.978390  1.250909      4.0  \n",
      "may>*       0.778996  1.202535      4.0  \n",
      "bring_will  1.040761  1.223519      4.0  \n",
      "know_*      0.873666  1.208891      4.0  \n",
      "support_*   0.880442  1.222372      4.0  \n",
      "press_may   0.899310  1.264196      4.0  \n",
      "top response:\n",
      "                       0         1         2         3         4         5  \\\n",
      "want_obviously  0.907577  1.255040  1.024453  1.132374  0.644533  0.924353   \n",
      "am_always       0.854325  1.144099  1.107041  1.091336  0.654140  0.932426   \n",
      "am_happy        1.010650  1.260987  0.992138  1.038787  0.664603  0.845439   \n",
      "raises_*        1.075140  1.345541  0.721301  1.037708  0.673027  0.997217   \n",
      "want_make       1.037227  1.219454  0.918769  1.088422  0.673788  0.820519   \n",
      "suspect_is      0.930851  0.998433  1.124743  0.904337  0.681417  1.013304   \n",
      "was_aware       1.011268  1.145679  1.185948  0.994870  0.684633  1.120382   \n",
      "want_give       1.008584  1.206816  0.873716  1.063327  0.691855  0.790736   \n",
      "am_aware        0.867760  1.214595  1.111893  0.862209  0.693952  1.096814   \n",
      "get_back        1.127525  1.195982  1.143053  1.103359  0.698802  1.124816   \n",
      "\n",
      "                       6         7  type_id  \n",
      "want_obviously  1.016696  1.171538      4.0  \n",
      "am_always       0.945989  1.223901      4.0  \n",
      "am_happy        0.981243  1.158455      4.0  \n",
      "raises_*        1.106424  1.269875      4.0  \n",
      "want_make       0.931422  1.156059      4.0  \n",
      "suspect_is      0.812966  1.104114      4.0  \n",
      "was_aware       1.108279  1.176795      4.0  \n",
      "want_give       0.911256  1.127786      4.0  \n",
      "am_aware        1.073320  1.258113      4.0  \n",
      "get_back        1.155597  1.169849      4.0  \n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "top prompt:\n",
      "                      0         1         2         3         4         5  \\\n",
      "continue_*     0.823351  1.134130  1.037442  1.124866  1.053865  0.497574   \n",
      "share_*        0.846008  1.188543  0.977091  1.147928  0.883145  0.500010   \n",
      "agree_be       0.794403  1.158973  1.120186  1.156777  1.065676  0.500284   \n",
      "agree_make     0.865376  1.224709  0.971921  1.175834  1.004983  0.500523   \n",
      "agree_is       0.867694  1.180016  1.120908  1.197067  1.101224  0.512990   \n",
      "agree_provide  0.950642  1.261847  0.997114  1.204639  1.076841  0.514871   \n",
      "share_does     0.903821  1.217232  0.960571  1.193007  0.903083  0.537473   \n",
      "thank_for      0.891254  1.308502  0.868387  1.164155  0.904928  0.540513   \n",
      "am_*           0.756437  1.181826  0.887287  1.054455  0.779334  0.542123   \n",
      "thank_*        0.892863  1.312506  0.872936  1.165661  0.899561  0.542336   \n",
      "\n",
      "                      6         7  type_id  \n",
      "continue_*     1.005578  1.087961      5.0  \n",
      "share_*        1.018849  1.123817      5.0  \n",
      "agree_be       1.108004  1.123413      5.0  \n",
      "agree_make     1.061907  1.116961      5.0  \n",
      "agree_is       1.172078  1.059885      5.0  \n",
      "agree_provide  1.110867  1.097183      5.0  \n",
      "share_does     1.072948  1.118779      5.0  \n",
      "thank_for      1.087871  1.137896      5.0  \n",
      "am_*           0.897968  1.142774      5.0  \n",
      "thank_*        1.097961  1.143658      5.0  \n",
      "top response:\n",
      "                         0         1         2         3         4         5  \\\n",
      "is_important      0.823091  1.205060  0.924298  1.129964  0.837571  0.463991   \n",
      "is_vital          0.913742  1.237164  0.929116  1.180855  1.005547  0.478293   \n",
      "is_also           0.906658  1.144689  0.971632  1.168583  1.043977  0.484196   \n",
      "does>*            0.805667  1.123173  1.113527  1.156456  1.063384  0.491098   \n",
      "encourage_*       0.938179  1.224590  0.940758  1.158620  0.930357  0.532729   \n",
      "ensure_certainly  0.853501  1.265089  0.927349  1.065540  0.848442  0.536360   \n",
      "agree_does        0.873188  1.197886  1.113913  1.192747  1.088213  0.537082   \n",
      "yes>*             0.899587  1.292784  1.035451  1.204411  1.082637  0.538339   \n",
      "is_maintain       0.814529  1.187589  1.096057  1.120161  0.967164  0.543586   \n",
      "is_essential      0.900532  1.287084  0.964456  1.169324  1.010876  0.548491   \n",
      "\n",
      "                         6         7  type_id  \n",
      "is_important      0.961220  1.140093      5.0  \n",
      "is_vital          1.056827  1.078302      5.0  \n",
      "is_also           1.010804  1.014715      5.0  \n",
      "does>*            1.117908  1.067902      5.0  \n",
      "encourage_*       1.012681  1.095876      5.0  \n",
      "ensure_certainly  1.011530  1.181549      5.0  \n",
      "agree_does        1.202567  1.105470      5.0  \n",
      "yes>*             1.132895  1.103997      5.0  \n",
      "is_maintain       1.033189  1.070921      5.0  \n",
      "is_essential      1.079019  1.189715      5.0  \n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "top prompt:\n",
      "                      0         1         2         3         4         5  \\\n",
      "now>*          0.930524  0.693128  1.143248  0.945555  1.021063  1.094530   \n",
      "make_what      0.942010  0.859143  1.075259  0.904114  1.000192  1.093396   \n",
      "reassure_will  0.888973  1.026586  1.014830  0.929147  0.937983  0.986476   \n",
      "reassure_*     0.934702  1.059738  0.902425  0.909888  0.939026  1.000741   \n",
      "said_*         1.077193  0.651062  1.103434  0.933763  1.075951  1.215159   \n",
      "confirm_be     0.766347  0.954816  1.137578  0.920282  1.000748  1.016848   \n",
      "let_*          0.938660  0.846508  1.017307  0.800363  0.908916  1.083131   \n",
      "mean_does      1.001652  0.725375  1.129852  0.856974  1.076539  1.184158   \n",
      "have_*         0.716586  0.900717  1.014631  0.914123  0.779637  0.864854   \n",
      "said_was       1.020007  0.712227  1.224304  1.025928  1.052290  1.140035   \n",
      "\n",
      "                      6         7  type_id  \n",
      "now>*          0.583877  0.985178      6.0  \n",
      "make_what      0.583950  1.017078      6.0  \n",
      "reassure_will  0.593081  1.127255      6.0  \n",
      "reassure_*     0.619766  1.068274      6.0  \n",
      "said_*         0.624460  0.959991      6.0  \n",
      "confirm_be     0.628671  1.137142      6.0  \n",
      "let_*          0.630685  1.078785      6.0  \n",
      "mean_does      0.635501  0.989025      6.0  \n",
      "have_*         0.636750  1.053029      6.0  \n",
      "said_was       0.645220  1.014187      6.0  \n",
      "top response:\n",
      "                     0         1         2         3         4         5  \\\n",
      "said_*        1.008460  0.745007  1.144703  0.951694  1.059596  1.171223   \n",
      "said_have     1.015991  0.798445  1.129700  0.945252  1.051453  1.188298   \n",
      "said_be       0.956783  0.748573  1.153907  0.945684  1.068200  1.129422   \n",
      "said_in       1.038718  0.755996  1.121740  0.953183  1.041797  1.202864   \n",
      "said_has      0.964807  0.757859  1.150898  0.967394  1.059911  1.126782   \n",
      "said_as       1.027185  0.861970  1.094457  0.975582  0.987995  1.167110   \n",
      "said_already  1.010268  0.807481  1.129660  0.940074  1.037511  1.195317   \n",
      "said_are      1.028496  0.809097  1.121967  0.977092  1.020916  1.154732   \n",
      "said_is       1.000165  0.740632  1.140880  0.977661  1.054847  1.119483   \n",
      "having>*      0.865910  0.845532  1.083879  0.960094  0.902623  1.022343   \n",
      "\n",
      "                     6         7  type_id  \n",
      "said_*        0.536245  1.032166      6.0  \n",
      "said_have     0.545423  1.079131      6.0  \n",
      "said_be       0.550210  1.039731      6.0  \n",
      "said_in       0.551362  1.025224      6.0  \n",
      "said_has      0.551524  1.040312      6.0  \n",
      "said_as       0.557739  1.075057      6.0  \n",
      "said_already  0.560472  1.083837      6.0  \n",
      "said_are      0.561869  1.051017      6.0  \n",
      "said_is       0.564995  1.023303      6.0  \n",
      "having>*      0.576072  1.005751      6.0  \n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "top prompt:\n",
      "                 0         1         2         3         4         5  \\\n",
      "since_*   1.213094  0.990212  1.224008  1.203994  1.246995  1.184356   \n",
      "show_*    1.214314  0.889387  1.256213  1.230375  1.272031  1.153693   \n",
      "higher_*  1.144120  0.950813  1.253374  1.192132  1.240355  1.106875   \n",
      "fell_*    1.287710  0.920701  1.198525  1.190211  1.256002  1.301735   \n",
      "higher>*  1.200784  0.951386  1.216936  1.203388  1.224080  1.153565   \n",
      "risen_*   1.268767  0.975887  1.238245  1.185496  1.264459  1.240751   \n",
      "show_not  1.175047  0.880137  1.316544  1.218451  1.302593  1.140063   \n",
      "of>*      0.970181  0.871688  1.245744  1.171498  1.225725  0.956709   \n",
      "fallen_*  1.331945  1.075112  1.167296  1.224179  1.283998  1.256190   \n",
      "rising_*  1.258761  1.041931  1.168686  1.233641  1.242892  1.209621   \n",
      "\n",
      "                 6         7  type_id  \n",
      "since_*   1.178085  0.567331      7.0  \n",
      "show_*    1.165530  0.585702      7.0  \n",
      "higher_*  1.146486  0.604508      7.0  \n",
      "fell_*    1.094603  0.607382      7.0  \n",
      "higher>*  1.162731  0.612031      7.0  \n",
      "risen_*   1.219560  0.616169      7.0  \n",
      "show_not  1.127924  0.629933      7.0  \n",
      "of>*      1.040517  0.637341      7.0  \n",
      "fallen_*  1.260661  0.639792      7.0  \n",
      "rising_*  1.189771  0.646509      7.0  \n",
      "top response:\n",
      "                  0         1         2         3         4         5  \\\n",
      "was_in     1.128811  0.857405  1.244481  1.171037  1.277109  1.065623   \n",
      "rising_*   1.222073  1.040522  1.191852  1.238949  1.291224  1.179716   \n",
      "is_higher  1.202345  0.994059  1.252543  1.257566  1.306345  1.140594   \n",
      "rising_is  1.210154  1.025047  1.206136  1.223676  1.276084  1.191190   \n",
      "show_*     1.167302  0.916576  1.237592  1.246625  1.284840  1.061825   \n",
      "rose_by    1.264394  1.047539  1.219113  1.221521  1.282758  1.230157   \n",
      "is_high    1.163104  0.963111  1.205858  1.137187  1.223285  1.115385   \n",
      "is_lower   1.238580  0.947525  1.264350  1.244424  1.319578  1.175294   \n",
      "rose_*     1.271165  1.058780  1.226415  1.220346  1.281384  1.244294   \n",
      "are_now    1.285630  0.987947  1.155043  1.213208  1.305769  1.191554   \n",
      "\n",
      "                  6         7  type_id  \n",
      "was_in     1.079523  0.592590      7.0  \n",
      "rising_*   1.225272  0.592683      7.0  \n",
      "is_higher  1.252620  0.600926      7.0  \n",
      "rising_is  1.212374  0.602456      7.0  \n",
      "show_*     1.189266  0.607254      7.0  \n",
      "rose_by    1.268158  0.607524      7.0  \n",
      "is_high    1.128822  0.612154      7.0  \n",
      "is_lower   1.208409  0.613125      7.0  \n",
      "rose_*     1.270736  0.615745      7.0  \n",
      "are_now    1.183639  0.616975      7.0  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(i)\n",
    "    pt_arcs.display_type(i,  k=10)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### going beyond root arcs\n",
    "\n",
    "If we initialize the `TextToArcs` transformer with `root_only=False`, we will use arcs beyond those attached to the root of the dependency parse. This may produce neater output, especially in domains where utterances are less well-structured (see [this notebook](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/conversations-gone-awry/Conversations_Gone_Awry_Prediction.ipynb) for a demo of this on Wikipedia talk page data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storing vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, `PromptTypes` produces a few vector representations of utterances. For efficiency, rather than storing these representations attached to the utterance (as values in utterance.meta), we store them in a corpus-wide matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_vect_repr(utterance_id, matrix name)` allows us to access the representation of a particular utterance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_vectors() missing 1 required positional argument: 'as_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-99407abb42fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prompt_types__prompt_repr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_utt_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_vectors() missing 1 required positional argument: 'as_dataframe'"
     ]
    }
   ],
   "source": [
    "corpus.get_vector_matrix('prompt_types__prompt_repr').get_vectors([test_utt_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17103396,  0.03069409, -0.14371186,  0.10998246, -0.31508472,\n",
       "       -0.03187113, -0.22291774, -0.12785629,  0.17717804,  0.02097519,\n",
       "       -0.35437991, -0.23905016, -0.06359704, -0.19447724, -0.05206238,\n",
       "       -0.03310699, -0.41512443, -0.06049149, -0.11375878, -0.01759784,\n",
       "       -0.04657898, -0.54313603,  0.1298065 , -0.08504893])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_vect_repr(test_utt_id, 'prompt_types__prompt_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save all of these representations to disk, we can call the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.dump_vector_reprs('prompt_types__prompt_repr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stores the representations (`vect_info.<FIELD NAME>.npy`) as a matrix, and the utterance IDs corresponding to each of the rows (`vect_info.<FIELD NAME>.keys`) -- both in the corpus directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversations.json                   info.parsed.jsonl\r\n",
      "corpus.json                          info.question_arcs.jsonl\r\n",
      "feat.question_arcs.json              \u001b[0m\u001b[01;34mpm_model\u001b[0m/\r\n",
      "feat.question_motifs.json            \u001b[01;34mpm_model_old\u001b[0m/\r\n",
      "feat.question_motifs_new.json        \u001b[01;34mpm-new-test\u001b[0m/\r\n",
      "feat.question_motifs_new__sink.json  \u001b[01;34mproc_old\u001b[0m/\r\n",
      "feat.question_motifs__sink.json      \u001b[01;34mpt_model\u001b[0m/\r\n",
      "\u001b[01;34mfull_pipe_models\u001b[0m/                    \u001b[01;34mpt_model_backup\u001b[0m/\r\n",
      "index.json                           \u001b[01;34mpt_model_old\u001b[0m/\r\n",
      "info.arcs_censored.jsonl             users.json\r\n",
      "info.arcs.jsonl                      utterances.json\r\n",
      "info.motifs.jsonl                    vect_info.prompt_types__prompt_repr.keys\r\n",
      "info.motifs__sink.jsonl              vect_info.prompt_types__prompt_repr.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls $ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vector representations can later be re-loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = convokit.Corpus(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.vector_reprs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus.load_vector_reprs('prompt_types__prompt_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17103396,  0.03069409, -0.14371186,  0.10998246, -0.31508472,\n",
       "       -0.03187113, -0.22291774, -0.12785629,  0.17717804,  0.02097519,\n",
       "       -0.35437991, -0.23905016, -0.06359704, -0.19447724, -0.05206238,\n",
       "       -0.03310699, -0.41512443, -0.06049149, -0.11375878, -0.01759784,\n",
       "       -0.04657898, -0.54313603,  0.1298065 , -0.08504893])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus.get_vect_repr(test_utt_id, 'prompt_types__prompt_repr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
